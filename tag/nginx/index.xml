<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Tag - nginx</title>
 <link href="https://jiegec.me/tag/nginx/index.xml" rel="self"/>
 <link href="https://jiegec.me/tag/nginx.html"/>
 <updated>2018-10-07T22:32:08+08:00</updated>
 <id>https://jiegec.me/tag/nginx.html</id>
 <author>
   <name>Jiege Chen</name>
 </author>
 
 <entry>
   <title>调整 Nginx 和 PHP 的上传文件大小限制</title>
   <link href="https://jiegec.me/programming/2018/06/10/nginx-php-upload-size-limit/"/>
   <updated>2018-06-10T16:04:00+08:00</updated>
   <id>https://jiegec.me/programming/2018/06/10/nginx-php-upload-size-limit</id>
   <content type="html">之前迁移的 MediaWiki ，有人提出说无法上传一个 1.4M 的文件。我去看了一下网站，上面写的是限制在 2M ，但是一上传就说 Entity Too Large，无法上传。后来经过研究，是 Nginx 对 POST 的大小进行了限制，同时 PHP 也有限制。

Nginx 的话，可以在 nginx.conf 的 http 中添加，也可以在 server 或者 location 中加入这么一行：

```
client_max_body_size 100m;
```

我的建议是，尽量缩小范围到需要的地方，即 location &gt; server &gt; http 。

在 PHP 中，则修改 /etc/php/7.0/fpm/php.ini ：

```
post_max_size = 100M
```

回到 MediaWiki 的上传页面，可以看到显示的大小限制自动变成了 100M ，这个是从 PHP 的配置中直接获得的。
</content>
 </entry>
 
 <entry>
   <title>在 Nginx 将某个子路径反代</title>
   <link href="https://jiegec.me/devops/2018/06/01/nginx-proxy-subpath/"/>
   <updated>2018-06-01T07:57:00+08:00</updated>
   <id>https://jiegec.me/devops/2018/06/01/nginx-proxy-subpath</id>
   <content type="html">现在遇到这么一个需求，访问根下面是提供一个服务，访问某个子路径（/abc），则需要提供另一个服务。这两个服务处于不同的机器上，我们现在通过反代把他们合在一起。在配置这个的时候，遇到了一些问题，最后得以解决。

```
upstream root {
    server 1.2.3.4:1234;
}
upstream subpath {
    server 4.3.2.1:4321;
}

server {
    listen 443 ssl;
    server_name test.example.com;

    # the last slash is useful, see below
    location /abc/ {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        # the last slash is useful too, see below
        proxy_pass http://subpath/;
    }

    location / {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://root;
    }
}
```

由于并不想 subpath 他看到路径中 /abc/ 这一层，导致路径和原来在根下不同，通过这样配置以后，特别是两个末尾的斜杠，可以让 nginx 把 GET /abc/index.html 改写为 GET /index.html ，这样我们就可以减少许多配置。当然，我们还是需要修改一下配置，现在是 host 在一个新的域名的一个新的子路径下，这主要是为了在返回的页面中，连接写的是正确的。
</content>
 </entry>
 
 <entry>
   <title>使用 Nginx 转发 VMware ESXi</title>
   <link href="https://jiegec.me/networking/2018/05/08/nginx-proxy-vmware-esxi/"/>
   <updated>2018-05-08T19:26:00+08:00</updated>
   <id>https://jiegec.me/networking/2018/05/08/nginx-proxy-vmware-esxi</id>
   <content type="html">我们的 VMware ESXi 在一台 NAT Router 之后，但是我们希望通过域名可以直接访问 VMware ESXi 。我们首先的尝试是，把 8443 转发到它的 443 端口，比如：

```shell
socat TCP-LISTEN:8443,reuseaddr,fork TCP:esxi_addr:443
```

它能工作地很好（假的，如果你把 8443 换成 9443 它就不工作了），但是，我们想要的是，直接通过 esxi.example.org 就可以访问它。于是，我们需要 Nginx 在其中做一个转发的功能。在这个过程中遇到了很多的坑，最后终于是做好了 （VMware Remote Console等功能还不行，需要继续研究）。

首先讲讲为啥把 8443 换成 9443 不能工作吧 -- 很简单，ESXi 的网页界面会请求 8443 端口。只是恰好我用 8443 转发到 443， 所以可以正常工作。这个很迷，但是测试的结果确实如此。VMware Remote Console 还用到了别的端口，我还在研究之中。

来谈谈怎么配置这个 Nginx 转发吧。首先是 80 跳转 443:
```
server {
        listen 80;
        listen 8080;
        server_name esxi.example.org;

        return 301 https://$host$request_uri;
}
```

这个很简单，接下来是转发 443 端口：
```

server {
        listen 443 ssl;
        server_name esxi.example.org;
        ssl_certificate /path/to/ssl/cert.pem;
        ssl_certificate_key /path/to/ssl/key.pem;

        location / {
                proxy_pass https://esxi_addr;
                proxy_ssl_verify off;
                proxy_ssl_session_reuse on;
                proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
        }
}
```

此时，打开 https://esxi.example.org 就能看到登录界面了。但是仍然无法登录。从 DevTools 看错误，发现它请求了 8443 端口。于是进行转发：
```
server {
        listen 8443 ssl;
        server_name esxi.example.org;
        ssl_certificate /path/to/ssl/cert.pem;
        ssl_certificate_key /path/to/ssl/key.pem;


        location / {
                if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Origin' 'https://esxi.example.org';
                        add_header 'Access-Control-Allow-Credentials' 'true';
                        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
                        add_header 'Access-Control-Max-Age' 1728000;
                        add_header 'Access-Control-Allow-Headers' 'VMware-CSRF-Token,DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Cookie,SOAPAction';
                        add_header 'Content-Type' 'text/plain; charset=utf-8';
                        add_header 'Content-Length' 0;
                        return 204;
                }

                add_header 'Access-Control-Allow-Origin' 'https://esxi.example.org';
                add_header 'Access-Control-Allow-Credentials' 'true';
                proxy_pass https://esxi_addr:443;
                proxy_ssl_verify off;
                proxy_ssl_session_reuse on;
                proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
        }
}
```

主要麻烦的是配置 CORS 的相关策略。我也是看了 DevTools 的错误提示半天才慢慢写出来的。这样配置以后，就可以成功登录 VMware ESXi 了。

20:02 更新：现在做了 WebSocket 转发，目前可以在浏览器中打开 Web Console 了。但是，在访问 https://esxi.example.org/ 的时候还是会出现一些问题，然而 https://esxi.example.org:8443/ 是好的。

转发 WebSocket：
```
map $http_upgrade $connection_upgrade {
        default upgrade;
        ''      close;
}

server {
        listen 8443 ssl;
        server_name esxi.example.org;
        ssl_certificate /path/to/ssl/cert.pem;
        ssl_certificate_key /path/to/ssl/key.pem;


        location / {

                if ($request_method = 'OPTIONS') {
                        add_header 'Access-Control-Allow-Origin' 'https://esxi.example.org';
                        add_header 'Access-Control-Allow-Credentials' 'true';
                        add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
                        add_header 'Access-Control-Max-Age' 1728000;
                        add_header 'Access-Control-Allow-Headers' 'VMware-CSRF-Token,DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Cookie,SOAPAction';
                        add_header 'Content-Type' 'text/plain; charset=utf-8';
                        add_header 'Content-Length' 0;
                        return 204;
                }

                add_header 'Access-Control-Allow-Origin' 'https://esxi.example.org' always;
                add_header 'Access-Control-Allow-Credentials' 'true' always;

                proxy_pass https://esxi_addr:443;
                proxy_ssl_verify off;
                proxy_ssl_session_reuse on;
                proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection $connection_upgrade;
        }
}
```

20:29 更新：找到了 VMware Remote Console 的端口：902，用 iptables 进行 DNAT 即可：
```shell
iptables -A PREROUTING -i wan_interface -p tcp -m tcp --dport 902 -j DNAT --to-destination esxi_addr:902
```

2018-05-09 08:07 更新：最后发现，还是直接隧道到内网访问 ESXi 最科学。或者，让 443 重定向到 8443 ：
```
server {
        listen 443 ssl;
        server_name esxi.example.org;
        ssl_certificate /path/to/ssl/cert.pem;
        ssl_certificate_key /path/to/ssl/key.pem;

        return 301 https://$host:8443$request_uri;
}
```
这样，前面也不用写那么多 CORS 的东西了。
</content>
 </entry>
 
 <entry>
   <title>On Nginx Memory Pool</title>
   <link href="https://jiegec.me/programming/2017/12/02/on-nginx-memory-pool/"/>
   <updated>2017-12-02T22:16:07+08:00</updated>
   <id>https://jiegec.me/programming/2017/12/02/on-nginx-memory-pool</id>
   <content type="html">今晚参加了 Tunight ，会长给我们讲了 Nginx 的一些内部运作的机制和原理。中间的时候，会长展示的代码中用到了线程池方面的一些函数，但是大多地方只有调用 `ngx_pcalloc` 而没有看到相应的对象释放的过程，于是在演示的最后，会长应大家要求对 Nginx 魔幻的线程池实现做了现场代码分析。

在分析的中途遇到了很多坑，最后才终于理清了内存池的工作原理。这里直接解释结论吧。以下代码均摘自 Nginx 1.13.7 ，代码都可以在官方仓库找到。

首先分析一下创建一个内存池的函数：
``` c
ngx_pool_t *
ngx_create_pool(size_t size, ngx_log_t *log)
{
    ngx_pool_t  *p;

    p = ngx_memalign(NGX_POOL_ALIGNMENT, size, log);
    if (p == NULL) {
        return NULL;
    }

    p-&gt;d.last = (u_char *) p + sizeof(ngx_pool_t);
    p-&gt;d.end = (u_char *) p + size;
    p-&gt;d.next = NULL;
    p-&gt;d.failed = 0;

    size = size - sizeof(ngx_pool_t);
    p-&gt;max = (size &lt; NGX_MAX_ALLOC_FROM_POOL) ? size : NGX_MAX_ALLOC_FROM_POOL;

    p-&gt;current = p;
    p-&gt;chain = NULL;
    p-&gt;large = NULL;
    p-&gt;cleanup = NULL;
    p-&gt;log = log;

    return p;
}
```

现在开始分段分析这个函数：在这里，一个内存池用一个 `ngx_pool_t (aka struct ngx_pool_s)` 类型的数据进行包装，所有的关于内存池的操作都基于相应的内存池对象。 `ngx_log_t` 表示输出信息的对象，与内存池无关，后面也不会讨论它。

``` c
    p = ngx_memalign(NGX_POOL_ALIGNMENT, size, log);
    if (p == NULL) {
        return NULL;
    }

    p-&gt;d.last = (u_char *) p + sizeof(ngx_pool_t);
    p-&gt;d.end = (u_char *) p + size;
    p-&gt;d.next = NULL;
    p-&gt;d.failed = 0;
```

这里通过调用 `ngx_memalign` 分配一段（能对齐就对齐，不能对齐就放弃的）以 size 为大小的内存，做为这个内存池第一个块的内存，这个块的头是完整的，其中 `p-&gt;d.last` 和 `p-&gt;d.end` 分别表示可用于分配对象的内存段的开始和结束，在用 `p-&gt;d.next` 连接起来的链表中，每个链表实际上只有 `d` 是存储了数据，后面的各个域都不再使用。这里的 `p-&gt;d.failed` 涉及到链表的优化，在以后会接触到。

``` c
    size = size - sizeof(ngx_pool_t);
    p-&gt;max = (size &lt; NGX_MAX_ALLOC_FROM_POOL) ? size : NGX_MAX_ALLOC_FROM_POOL;

    p-&gt;current = p;
    p-&gt;chain = NULL;
    p-&gt;large = NULL;
    p-&gt;cleanup = NULL;
    p-&gt;log = log;

    return p;
```

这里的 `size` 计算出实际用于对象分配的内存大小， `p-&gt;max` 存储了当前这个块最大能容纳的对象的大小， `p-&gt;current` 会和上面的 `p-&gt;d.failed` 合在一起对链表进行优化。 `p-&gt;chain` 与其他功能关系较密切，不会在本文中展开，而 `p-&gt;cleanup` 允许外部注册一些清理函数，实现起来并不难。

接下来，由于 `ngx_pnalloc` 和 `ngx_pcalloc` 都和 `ngx_palloc` 相近，这里只对 `ngx_palloc` 进行分析：

``` c
void *
ngx_palloc(ngx_pool_t *pool, size_t size)
{
#if !(NGX_DEBUG_PALLOC)
    if (size &lt;= pool-&gt;max) {
        return ngx_palloc_small(pool, size, 1);
    }
#endif

    return ngx_palloc_large(pool, size);
}
```

这里分了两种情况，如果要分配的内存大于一个块的最大值，那么这段内存必须要单独分配单独维护，所以调用了 `ngx_palloc_large` ，下面对其分析：

``` c
static void *
ngx_palloc_large(ngx_pool_t *pool, size_t size)
{
    void              *p;
    ngx_uint_t         n;
    ngx_pool_large_t  *large;

    p = ngx_alloc(size, pool-&gt;log);
    if (p == NULL) {
        return NULL;
    }

    n = 0;

    for (large = pool-&gt;large; large; large = large-&gt;next) {
        if (large-&gt;alloc == NULL) {
            large-&gt;alloc = p;
            return p;
        }

        if (n++ &gt; 3) {
            break;
        }
    }

    large = ngx_palloc_small(pool, sizeof(ngx_pool_large_t), 1);
    if (large == NULL) {
        ngx_free(p);
        return NULL;
    }

    large-&gt;alloc = p;
    large-&gt;next = pool-&gt;large;
    pool-&gt;large = large;

    return p;
}
```

这里的 `ngx_alloc` 就是对 `malloc` 的简单封装，直接分配一段内存，然后向 `pool-&gt;large` 中以 `ngx_pool_large_t` 组成的链表中插入。这里有一个小优化：因为 `ngx_pool_large_t` 本身也要占用内存，为了复用已经被释放的 `ngx_pool_large_t` ，尝试链表的前几项，如果几项中都没有空的位置，因为 `ngx_pool_large_t` 本身是一个很小的对象，自然可以复用自己在内存池中分配对象的方法 `ngx_palloc_small` ，然后把它加入到 `pool-&gt;large` 的链表的第一向前。如果很大的内存都在分配后很快释放，这种方法可以复用很多的 `ngx_pool_large_t` 。

接下来分析 `ngx_palloc_small` ：

``` c
static ngx_inline void *
ngx_palloc_small(ngx_pool_t *pool, size_t size, ngx_uint_t align)
{
    u_char      *m;
    ngx_pool_t  *p;

    p = pool-&gt;current;

    do {
        m = p-&gt;d.last;

        if (align) {
            m = ngx_align_ptr(m, NGX_ALIGNMENT);
        }

        if ((size_t) (p-&gt;d.end - m) &gt;= size) {
            p-&gt;d.last = m + size;

            return m;
        }

        p = p-&gt;d.next;

    } while (p);

    return ngx_palloc_block(pool, size);
}
```

首先，从 `pool-&gt;current` 遍历（这样做的原因下面会提到）已有的各个块，寻找有没有哪个块能容纳下现在需要的大小，如果能就可以调整 `p-&gt;d.last` 返回，否则就分配一个新的块到内存池中，再从新的块中分配需要的大小的内存。需要一提的是，在设计中，小的对象是随着整个内存池的销毁而被一起释放的，不会在中途被释放，而大的对象尽量要用完即释放。接下来分析 `ngx_palloc_block` ：

``` c
static void *
ngx_palloc_block(ngx_pool_t *pool, size_t size)
{
    u_char      *m;
    size_t       psize;
    ngx_pool_t  *p, *new;

    psize = (size_t) (pool-&gt;d.end - (u_char *) pool);

    m = ngx_memalign(NGX_POOL_ALIGNMENT, psize, pool-&gt;log);
    if (m == NULL) {
        return NULL;
    }

    new = (ngx_pool_t *) m;

    new-&gt;d.end = m + psize;
    new-&gt;d.next = NULL;
    new-&gt;d.failed = 0;

    m += sizeof(ngx_pool_data_t);
    m = ngx_align_ptr(m, NGX_ALIGNMENT);
    new-&gt;d.last = m + size;

    for (p = pool-&gt;current; p-&gt;d.next; p = p-&gt;d.next) {
        if (p-&gt;d.failed++ &gt; 4) {
            pool-&gt;current = p-&gt;d.next;
        }
    }

    p-&gt;d.next = new;

    return m;
}
```

为了节省内存，结构体中并没有记录实际分配的内存块的大小，于是根据第一个块的大小分配当前的块，虽然这里用的也是一个类型为 `ngx_pool_t` 结构体，实际上只用到了 `new-&gt;d` 中的内容维护块组成的链表和块内的分配情况。然后从 `pool-&gt;current` 开始找块的链表的结尾，找到节尾后把当前的块加到结尾的后面，然后把刚才需要分配的小对象的地址返回。与此同时，由于调用这个函数的时候，一定是当前的对象在已有的从 `pool-&gt;current` 开始的块中都放不下了，我们给这些块的 `p-&gt;d.failed` 进行自增，意思是说这个块在分配新的对象的时候又一次放不下了，如果放不下的次数比较多，我们可以认为这个块已经装得比较满了，那么，我们把 `pool-&gt;current` 设为它的后继，以后在分配新的对象的时候就会自动跳过这些比较满的块，从而提高了效率。

``` c
ngx_int_t
ngx_pfree(ngx_pool_t *pool, void *p)
{
    ngx_pool_large_t  *l;

    for (l = pool-&gt;large; l; l = l-&gt;next) {
        if (p == l-&gt;alloc) {
            ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool-&gt;log, 0,
                           &quot;free: %p&quot;, l-&gt;alloc);
            ngx_free(l-&gt;alloc);
            l-&gt;alloc = NULL;

            return NGX_OK;
        }
    }

    return NGX_DECLINED;
}
```

从 `ngx_pfree` 的实现可以看出，只有大的对象才会要求尽快释放，小的对象和没有被手动释放的大的对象都会随着内存池生命周期的结束而一起释放。如 `ngx_destroy_pool` 中的实现：

``` c
void
ngx_destroy_pool(ngx_pool_t *pool)
{
    ngx_pool_t          *p, *n;
    ngx_pool_large_t    *l;
    ngx_pool_cleanup_t  *c;

    for (c = pool-&gt;cleanup; c; c = c-&gt;next) {
        if (c-&gt;handler) {
            ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool-&gt;log, 0,
                           &quot;run cleanup: %p&quot;, c);
            c-&gt;handler(c-&gt;data);
        }
    }

#if (NGX_DEBUG)

    /*
     * we could allocate the pool-&gt;log from this pool
     * so we cannot use this log while free()ing the pool
     */

    for (l = pool-&gt;large; l; l = l-&gt;next) {
        ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool-&gt;log, 0, &quot;free: %p&quot;, l-&gt;alloc);
    }

    for (p = pool, n = pool-&gt;d.next; /* void */; p = n, n = n-&gt;d.next) {
        ngx_log_debug2(NGX_LOG_DEBUG_ALLOC, pool-&gt;log, 0,
                       &quot;free: %p, unused: %uz&quot;, p, p-&gt;d.end - p-&gt;d.last);

        if (n == NULL) {
            break;
        }
    }

#endif

    for (l = pool-&gt;large; l; l = l-&gt;next) {
        if (l-&gt;alloc) {
            ngx_free(l-&gt;alloc);
        }
    }

    for (p = pool, n = pool-&gt;d.next; /* void */; p = n, n = n-&gt;d.next) {
        ngx_free(p);

        if (n == NULL) {
            break;
        }
    }
}
```

这个函数首先调用了一系列用户定义的 `pool-&gt;cleanup` 链表中的函数，允许自定义回收一些特定的资源。然后对每一个 `pool-&gt;large` 链表中的内容分别释放，最后再把各个块中所有的内存整块释放。注意 `ngx_large_block_t` 也是存在块中的，所以顺序不能反了。
</content>
 </entry>
 
</feed>
