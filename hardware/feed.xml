<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hardwares on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/hardware/</link>
    <description>Recent content in Hardwares on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jan 2023 15:41:00 +0800</lastBuildDate><atom:link href="https://jia.je/hardware/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PCIe Bifurcation</title>
      <link>https://jia.je/hardware/2023/01/05/pcie-bifurcation/</link>
      <pubDate>Thu, 05 Jan 2023 15:41:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2023/01/05/pcie-bifurcation/</guid>
      <description>背景 最近看到两篇关于 PCIe Bifurcation 的文章：
intel 部分桌面级 CPU 的 pcie 通道拆分另类低成本实现 Intel Alder Lake 12 代酷睿 CPU PCIe 拆分实现方法 文章讲的是如何在 CPU 上进行跳线，从而实现 PCIe Bifurcation 的配置。正好借此机会来研究一下 PCIe Bifurcation。
PCIe Bifurcation PCIe Bifurcation 的目的是让 PCIe 有更好的灵活性。从 CPU 出来的几路 PCIe，它的宽度一般是确定的，比如有一个 x16，但是实际使用的时候，想要接多个设备，例如把 x16 当成两个 x8 来用，这就是 PCIe Bifurcation。这需要 PCIe 两端的支持，CPU 端需要可配置 PCIe Bifurcation，不然只能从一个 x16 降级到一个 x8，剩下的 8x 就没法利用了；设备端需要转接卡，把 x16 的信号分成两路，然后提供两个 PCIe 插槽，有时则是主板设计时就做了拆分，不需要额外的转接卡。
那么怎么配置 CPU 端的 PCIe Bifurcation 呢？其实就是上面两篇文章提到的办法：CPU 根据 CFG 信号来决定 PCIe Bifurcation 配置，例如要选择 1x16，2x8 还是 1x8+2x4 等等。简单总结一下实现思路都是：</description>
    </item>
    
    <item>
      <title>ACPI 学习笔记</title>
      <link>https://jia.je/hardware/2022/12/10/acpi-notes/</link>
      <pubDate>Sat, 10 Dec 2022 20:44:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/12/10/acpi-notes/</guid>
      <description>标准 ACPI 标准可以从官网下载。
ACPI 的表现形式为一颗树加若干个表，表的结构比较规整，里面每个字段都有固定的含义。树的结点可能是属性，或者是一些函数。操作系统可以操作上面的属性，调用 ACPI 中的函数，来进行一些硬件相关的操作。ACPI 一般与主板密切相关，主板厂家配置好 ACPI 后，操作系统就不需要给每个主板都写一遍代码了。
ASL 为了开发 ACPI，需要使用 ACPI Source Language(ASL) 来进行编程，使用 iasl 编译成 ACPI 表以后，由操作系统进行解释执行。推荐阅读一个比较好的 ASL 教程：ACPI Source Language (ASL) Tutorial。
简单来说，ASL 中的变量类型：
Integer: int32_t/int64_t String: char * Buffer: uint8_t [] Package: object [] Object Reference: object &amp;amp; Method ACPI 需要访问硬件，一般是通过 MMIO 或者 IO Port 来进行访问。在内核开发的时候，MMIO 一般是用一系列 volatile 指针来对应硬件的寄存器定义。ASL 中也可以做类似的事情，分为两步：OperationRegion 和 Field。
OperationRegion 就是声明了一片地址空间，以及对应的类型，常见的类型有 SystemMemory、SystemIO、PCI_Config、SMBus 等等。当 ACPI 中的代码要访问 OperationRegion 中的数据的时候，内核按照类型去进行实际的访问。
有了地址空间以后，还需要根据寄存器的定义，给各个字段起个名字，这就是 Field。Field 给 OperationRegion 中的字段起名，与硬件的定义想对应，这就像在内核中定义一个结构体，保证结构体的成员的偏移和硬件是一致的。这样就可以通过成员来访问，而不是每次都去计算一次偏移。</description>
    </item>
    
    <item>
      <title>InfiniBand 学习笔记</title>
      <link>https://jia.je/hardware/2022/12/06/infiniband-notes/</link>
      <pubDate>Tue, 06 Dec 2022 18:47:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/12/06/infiniband-notes/</guid>
      <description>参考文献 Infiniband Architecture Overview InfiniBand Architecture Specification Volume 1 Release 1.2.1 InfiniBand Architecture Specification Volume 2 Release 1.4 An Introduction to the InfiniBand Architecture InfiniBand Network Architecture - MindShare ArchWiki - InfiniBand 概览 InfiniBand 的网络分为两层，第一层是由 End Node 和 Switch 组成的 Subnet，第二层是由 Router 连接起来的若干个 Subnet。有点类似以太网以及 IP 的关系，同一个二层内通过 MAC 地址转发，三层间通过 IP 地址转发。
在 IB 网络中，End Node 一般是插在结点上的 IB 卡（Host Channel Adapter，HCA）或者是存储结点上的 Target Channel Adapter。End Node 之间通过 Switch 连接成一个 Subnet，由 Subnet Manager 给每个 Node 和 Switch 分配 Local ID，同一个 Subnet 中通过 LID（Local ID）来路由。但是 LID 位数有限，为了进一步扩展，可以用 Router 连接多个 Subnet，此时要通过 GID（Global ID）来路由。</description>
    </item>
    
    <item>
      <title>升级 Mellanox 网卡固件</title>
      <link>https://jia.je/hardware/2022/11/23/upgrade-mlnx-firmware/</link>
      <pubDate>Wed, 23 Nov 2022 19:24:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/11/23/upgrade-mlnx-firmware/</guid>
      <description>背景 最近发现有一台机器，插上 ConnectX-4 IB 网卡后，内核模块可以识别到设备，但是无法使用，现象是 ibstat 等命令都看不到设备。降级 OFED 从 5.8 到 5.4 以后问题消失，所以认为可能是新的 OFED 与比较旧的固件版本有兼容性问题，所以尝试升级网卡固件。升级以后，问题就消失了。
安装 MFT 首先，在 https://network.nvidia.com/products/adapter-software/firmware-tools/ 下载 MFT，按照指示解压，安装后，启动 mst 服务，就可以使用 mlxfwmanager 得到网卡的型号以及固件版本：
Device Type: ConnectX4 Description: Mellanox ConnectX-4 Single Port EDR PCIE Adapter LP PSID: DEL2180110032 Versions: Current FW 12.20.1820 升级固件 从 PSID 可以看到，这是 DELL OEM 版本的网卡，可以在 https://network.nvidia.com/support/firmware/dell/ 处寻找最新固件，注意需要保证 PSID 一致，可以找到这个 PSID 的 DELL 固件地址：https://www.mellanox.com/downloads/firmware/fw-ConnectX4-rel-12_28_4512-06W1HY_0JJN39_Ax-FlexBoot-3.6.203.bin.zip。
下载以后，解压，然后就可以升级固件：
mlxfwmanager -u -i fw-ConnectX4-rel-12_28_4512-06W1HY_0JJN39_Ax-FlexBoot-3.6.203.bin 升级以后重启就工作了。
考虑到类似的情况之后还可能发生，顺便还升级了其他几台机器的网卡，下面是一个例子：
Device Type: ConnectX4 Description: ConnectX-4 VPI adapter card; FDR IB (56Gb/s) and 40GbE; dual-port QSFP28; PCIe3.</description>
    </item>
    
    <item>
      <title>CXL 学习笔记</title>
      <link>https://jia.je/hardware/2022/11/20/cxl-notes/</link>
      <pubDate>Sun, 20 Nov 2022 23:05:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/11/20/cxl-notes/</guid>
      <description>背景 前段时间学习了 PCIe，趁此机会，进一步学习一下密切相关的 CXL。
CXL 的标准是公开下载的：https://www.computeexpresslink.org/download-the-specification，我目前参考的是 2022 年 8 月 1 日的 CXL 3.0 版本。
CXL 设备类型 CXL 对 PCIe 的重要的扩展，一是在于让设备可以和 CPU 实现缓存一致性（CXL.cache），二是可以做远程的内存（CXL.mem）。
具体下来，CXL 标准主要定义了三类设备：
CXL Type 1: 设备带有与 CPU 一致的缓存，实现 CXL.io 和 CXL.cache CXL Type 2: 设备带有自己的内存和与 CPU 一致的缓存，实现 CXL.io，CXL.cache 和 CXL.mem CXL Type 3: 设备带有自己的内存，实现 CXL.io 和 CXL.mem CXL 传输层 CXL.io CXL.io 基本上就是 PCIe 协议：
CXL.io provides a non-coherent load/store interface for I/O devices. Figure 3-1 shows where the CXL.</description>
    </item>
    
    <item>
      <title>PCIe 学习笔记</title>
      <link>https://jia.je/hardware/2022/11/12/pcie-notes/</link>
      <pubDate>Sat, 12 Nov 2022 14:56:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/11/12/pcie-notes/</guid>
      <description>背景 最近在知乎上看到 LogicJitterGibbs 的 资料整理：可以学习 1W 小时的 PCIe，我跟着资料学习了一下，然后在这里记录一些我学习 PCIe 的笔记。
下面的图片主要来自 PCIe 3.0 标准以及 MindShare 的 PCIe 3.0 书本。
分层 PCIe 定义了三个层：Transaction Layer，Data Link Layer，Physical Layer，和 TCP/IP 四层模型很像。PCIe 也是基于 Packet 传输的。
Transaction Layer Transaction Layer 的核心是 Transaction Layer Packet(TLP)。TLP 格式：
即可选的若干个 Prefix，一个 Header，可选的 Data Payload，可选的 Digest。
Prefix 和 Header 开头的一个字节是 Fmt[2:0] 和 Type[4:0] 字段。Fmt 决定了 header 的长度，有无数据，或者这是一个 Prefix。
它支持几类 Packet：
Memory: MMIO Read Request(MRd)/Completion(CplD) Write Request(MWr): 注意只有 Request，没有 Completion AtomicOp Request(FetchAdd/Swap/CAS)/Completion(CplD) Locked Memory Read(MRdLk)/Completion(CplDLk): Legacy IO: Legacy Read Request(IORd)/Completion(CplD) Write Request(IOWr)/Completion(Cpl) Configuration: 访问配置空间 Read Request(CfgRd0/CfgRd1)/Completion(CplD) Write Request(CfgWr0/CfgWr1)/Completion(Cpl) Message: 传输 event Request(Msg/MsgD) 括号里的是 TLP Type，对应了它 Fmt 和 Type 字段的取值。如果 Completion 失败了，原来应该是 CplD/CplDLk 的 Completion 会变成不带数据的 Cpl/CplLk。</description>
    </item>
    
    <item>
      <title>在 GNURadio Companion 中收听 FM 广播</title>
      <link>https://jia.je/hardware/2022/10/24/gnuradio-fm-radio/</link>
      <pubDate>Mon, 24 Oct 2022 23:33:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/10/24/gnuradio-fm-radio/</guid>
      <description>背景 以前买过 RTL-SDR，用 Gqrx 做过收音机，当时还给 Homebrew 尝试提交过几个 sdr 相关的 pr，但是限于知识的缺乏，后来就没有再继续尝试了。
前两天，@OceanS2000 讲了一次 Tunight: 高级收音机使用入门，又勾起了我的兴趣，所以我来尝试一下在 GNURadio Companion 中收听 FM 广播电台。
我没有上过无线电相关课程，所以下面有一些内容可能不正确或者不准确。
安装 我的实验环境是 NixOS，所以是用下面的配置来安装 gnuradio 的：
# SDR # https://github.com/NixOS/nixpkgs/pull/170253 (gnuradio.override { extraMakeWrapperArgs = [ &amp;#34;--prefix&amp;#34; &amp;#34;SOAPY_SDR_PLUGIN_PATH&amp;#34; &amp;#34;:&amp;#34; (soapyrtlsdr + &amp;#34;/lib/SoapySDR/modules0.8/&amp;#34;) ]; }) soapysdr-with-plugins 其中 gnuradio 的 override 是为了让它可以找到 soapyrtlsdr 的库，否则它会找不到设备；soapysdr-with-plugins 是为了提供 SoapySDRUtil 命令，来确认它可以找到 RTL-SDR 设备：
$ SoapySDRUtil --probe ---------------------------------------------------- -- Device identification ---------------------------------------------------- driver=RTLSDR hardware=R820T origin=https://github.com/pothosware/SoapyRTLSDR rtl=0 ---------------------------------------------------- -- Peripheral summary ---------------------------------------------------- Channels: 1 Rx, 0 Tx Timestamps: NO Other Settings: * Direct Sampling - RTL-SDR Direct Sampling Mode [key=direct_samp, default=0, type=string, options=(0, 1, 2)] * Offset Tune - RTL-SDR Offset Tuning Mode [key=offset_tune, default=false, type=bool] * I/Q Swap - RTL-SDR I/Q Swap Mode [key=iq_swap, default=false, type=bool] * Digital AGC - RTL-SDR digital AGC Mode [key=digital_agc, default=false, type=bool] ---------------------------------------------------- -- RX Channel 0 ---------------------------------------------------- Full-duplex: NO Supports AGC: YES Stream formats: CS8, CS16, CF32 Native format: CS8 [full-scale=128] Stream args: * Buffer Size - Number of bytes per buffer, multiples of 512 only.</description>
    </item>
    
    <item>
      <title>「教学」Wishbone 总线协议</title>
      <link>https://jia.je/hardware/2022/06/19/wishbone/</link>
      <pubDate>Sun, 19 Jun 2022 17:05:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/06/19/wishbone/</guid>
      <description>背景 最近在研究如何把 Wishbone 总线协议引入计算机组成原理课程，因此趁此机会学习了一下 Wishbone 的协议。
总线 总线是什么？总线通常用于连接 CPU 和外设，为了更好的兼容性和可复用性，会想到能否设计一个统一的协议，其中 CPU 实现的是发起请求的一方（又称为 master），外设实现的是接收请求的一方（又称为 slave），那么如果要添加外设、或者替换 CPU 实现，都会变得比较简单，减少了许多适配的工作量。
那么，我们来思考一下，一个总线协议需要包括哪些内容？对于 CPU 来说，程序会读写内存，读写内存就需要以下几个信号传输到内存：
地址（addr）：例如 32 位处理器就是 32 位地址，或者按照内存的大小计算地址线的宽度 数据（w_data 和 r_data）：分别是写数据和读数据，宽度通常为 32 位 或 64 位，也就是一个时钟周期可以传输的数据量 读还是写（we）：高表示写，低表示读 字节有效（be）：例如为了实现单字节写，虽然 w_data 可能是 32 位宽，但是实际写入的是其中的一个字节 除了请求的内容以外，为了表示 CPU 想要发送请求，还需要添加 valid 信号：高表示发送请求，低表示不发送请求。很多时候，外设的速度比较慢，可能无法保证每个周期都可以处理请求，因此外设可以提供一个 ready 信号：当 valid=1 &amp;amp;&amp;amp; ready=1 的时候，发送并处理请求；当 valid=1 &amp;amp;&amp;amp; ready=0 的时候，表示外设还没有准备好，此时 CPU 需要一直保持 valid=1 不变，等到外设准备好后，valid=1 &amp;amp;&amp;amp; ready=1 请求生效。
简单总结一下上面的需求，可以得到 master 和 slave 端分别的信号列表。这次，我们在命名的时候用 _o 表示输出、_i 表示输入，可以得到 master 端（CPU 端）的信号：</description>
    </item>
    
    <item>
      <title>「教学」异步 SRAM 时序</title>
      <link>https://jia.je/hardware/2022/05/19/async-sram-timing/</link>
      <pubDate>Thu, 19 May 2022 08:40:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/19/async-sram-timing/</guid>
      <description>背景 在一些场合里，我们会使用异步的（即没有时钟信号的）外部 SRAM 来存储数据，而我们经常使用的很多外部接口都是同步接口（即有时钟信号的接口），比如 SPI 和 I2C 等等，UART 虽然是异步，但是它速度很低，不怎么需要考虑时序的问题。所以在 FPGA 上编写一个正确的异步 SRAM 控制器是具有一定的挑战的。
寄存器时序 考虑到读者可能已经不记得寄存器的时序了，这里首先来复习一下 setup 和 hold 的概念。如果你已经比较熟悉了，可以直接阅读下一节。
寄存器在时钟的上升沿（下图的 a）进行采样，为了保证采样的稳定性，输入引脚 D 需要在时钟上升沿之前 \(t_{su}\) 的时刻（下图的 b）到时钟上升沿之后 \(t_h\) 的时刻（下图的 c）保持稳定，输出引脚 Q 会在时钟上升沿之后 \(t_{cko}\) 的时刻（下图的 d）变化：
接口 首先我们来看看异步 SRAM 的接口。下文中，采用 IS61WV102416BLL-10TLI 和 AS7C34098A-10TCN 作为例子：
可以看到，它有 20 位的地址，16 位的数据，若干个控制信号，同时只能进行读或者写（简称 1RW）。它没有时钟信号，所以是异步 SRAM。
时序 对于一个同步接口，我们通常只需要给一个满足时钟周期的时钟，然后通过约束文件保证 setup 和 hold 条件满足即可。但是对于异步接口，由于输出的时候没有时钟，我们需要更小心地完成这件事情。
读时序 首先来看一下比较简单的读时序：
可以看到地址和数据的关系：首先是地址需要稳定 \(t_{RC}\) 的时间，那么数据合法的范围是地址稳定的初始时刻加上 \(t_{AA}\)，到地址稳定的结束时刻加上 \(t_{OH}\)。我们再来看一下这几个时间的范围：
首先可以看到读周期时间 \(t_{RC}\) 至少是 10ns，这对应了型号中最后的数字，这表示了这个 SRAM 最快的读写速度。比较有意思的是 \(t_{AA}\) 最多是 10ns，刚好和 \(t_{RC}\) 的最小值相等。</description>
    </item>
    
    <item>
      <title>「教学」ACE 缓存一致性协议</title>
      <link>https://jia.je/hardware/2022/05/16/ace/</link>
      <pubDate>Mon, 16 May 2022 00:34:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/16/ace/</guid>
      <description>背景 最近几天分析了 TileLink 的缓存一致性协议部分内容，见TileLink 总线协议分析，趁此机会研究一下之前尝试过研究，但是因为缺少一些基础知识而弃坑的 ACE 协议分析。
下面主要参考了 IHI0022E 的版本，也就是 AXI4 对应的 ACE 版本。
回顾 首先回顾一下一个缓存一致性协议需要支持哪些操作。对于较上一级 Cache 来说，它需要这么几件事情：
读或写 miss 的时候，需要请求这个缓存行的数据，并且更新自己的状态，比如读取到 Shared，写入到 Modified 等。 写入一个 valid &amp;amp;&amp;amp; !dirty 的缓存行的时候，需要升级自己的状态，比如从 Shared 到 Modified。 需要 evict 一个 valid &amp;amp;&amp;amp; dirty 的缓存行的时候，需要把 dirty 数据写回，并且降级自己的状态，比如 Modified -&amp;gt; Shared/Invalid。如果需要 evict 一个 valid &amp;amp;&amp;amp; !dirty 的缓存行，可以选择通知，也可以选择不通知下一级。 收到 snoop 请求的时候，需要返回当前的缓存数据，并且更新状态。 需要一个方法来通知下一级 Cache/Interconnect，告诉它第一和第二步完成了。 如果之前看过我的 TileLink 分析，那么上面的这些操作对应到 TileLink 就是：
读或写 miss 的时候，需要请求这个缓存行的数据（发送 AcquireBlock，等待 GrantData），并且更新自己的状态，比如读取到 Shared，写入到 Modified 等。 写入一个 valid &amp;amp;&amp;amp; !</description>
    </item>
    
    <item>
      <title>向 Rocket Chip 添加自定义调试信号</title>
      <link>https://jia.je/hardware/2022/05/13/rocket-chip-custom-debug/</link>
      <pubDate>Fri, 13 May 2022 08:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/13/rocket-chip-custom-debug/</guid>
      <description>背景 最近在尝试把核心作为一个 Tile 加到 Rocket System 中，所以想要把核心之前自定义的调试信号接到顶层上去。Rocket System 自带的支持是 trace，也就是输出每个周期 retire 的指令信息，但和自定义的不大一样，所以研究了一下怎么添加自定义的调试信号，并且连接到顶层。
分析 Trace 信号连接方式 首先，观察 Rocket Chip 自己使用的 Trace 信号是如何连接到顶层的。在顶层上，可以找到使用的是 testchipip.CanHaveTraceIO:
trait CanHaveTraceIO { this: HasTiles =&amp;gt; val module: CanHaveTraceIOModuleImp // Bind all the trace nodes to a BB; we&amp;#39;ll use this to generate the IO in the imp val traceNexus = BundleBridgeNexusNode[Vec[TracedInstruction]]() val tileTraceNodes = tiles.flatMap { case ext_tile: WithExtendedTraceport =&amp;gt; None case tile =&amp;gt; Some(tile) }.map { _.</description>
    </item>
    
    <item>
      <title>「教学」内存认证算法</title>
      <link>https://jia.je/hardware/2022/05/10/memory-authentication/</link>
      <pubDate>Tue, 10 May 2022 20:28:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/10/memory-authentication/</guid>
      <description>背景 之前 @松 给我讲过一些内存认证（Memory Authentication）算法的内容，受益匪浅，刚好今天某硬件群里又讨论到了这个话题，于是趁此机会再学习和整理一下相关的知识。
内存认证计算的背景是可信计算，比如要做一些涉及重要数据的处理，从软件上，希望即使系统被攻击非法进入了，也可以保证重要信息不会泄漏；从硬件上，希望即使系统可以被攻击者进行一些物理的操作（比如导出或者修改内存等等），也可以保证攻击者无法读取或者篡改数据。
下面的内容主要参考了 Hardware Mechanisms for Memory Authentication: A Survey of Existing Techniques and Engines 这篇 2009 年的文章。
威胁模型 作为一个防御机制，首先要确定攻击方的能力。一个常见的威胁模型是认为，攻击者具有物理的控制，可以任意操控内存中的数据，但是无法读取或者修改 CPU 内部的数据。也就是说，只有 CPU 芯片内的数据是可信的，离开了芯片都是攻击者掌控的范围。一个简单的想法是让内存中保存的数据是加密的，那么怎样攻击者可以如何攻击加密的数据？下面是几个典型的攻击方法：
Spoofing attack：把内存数据改成任意攻击者控制的数据；这种攻击可以通过签名来解决 Splicing or relocation attack：把某一段内存数据挪到另一部分，这样数据的签名依然是正确的；所以计算签名时需要把地址考虑进来，这样地址变了，验证签名就会失败 Replay attack：如果同一个地址的内存发生了改变，攻击者可以把旧的内存数据再写进去，这样签名和地址都是正确的；为了防止重放攻击，还需要引入计数器或者随机 nonce Authentication Primitives 为了防御上面几种攻击方法，上面提到的文章里提到了如下的思路：
一是 Hash Function，把内存分为很多个块，每一块计算一个密码学 Hash 保存在片内，那么读取数据的时候，把整块数据读取进来，计算一次 Hash，和片内保存的结果进行比对；写入数据的时候，重新计算一次修改后数据的 Hash，更新到片内的存储。这个方法的缺点是没有加密，攻击者可以看到内容，只不过一修改就会被 CPU 发现（除非 Hash 冲突），并且存储代价很大：比如 512-bit 的块，每一块计算一个 128-bit 的 Hash，那就浪费了 25% 的空间，而片内空间是十分宝贵的。
二是 MAC Function，也就是密码学的消息验证码，它需要一个 Key，保存在片内；由于攻击者不知道密码，根据 MAC 的性质，攻击者无法篡改数据，也无法伪造 MAC，所以可以直接把计算出来的 MAC 也保存到内存里。为了防御重放攻击，需要引入随机的 nonce，并且把 nonce 保存在片内，比如每 512-bit 的数据，保存 64-bit 的 nonce，这样片内需要保存 12.</description>
    </item>
    
    <item>
      <title>TileLink 总线协议分析</title>
      <link>https://jia.je/hardware/2022/05/09/tilelink/</link>
      <pubDate>Mon, 09 May 2022 16:15:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/09/tilelink/</guid>
      <description>背景 最近在研究一些支持缓存一致性的缓存的实现，比如 rocket-chip 的实现和 sifive 的实现，因此需要研究一些 TileLink 协议。本文讨论的时候默认读者具有一定的 AXI 知识，因此很多内容会直接参考 AXI。
信号 根据 TileLink Spec 1.8.0，TileLink 分为以下三种：
TL-UL: 只支持读写，不支持 burst，类比 AXI-Lite TL-UH：支持读写，原子指令，预取，支持 burst，类比 AXI+ATOP（AXI5 引入的原子操作） TL-C：在 TL-UH 基础上支持缓存一致性协议，类比 AXI+ACE/CHI TileLink Uncached TileLink Uncached(TL-UL 和 TL-UH) 包括了两个 channel：
A channel: M-&amp;gt;S 发送请求，类比 AXI 的 AR/AW/W D channel: S-&amp;gt;M 发送响应，类比 AXI 的 R/W 因此 TileLink 每个周期只能发送读或者写的请求，而 AXI 可以同时在 AR 和 AW channel 上发送请求。
一些请求的例子：
读：M-&amp;gt;S 在 A channel 上发送 Get，S-&amp;gt;M 在 D channel 上发送 AccessAckData 写：M-&amp;gt;S 在 A channel 上发送 PutFullData/PutPartialData，S-&amp;gt;M 在 D channel 是发送 AccessAck 原子操作：M-&amp;gt;S 在 A channel 上发送 ArithmeticData/LogicalData，S-&amp;gt;M 在 D channel 上发送 AccessAckData 预取操作：M-&amp;gt;S 在 A channel 上发送 Intent，S-&amp;gt;M 在 D channel 上发送 AccessAck AXI4ToTL 针对 AXI4ToTL 模块的例子，来分析一下如何把一个 AXI4 Master 转换为 TileLink。</description>
    </item>
    
    <item>
      <title>试用沁恒 CH32V307 评估板</title>
      <link>https://jia.je/hardware/2022/04/19/wch-ch32v307-eval/</link>
      <pubDate>Tue, 19 Apr 2022 22:53:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/04/19/wch-ch32v307-eval/</guid>
      <description>背景 之前有一天看到朋友在捣鼓 CH32V307，因此自己也萌生了试用 CH32V307 评估板的兴趣，于是在沁恒官网申请样品，很快就接到电话了解情况，几天后就顺丰送到了，不过因为疫情原因直到现在才拿到手上，只能说疫情期间说不定货比人还快。
开箱 收到的盒子里有一个 CH32V307 评估板，和一个 WCH-Link，相关资料可以在 官网 或者 openwch/ch32v307 下载。在说明书中有如下的图示：
板子自带的跳线帽不是很多，建议自备一些，或者用杜邦线替代。比较重要的是 WCH-Link 子板上 CH549 和 CH2V307 连接的几个信号，和下面 BOOT0/1 的选择。
WCH-Link 可以看到评估板自带了一个 WCH-Link，所以不需要附赠的那一个，直接把 11 号 Type-C 连接到电脑上即可。这里还遇到一个小插曲，用 Type-C to Type-C 的线连电脑上不工作，连 PWR LED 都点不亮，换一根 Type-A to Type-C 的就可以，没有继续研究是什么原因。电脑上可以看到 WCH-Link 的设备：VID=1a86, PID=8010。比较有意思的是，在 RISC-V 模式（CON 灯不亮）的时候 PID 是 8010，ARM 模式（CON 灯亮）的时候 PID 是 8011，从 RISC-V 模式切换到 ARM 模式的方法是连接 TX 和 GND 后上电，反过来要用 MounRiver，详见 WCH-Link 使用说明 V1.0 V1.3 和原理图 V1.1。</description>
    </item>
    
    <item>
      <title>导出 Vivado 下载 Bitstream 的 SVF 文件</title>
      <link>https://jia.je/hardware/2022/04/10/vivado-program-bitstream-svf/</link>
      <pubDate>Sun, 10 Apr 2022 09:36:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/04/10/vivado-program-bitstream-svf/</guid>
      <description>背景 最近在研究如何实现一个远程 JTAG 的功能，目前实现在 jiegec/jtag-remote-server，实现了简单的 XVC 协议，底层用的是 libftdi 的 MPSSE 协议来操作 JTAG。但是，在用 Vivado 尝试的时候，SysMon 可以正常使用，但是下载 Bitstream 会失败，所以要研究一下 Vivado 都做了什么（目前已经修好，是最后一个字节的部分位读取的处理问题）。
SVF SVF 格式其实是一系列的 JTAG 上的操作。想到这个，也是因为在网上搜到了一个 dcfeb_v45.svf，里面描述的就是一段 JTAG 操作：
// Created using Xilinx Cse Software [ISE - 12.4] // Date: Mon May 09 11:00:32 2011 TRST OFF; ENDIR IDLE; ENDDR IDLE; STATE RESET; STATE IDLE; FREQUENCY 1E6 HZ; //Operation: Program -p 0 -dataWidth 16 -rs1 NONE -rs0 NONE -bpionly -e -loadfpga TIR 0 ; HIR 0 ; TDR 0 ; HDR 0 ; TIR 0 ; HIR 0 ; HDR 0 ; TDR 0 ; //Loading device with &amp;#39;idcode&amp;#39; instruction.</description>
    </item>
    
    <item>
      <title>浅谈乱序执行 CPU（二）</title>
      <link>https://jia.je/hardware/2022/03/31/brief-into-ooo-2/</link>
      <pubDate>Thu, 31 Mar 2022 01:18:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/31/brief-into-ooo-2/</guid>
      <description>背景 之前写过一个浅谈乱序执行 CPU，随着学习的深入，内容越来越多，页面太长，因此把后面的一部分内容独立出来，变成了这篇博客文章。之后也许会有（三）（四）等等。
内存访问 内存访问是一个比较复杂的操作，它涉及到缓存、页表、内存序等问题。在乱序执行中，要尽量优化内存访问对其他指令的延迟的影响，同时也要保证正确性。这里参考的是 BOOM 的 LSU 设计。
首先是正确性。一般来说可以认为，Load 是没有副作用的（实际上，Load 会导致 Cache 加载数据，这也引发了以 Meltdown 为首的一系列漏洞），因此可以很激进地预测执行 Load。但是，Store 是有副作用的，写出去的数据就没法还原了。因此，Store 指令只有在 ROB Head 被 Commit 的时候，才会写入到 Cache 中。
其次是性能，我们希望 Load 指令可以尽快地完成，这样可以使得后续的计算指令可以尽快地开始进行。当 Load 指令的地址已经计算好的时候，就可以去取数据，这时候，首先要去 Store Queue 里面找，如果有 Store 指令要写入的地址等于 Load 的地址，说明后面的 Load 依赖于前面的 Store，如果 Store 的数据已经准备好了，就可以直接把数据转发过来，就不需要从 Cache 中获取，如果数据还没准备好，就需要等待这一条 Store 完成；如果没有找到匹配的 Store 指令，再从内存中取。不过，有一种情况就是，当 Store 指令的地址迟迟没有计算出来，而后面的 Load 已经提前从 Cache 中获取数据了，这时候就会出现错误，所以当 Store 计算出地址的时候，需要检查后面的 Load 指令是否出现地址重合，如果出现了，就要把这条 Load 以及依赖这条 Load 指令的其余指令重新执行。POWER8 处理器微架构论文中对此也有类似的表述：
The POWER8 IFU also implements mechanisms to mitigate performance degradation associated with pipeline hazards.</description>
    </item>
    
    <item>
      <title>用 sv2v&#43;yosys 把 fpnew 转为 verilog 网表</title>
      <link>https://jia.je/hardware/2022/03/30/sv2v-fpnew/</link>
      <pubDate>Wed, 30 Mar 2022 00:33:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/30/sv2v-fpnew/</guid>
      <description>背景 FPnew 是一个比较好用的浮点计算单元，但它是 SystemVerilog 编写的，并且用了很多高级特性，虽然闭源软件是支持的，但是开源拖拉机经常会遇到这样那样的问题。所以一直想用 sv2v 把它翻译成 Verilog，但此时的 Verilog 还有很多复杂的结构，再用 yosys 转换为一个通用可综合的网表。
步骤 经过一系列踩坑，一个很重要的点是要用最新的 sv2v(v0.0.9-24-gf868f06) 和 yosys(0.15+70)。Debian 打包的 yosys 版本比较老，不能满足需求。
首先，用 verilator 进行预处理，把一堆 sv 文件合成一个：
$ cat a.sv b.sv c.sv &amp;gt; test.sv $ verilator -E test.sv &amp;gt; merged.sv $ sed -i &amp;#39;/^`line/d&amp;#39; merged.sv 注意这里用 sed 去掉了无用的行号信息。然后，用 sv2v 进行转换：
$ sv2v merged.sv &amp;gt; merge.v $ sed -i &amp;#39;/\$$fatal/d&amp;#39; merge.v 这里又用 sed 把不支持的 $fatal 去掉。最后，用 yosys 进行处理：
$ yosys -p &amp;#39;read_verilog -defer merge.</description>
    </item>
    
    <item>
      <title>Synopsys Design Compiler 综合实践</title>
      <link>https://jia.je/hardware/2022/03/14/design-compiler-synthesis/</link>
      <pubDate>Mon, 14 Mar 2022 15:22:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/14/design-compiler-synthesis/</guid>
      <description>工艺库 综合很重要的一步是把 HDL 的逻辑变成一个个单元，这些单元加上连接方式就成为了网表。那么，基本单元有哪些，怎么决定用哪些基本单元？
这个就需要工艺库了，工艺库定义了一个个单元，单元的引脚、功能，还有各种参数，这样 Design Compiler 就可以按照这些信息去找到一个优化的网表。
Liberty 格式 网上可以找到一些 Liberty 格式的工艺库，比如 Nangate45，它的设定是 25 摄氏度，1.10 伏，属于 TT（Typical/Typical）的 Process Corner。
在里面可以看到一些基本单元的定理，比如 AND2_X1，就是一个 drive strength 是 1 的二输入与门：
cell (AND2_X1) { drive_strength : 1; pin (A1) { direction : input; } pin (A2) { direction : input; } pin (ZN) { direction : output; function : &amp;#34;(A1 &amp;amp; A2)&amp;#34;; } /* ... */ } 这样就定义了两个输入 pin，一个输出 pin，还有它实现的功能。还有很重要的一点是保存了时序信息，比如：
lu_table_template (Timing_7_7) { variable_1 : input_net_transition; variable_2 : total_output_net_capacitance; index_1 (&amp;#34;0.</description>
    </item>
    
    <item>
      <title>OpenROAD Flow 初尝试</title>
      <link>https://jia.je/hardware/2022/03/12/try-openroad-flow/</link>
      <pubDate>Sat, 12 Mar 2022 22:52:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/12/try-openroad-flow/</guid>
      <description>背景 最近在尝试接触一些芯片前后端的知识。正好有现成的开源工具链 OpenROAD 来做这个事情，借此机会来学习一下整个流程。
尝试过程 首先 clone 仓库 OpenROAD-flow-scripts，然后运行：./build_openroad.sh，脚本会克隆一些仓库，自动进行编译。
编译中会找不到一些库，比如可能需要安装这些依赖：liblemon-dev libeigen3-dev libreadline-dev swig，此外运行的时候还需要 klayout 依赖。
如果遇到解决 cmake 找不到 LEMON 的问题，这是一个 BUG，可以运行下面的命令解决：
cd /usr/lib/x86_64-linux-gnu/cmake/lemon cp lemonConfig.cmake LEMONConfig.cmake 编译后整个目录大概有 4.8G，输出的二进制目录是 133M。
如果要跑一下样例里的 nangate45 工艺的 gcd 例子，运行：
cd flow make DESIGN_CONFIG=./designs/nangate45/gcd/config.mk 分析 GCD 测例 这个测例的代码提供了这样一个接口：
module gcd ( input wire clk, input wire [ 31:0] req_msg, output wire req_rdy, input wire req_val, input wire reset, output wire [ 15:0] resp_msg, input wire resp_rdy, output wire resp_val ); endmodule 从名字可以推断出，外部通过 req 发送请求到 GCD 模块，然后模块计算出 GCD 后再返回结果。</description>
    </item>
    
    <item>
      <title>通过 JTAG 对 VCU128 上的 Rocket Chip 进行调试</title>
      <link>https://jia.je/hardware/2022/03/09/rocket-chip-jtag-debug/</link>
      <pubDate>Wed, 09 Mar 2022 19:04:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/09/rocket-chip-jtag-debug/</guid>
      <description>前言 两年前，我尝试过用 BSCAN JTAG 来配置 Rocket Chip 的调试，但是这个方法不是很好用，具体来说，如果有独立的一组 JTAG 信号，配置起来会更方便，而且不用和 Vivado 去抢，OpenOCD 可以和 Vivado hw_server 同时运行和工作。但是，苦于 VCU128 上没有 PMOD 接口，之前一直没考虑过在 VCU128 上配置独立的 JTAG。然后最近研究了一下，终于解决了这个问题。
寻找 JTAG 接口 前几天在研究别的问题的时候，看到 VCU128 文档中的这段话：
The FT4232HL U8 multi-function USB-UART on the VCU128 board provides three level-shifted UART connections through the single micro-AB USB connector J2. • Channel A is configured in JTAG mode to support the JTAG chain • Channel B implements 4-wire UART0 (level-shifted) FPGA U1 bank 67 connections • Channel C implements 4-wire UART1 (level-shifted) FPGA U1 bank 67 connections • Channel D implements 2-wire (level-shifted) SYSCTLR U42 bank 501 connections 其中 Channel A 是到 FPGA 本身的 JTAG 接口，是给 Vivado 用的，如果是通过 BSCAN 的方式，也是在这个 Channel 上，但是需要经过 FPGA 自己的 TAP 再隧道到 BSCAN 上，比较麻烦。Channel B 和 C 是串口，Channel D 是连接 VCU128 上的 System Controller 的。之前的时候，都是直接用 Channel B 做串口，然后突发奇想：注意到这里是 4-wire UART，说明连接到 FPGA 是四条线，那是不是也可以拿来当 JTAG 用？</description>
    </item>
    
    <item>
      <title>分析 Rocket Chip 中 Diplomacy 系统</title>
      <link>https://jia.je/hardware/2022/01/05/diplomacy/</link>
      <pubDate>Wed, 05 Jan 2022 00:29:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/01/05/diplomacy/</guid>
      <description>概念 Diplomacy 主要实现了两个功能：
把整个总线结构在代码中表现出来 自动配置总线中各个端口的参数 具体来说，第一点实现了类似 Vivado Board Design 中连线的功能，第二点则是保证总线两端的参数一致，可以连接起来。
Diplomacy 为了表示总线的结构，每个模块可以对应一个 Node，Node 和 Node 之间连接形成一个图。Node 的类型主要有以下几个：
Client：对应 AXI 里面的 Master，发起请求 Manager：对应 AXI 里面的 Slave，处理请求 Adapter：对应 AXI Width Converter/Clock Converter/AXI4 to AXI3/AXI4 to AHB bridge 等，会修改 AXI 的参数，然后每个输入对应一个输出 Nexus：对应 AXI Crossbar，多个输入和多个输出 每个 Node 可能作为 Manager 连接上游的 Client，这个叫做入边（Inward Edge）；同样地，也可以作为 Client 连接下游的 Manager，这个是出边（Outward Edge）。想象成一个 DAG，从若干个 Client 流向 Manager。
连接方式采用的是 := :=* :*= :*=* 操作符，左侧是 Client，右侧是 Manager。
Rocket Chip 总线结构 Rocket Chip 主要有以下几个总线：</description>
    </item>
    
    <item>
      <title>Chisel3 Cookbook</title>
      <link>https://jia.je/hardware/2022/01/03/chisel3-cookbook/</link>
      <pubDate>Mon, 03 Jan 2022 22:19:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/01/03/chisel3-cookbook/</guid>
      <description>Chisel 版本选择 尽量选择较新版本的 Chisel。Chisel v3.5 完善了编译器插件，使得生成的代码中会包括更多变量名信息。
去掉输出 Verilog 文件中的寄存器随机初始化 版本：FIRRTL &amp;gt;= 1.5.0-RC2
代码：
new ChiselStage().execute( Array(&amp;#34;-X&amp;#34;, &amp;#34;verilog&amp;#34;, &amp;#34;-o&amp;#34;, s&amp;#34;${name}.v&amp;#34;), Seq( ChiselGeneratorAnnotation(genModule), CustomDefaultRegisterEmission( useInitAsPreset = false, disableRandomization = true ) ) ) 设置 disableRandomization=true 即可。useInitAsPreset 不建议开启。
关闭 FIRRTL 优化，输出尽可能与源代码一致的 Verilog 设置 Chisel 生成 MinimumVerilog：
new ChiselStage().execute( Array(&amp;#34;-X&amp;#34;, &amp;#34;mverilog&amp;#34;, &amp;#34;-o&amp;#34;, s&amp;#34;${name}.v&amp;#34;), Seq( ChiselGeneratorAnnotation(genModule) ) ) 此时代码中会保留更多原始 Chisel 代码的元素。
重命名 AXI4 为标准命名 Rocket Chip 中 AXI4Bundle 直接生成的名字和标准写法不同，可以利用 Chisel3 3.5.0 的 DataView 功能进行重命名：
// https://www.</description>
    </item>
    
    <item>
      <title>「教学」缓存一致性协议分析</title>
      <link>https://jia.je/hardware/2021/12/17/cache-coherency-protocol/</link>
      <pubDate>Fri, 17 Dec 2021 07:39:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/17/cache-coherency-protocol/</guid>
      <description>背景 最近在《高等计算机系统结构》课程中学习缓存一致性协议算法，这里用自己的语言来组织一下相关知识的讲解。
Write-invalidate 和 Write-update 最基础的缓存一致性思想有两种：
Write-invalidate：写入数据的时候，将其他 Cache 中这条 Cache Line 设为 Invalid Write-update：写入数据的时候，把新的结果写入到有这条 Cache Line 的其他 Cache Write-once 协议 Write-once 协议定义了四个状态：
Invalid：表示这个块不合法 Valid：表示这个块合法，并可能是共享的，同时数据没有修改 Reserved：表示这个块合法，不是共享的，同时数据没有更改 Dirty：表示这个块合法，不是共享的，数据做了修改，和内存不同。 可见，当一个缓存状态在 R 或者 D，其他缓存只能是 I；而缓存状态是 V 的时候，可以有多个缓存在 V 状态。
Write-once 协议的特点是，第一次写的时候，会写入到内存（类似 Write-through），连续写入则只写到缓存中，类似 Write-back。
当 Read hit 的时候，状态不变。
Read hit: The information is supplied by the current cache. No state change. 当 Read miss 的时候，会查看所有缓存，如果有其他缓存处于 Valid/Reserved/Dirty 状态，就从其他缓存处读取数据，然后设为 Valid，其他缓存也设为 Valid。如果其他缓存处于 Dirty 状态，还要把数据写入内存。
Read miss: The data is read from main memory.</description>
    </item>
    
    <item>
      <title>DRAM 在 Kintex 7 FPGA 上内部 Vref 的性能问题</title>
      <link>https://jia.je/hardware/2021/12/13/dram-fpga-vref-problem/</link>
      <pubDate>Mon, 13 Dec 2021 20:06:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/13/dram-fpga-vref-problem/</guid>
      <description>背景 最近我们设计的 Kintex 7 FPGA 开发板在测试 DDR SDRAM 的时候遇到了一个问题，因为采用了 Internel VREF，MIG 在配置的时候限制了频率只能是 400 MHz，对应 800 MT/s，这样无法达到 DDR 的最好性能。
原理 首先，VREF 在 DDR 中是用来区分低电平和高电平的。在 JESD79-4B 标准中，可以看到，对于直流信号，电压不小于 VREF+0.075V 时表示高电平，而电压不高于 VREF-0.075V 时表示低电平。VREF 本身应该介于 VDD 的 0.49 倍到 0.51 倍之间。
在连接 FPGA 的时候，有两种选择：
Internal VREF: 从 FPGA 输出 VREF 信号到 DRAM External VREF：接入 FPGA 以外的 VREF 对于 7 Series 的 FPGA，Xilinx 要求如下：
For DDR3 SDRAM interfaces running at or below 800 Mb/s (400 MHz), users have the option of selecting Internal VREF to save two I/O pins or using external VREF.</description>
    </item>
    
    <item>
      <title>「教学」DRAM 结构和特性</title>
      <link>https://jia.je/hardware/2021/12/12/dram/</link>
      <pubDate>Sun, 12 Dec 2021 15:06:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/12/dram/</guid>
      <description>DRAM 是如何组织的 DRAM 分成很多层次：Bank Group，Bank，Row，Column，从大到小，容量也是各级别的乘积。
举例子：
4 Bank Group 4 Bank per Bank Group 32,768 Row per Bank 1024 Column per Row 4 Bits per Column 那么总大小就是 4*4*32768*1024*4=2 Gb。
访问模式 DRAM 的访问模式决定了访问内存的实际带宽。对于每次访问，需要这样的操作：
用 ACT(Bank Activate) 命令打开某个 Bank Group 下面的某个 Bank 的某个 Row，此时整个 Row 的数据都会复制到 Sense Amplifier 中。这一步叫做 RAS（Row Address Strobe） 用 RD(Read)/WR(Write) 命令按照 Column 访问数据。这一步叫做 CAS（Column Address Strobe）。 在访问其他 Row 之前，需要用 PRE(Single Bank Precharge) 命令将 Sense Amplifier 中整个 Row 的数据写回 Row 中。 可以看到，如果访问连续的地址，就可以省下 ACT 命令的时间，可以连续的进行 RD/WR 命令操作。</description>
    </item>
    
    <item>
      <title>「教学」RISC-V Debug 协议</title>
      <link>https://jia.je/hardware/2021/12/12/riscv-debug/</link>
      <pubDate>Sun, 12 Dec 2021 14:01:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/12/riscv-debug/</guid>
      <description>背景 之前用过一些 RISC-V 核心，但是遇到调试相关的内容的时候就两眼一抹黑，不知道原理，出了问题也不知道如何排查，趁此机会研究一下工作原理。
架构 为了调试 RISC-V 核心，需要很多部件一起工作。按 RISC-V Debug Spec 所述，有这么几部分：
Debugger: GDB，连接到 OpenOCD 启动的 GDB Server Debug Translator: OpenOCD，向 GDB 提供 Server 实现，同时会通过 FTDI 等芯片控制 JTAG Debug Transport Hardware: 比如 FTDI 的芯片，可以提供 USB 接口，让 OpenOCD 控制 JTAG 信号 TMS/TDI/TCK 的变化，并读取 TDO Debug Transport Module: 在芯片内部的 JTAG 控制器（TAP），符合 JTAG 标准 Debug Module Interface：RISC-V 自定义的一系列寄存器，通过这些寄存器来控制 Debug Module 的行为 Debug Module：调试器，控制 RISC-V 核心，同时也支持直接访问总线，也有内部的 Program Buffer 可以看到，DMI 是实际的调试接口，而 JTAG 可以认为是一个传输协议。
JTAG 首先什么是 JTAG？简单来说，它工作流程是这样的：</description>
    </item>
    
    <item>
      <title>Manycore 处理器架构分析</title>
      <link>https://jia.je/hardware/2021/12/06/manycore/</link>
      <pubDate>Mon, 06 Dec 2021 00:11:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/06/manycore/</guid>
      <description>参考文档 Intel® Many Integrated Core Architecture (Intel® MIC Architecture) - Advanced Intel® Xeon Phi coprocessor (codename Knights Corner) https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=7453080 Knights Landing (KNL): 2nd Generation Intel® Xeon Phi™ Processor Fujitsu A64FX Fujitsu Presents Post-K CPU Specifications Fujitsu High Performance CPU for the Post-K Computer SUPERCOMPUTER FUGAKU - SUPERCOMPUTER FUGAKU, A64FX 48C 2.2GHZ, TOFU INTERCONNECT D Preliminary Performance Evaluation of the Fujitsu A64FX Using HPC Applications FUJITSU Processor A64FX NVIDIA A100 Tensor Core GPU Architecture NVIDIA TESLA V100 GPU ARCHITECTURE NVIDIA A100 TENSOR CORE GPU Xeon Phi - Intel MIC MIC: Many Integrated Core Architecture</description>
    </item>
    
    <item>
      <title>Sunway 处理器架构分析</title>
      <link>https://jia.je/hardware/2021/12/04/sunway/</link>
      <pubDate>Sat, 04 Dec 2021 09:13:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/04/sunway/</guid>
      <description>参考文档 高性能众核处理器申威 26010 稀疏矩阵向量乘法在申威众核架构上的性能优化 Sunway SW26010 The Sunway TaihuLight supercomputer: system and applications Report on the Sunway TaihuLight System Closing the “Quantum Supremacy” Gap: Achieving Real-Time Simulation of a Random Quantum Circuit Using a New Sunway Supercomputer SW_Qsim: A Minimize-Memory Quantum Simulator with High-Performance on a New Sunway Supercomputer 18.9-Pflops Nonlinear Earthquake Simulation on Sunway TaihuLight: Enabling Depiction of 18-Hz and 8-Meter Scenarios A FIRST PEEK AT CHINA’S SUNWAY EXASCALE SUPERCOMPUTER THE NITTY GRITTY OF THE SUNWAY EXASCALE SYSTEM NETWORK AND STORAGE Sunway supercomputer architecture towards exascale computing: analysis and practice SW26010 Sunway TaihuLight 的层次：</description>
    </item>
    
    <item>
      <title>移植系统到 Rocket Chip on VCU128</title>
      <link>https://jia.je/hardware/2021/10/18/port-system-to-rocket-chip-on-vcu128/</link>
      <pubDate>Mon, 18 Oct 2021 08:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/10/18/port-system-to-rocket-chip-on-vcu128/</guid>
      <description>背景 最近需要在 VCU128 上搭建一个 SOC，然后想到可以把 OpenSBI、U-Boot 和 Linux 移植到这个平台上方便测试，于是又开始折腾这些东西。代码仓库都已经开源：
rocket-chip-vcu128 opensbi u-boot linux Rocket Chip on VCU128 第一部分是基于之前 rocket2thinpad 在 Thinpad 上移植 Rocket Chip 的经验，做了一些更新，主要是因为 VCU128 的外设不大一样，同时我也要运行更复杂的程序，主要做了这些事情：
添加了 VCU128 的内存和外设：HBM、SPI、I2C、UART、ETH 打开了更多核心选项：S-mode 和 U-mode 主要踩过的坑：
BSCAN 不工作，估计是因为一些参数不对，@jsteward 之前在 zcu 平台上做了一些测试，估计要用类似的办法进行修改；我最后直接去掉了这部分逻辑 这个板子的 PHY RESET 信号要通过 I2C 接口访问 TI 的 Port Expander，所以没法直接连，要通过 gpio 输出来手动 reset SPI Startup Flash 的时序配置，见我之前的博客 Xilinx PCS/PMA IP 也会自己挂一个设备到 MDIO bus 上，应该有自己的 PHY 地址，而不要和物理的 PHY 冲突 U-Boot 在 U-Boot 上花了比较多的时间，用它的目的主要是：</description>
    </item>
    
    <item>
      <title>「教学」AXI Quad SPI 时序分析</title>
      <link>https://jia.je/hardware/2021/09/27/xilinx-axi-quad-spi-timing/</link>
      <pubDate>Mon, 27 Sep 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/09/27/xilinx-axi-quad-spi-timing/</guid>
      <description>背景 之前一直没搞懂 Vivado 中 xdc 需要怎么编写，遇到一些必须要写 xdc 的时候就很头疼，不知道怎么写才可以得到正确的结果。今天分析了一下 AXI Quad SPI 的时序 xdc，终于理解了其中的含义。
AXI Quad SPI AXI Quad SPI 是一个 SPI 的控制器，它支持 XIP（eXecute In Place）模式，即可以暴露一个只读 AXI Slave 接口，当接收到读请求的时候，就按照标准的 SPI Flash 命令去对应的地址进行读取，然后返回结果。由于不同厂家的 SPI Flash 支持有所不同，所以 IP 上的设置可以看到厂家的选择。
特别地，一个常见的需求是希望访问 Cfg（Configuration）Flash，亦即用来保存 Bitstream 的 Flash。当 FPGA 上电的时候，如果启动模式设置为 SPI Flash，FPGA 就会向 Cfg Flash 读取 Bitstream，Cfg Flash 需要连接到 FPGA 的指定引脚上，当 FPGA 初始化的时候由内部逻辑驱动，初始化完成后又要转交给用户逻辑。转交的方式就是通过 STARTUP 系列的 primitive。
通常，如果要连接外部的 SPI Flash，需要连接几条信号线到顶层，然后通过 xdc 把信号绑定到引脚上，然后引脚连接了一个外部的 SPI Flash。但由于 Cfg Flash 比较特殊，所以信号从 AXI Quad SPI 直接连到 STARTUP 系列的 primitive 上。如果是采用 STARTUPE2 原语的 7 系列的 FPGA，那么只有时钟会通过 STARTUPE2 pritimive 连接到 SPI Flash 上，其他数据信号还是正常通过顶层绑定；如果是采用 STARTUPE3 原语的 UltraScale 系列的 FPGA，那么时钟和数据都通过 STARTUPE3 primitive 连接到 SPI Flash。</description>
    </item>
    
    <item>
      <title>浅谈乱序执行 CPU</title>
      <link>https://jia.je/hardware/2021/09/14/brief-into-ooo/</link>
      <pubDate>Tue, 14 Sep 2021 13:47:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/09/14/brief-into-ooo/</guid>
      <description>背景 最早学习乱序执行 CPU 的时候，是在 Wikipedia 上自学的，后来在计算机系统结构课上又学了一遍，但发现学的和现在实际采用的乱序执行 CPU 又有很大区别，后来又仔细研究了一下，觉得理解更多了，就想总结一下。
经典 Tomasulo 参考 Stanford 教材
经典 Tomasulo，也是 Wikipedia 上描述的 Tomasulo 算法，它的核心是保留站。指令在 Decode 之后，会被分配到一个保留站中。保留站有以下的这些属性：
Op：需要执行的操作 Qj，Qk：操作数依赖的指令目前所在的保留站 ID Vj，Qk：操作数的值 Rj，Rk：操作数是否 ready（或者用特殊的 Qj，Qk 值表示是否 ready） Busy：这个保留站被占用 此外还有一个 mapping（Wikipedia 上叫做 RegisterStat），记录了寄存器是否会被某个保留站中的指令写入。
指令分配到保留站的时候，会查询 RegisterStat，得知操作数寄存器是否 ready，如果不 ready，说明有一个先前的指令要写入这个寄存器，那就记录下对应的保留站 ID。当操作数都 ready 了，就可以进入计算单元计算。当一条指令在执行单元中完成的时候，未出现 WAW 时会把结果写入寄存器堆，并且通过 Common Data Bus 进行广播，目前在保留站中的指令如果发现，它所需要的操作数刚好计算出来了，就会把取值保存下来。
这里有一些细节：因为保留站中的指令可能要等待其他指令的完成，为了保证计算单元利用率更高，对于同一个计算类型（比如 ALU），需要有若干个同类的保留站（比如 Add1，Add2，Add3）。在 Wikipedia 的表述中，每个保留站对应了一个计算单元，这样性能比较好，但自然面积也就更大。如果节省面积，也可以减少计算单元的数量，然后每个计算单元从多个保留站中获取计算的指令。
可以思考一下，这种方法的瓶颈在什么地方。首先，每条指令都放在一个保留站中，当保留站满的时候就不能发射新的指令。其次，如果计算单元的吞吐跟不上保留站的填充速度，也会导致阻塞。
这种方法的一个比较麻烦的点在于难以实现精确异常。精确异常的关键在于，异常之前的指令都生效，异常和异常之后的指令不生效，但这种方法无法进行区分。
从寄存器重命名的角度来看，可以认为这种方法属于 Implicit Register Renaming，也就是说，把 Register 重命名为保留站的 ID。
再分析一下寄存器堆需要哪些读写口。有一条规律是，寄存器堆的面积与读写口个数的平方成正比。对于每条发射的指令，都需要从寄存器堆读操作数，所以读口是操作数 x 指令发射数。当执行单元完成计算的时候，需要写入寄存器堆，所以每个执行单元都对应一个寄存器堆的写口。
硬件实现的时候，为了性能，希望保留站可以做的比较多，这样可以容纳更多的指令。但是，保留站里面至少要保存操作数的值，会比较占用面积，并且时延也比较大。
ROB (ReOrder Buffer) 参考教材</description>
    </item>
    
    <item>
      <title>硬盘相关的概念</title>
      <link>https://jia.je/hardware/2021/05/06/disk/</link>
      <pubDate>Thu, 06 May 2021 11:37:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/05/06/disk/</guid>
      <description>ATA ATA 定义了发送给硬盘的命令，标准定义了命令：
ech IDENTIFY DEVICE: 获取设备信息 25h READ DMA EXT: 读取扇区 35h WRITE DMA EXT: 写入扇区 ATA 同时也是接口，图片如下。ATA 前身是 IDE，现在 ATA 叫做 PATA。
AHCI AHCI 可以简单理解为 PCIe &amp;lt;-&amp;gt; SATA 的转换器。AHCI 暴露为一个 PCIe 设备：
$ lspci -vv 00:1f.2 SATA controller: Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller (rev 05) Kernel modules: ahci 处理器通过 IO port/MMIO 访问 AHCI，然后 AHCI HBA 连接到 SATA 设备。
SATA SATA 一般说的是接口。它一般分为两个部分，数据和电源。数据部分只有 7 个 pin，三个 GND 和两对差分线（A+A- B+B-），图片如下：</description>
    </item>
    
    <item>
      <title>Linksys E8450 OpenWRT 配置 w/ 802.11ax</title>
      <link>https://jia.je/hardware/2021/03/18/linksys-e8450-openwrt/</link>
      <pubDate>Thu, 18 Mar 2021 12:25:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/03/18/linksys-e8450-openwrt/</guid>
      <description>背景 之前用的 newifi 路由器（Lenovo y1s）无线网总是出问题，于是换了一个新的支持 802.11ax 的路由器 Linksys E8450，目前在 openwrt snapshot 支持。Openwrt 的支持页面：Linksys E8450。
过程 按照支持页面，下载固件：
$ wget https://downloads.openwrt.org/snapshots/targets/mediatek/mt7622/openwrt-mediatek-mt7622-linksys_e8450-squashfs-sysupgrade.bin 然后访问固件升级页面：http://192.168.1.1/config-admin-firmware.html#firmware，选择下载的 bin 文件。点击“开始升级”，然后等待。一段时间后，ssh 到路由器：
$ ssh root@192.168.1.1 The authenticity of host &amp;#39;192.168.1.1 (192.168.1.1)&amp;#39; can&amp;#39;t be established. ED25519 key fingerprint is SHA256:REDACTED. No matching host key fingerprint found in DNS. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added &amp;#39;192.</description>
    </item>
    
    <item>
      <title>PCB 笔记</title>
      <link>https://jia.je/hardware/2021/03/08/pcb-notes/</link>
      <pubDate>Mon, 08 Mar 2021 00:03:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/03/08/pcb-notes/</guid>
      <description>记录一下在学习画板子过程中学到的心得。
工具 目前使用过 KiCad 和 lceda：
KiCad: 开源软件，跨平台。 lceda：在线编辑，不需要安装，和 lcsc 有深度集成。 项目 jiegec/HT42B534USB2UART 采用的是 KiCad 5 编写的。目前正在做的另一个项目采用 lceda
流程 选择所需要使用的芯片，查找芯片的 datasheet。 寻找采用了芯片的一些设计，特别是看 schematic。 按照 datasheet 里面推荐的电路，或者是其他人的设计，画自己需要的 schematic。 设置好各个元件的 footprint，然后转到 PCB 设计。 在 PCB 里面布线，生成 Gerber 等文件。 把 Gerber 给到生产商（比如 jlc），交付生产。 如果是自己焊接，则需要购买元件，比如从 lcsc 购买。 收到 PCB 和元件后，自己按照 BOM 和 schematic 焊接各个元件。 笔记 对于一些连接很多元件的信号，比如 GND，可以留作铺铜解决。也就是说，先不管 GND，把其他所有的信号都接好以后，再在顶层铺铜；如果还是有没有连接上的 GND，可以通过过孔（Via）走到底层，在底层再铺一层铜。 对于外部供电的 VCC 和 GND，在 KiCad 中需要用 PWR_FLAG 标记一下。 在 KiCad 中设计 PCB 前，要把生产商的工艺参数设置好，不然画了也要重画。 lceda 在选择元件的时候，可以直接从 lcsc 里选择，这样可以保证封装和商品可以对得上，不需要手动进行匹配。 如果要用 jlc 的 SMT 贴片，先在 SMT 元件列表 里搜索所需要的元件；推荐用基本库，如果用其他库，则要加钱；选好元件以后，用元件编号去 lceda 里搜索并添加到 schematic。 对于涉及模拟信号的设计，比如音频，需要特别注意模拟信号的电和地都是单独的：AVCC 和 AGND。所以要特别注意 datasheet 里面不同的地的表示方法。最后，再用磁珠把 VCC 和 AVCC、GND 和 AGND 分别连接起来就可以了。可以参考 DE2 板子中第 19 页的音频部分设计 和 Staying well grounded。 在 schematic 里经常会出现在电源附近的电容，那么，在 PCB 中，也尽量把这些电容放在对应的电源的旁边。 耳机插座里面，一般分三种组成部件：Tip，Ring，Sleeve。只有两段的是 TS，三段的是 TRS，四段的是 TRRS。TS 是单声道，T 是声音，S 是地。TRS 是双声道，T 是左声道（或者单声道），R 是右声道，S 是地。TRRS 则是双声道加录音。一般来说，LINE IN 是双声道，MIC IN 是单声道，它们的阻抗也不同；LINE OUT 和 HEADPHONE OUT 都是双声道，但 HEADPHONE OUT 经过了额外的放大器。 遇到一个 SPI 协议没有 SPI_MISO 引脚的芯片，可能说明它是 write-only 的。 手焊的基本元件，一般用 0603 加一些 Padding 的封装；SMT 的话，则建议用 0402 封装。 I2C 的信号线一般需要加一个几 K 欧姆的上拉电阻到 VCC。 未完待续。</description>
    </item>
    
    <item>
      <title>Skid Buffer</title>
      <link>https://jia.je/hardware/2021/01/26/skid-buffer/</link>
      <pubDate>Tue, 26 Jan 2021 20:34:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/01/26/skid-buffer/</guid>
      <description>Skid buffer Skid buffer 指的就是，对于 valid + ready 的握手信号，用空间（更多的逻辑）来换取时间（更好的时序）的一个硬件模块。
简单来说，背景就是，为了解决 valid 和 ready 信号在数据流水线上一路经过组合逻辑导致的时序问题，在中途加上一些寄存器来阻隔。当然了，代价就是延迟和面积，不过吞吐量还是需要保持的。
由于需求的不同，Skid buffer 也有不同的实现。目前，找到了四个实现，实现上有所不同，特性也不大一样。
统一约定 由于我在 SpinalHDL 语言中重新实现了下面的这些 Skid buffer，所以按照 SpinalHDL 的 Stream 定义接口：
class SkidBufferCommon[T &amp;lt;: Data]( gen: =&amp;gt; T ) extends Component { val io = new Bundle { val s = slave(Stream(gen)) val m = master(Stream(gen)) } } 在这里，io.s 表示从上游取的数据，io.m 表示传递给下游的数据。
输出信号共有：io.s.ready、io.m.valid 和 io.m.payload。
ZipCPU 版本 第一个版本来自 ZipCPU：
博客地址：Building a Skid Buffer for AXI processing 代码地址：skidbuffer.</description>
    </item>
    
    <item>
      <title>以太网的物理接口</title>
      <link>https://jia.je/hardware/2020/12/27/ethernet-physical-interfaces/</link>
      <pubDate>Sun, 27 Dec 2020 08:46:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/12/27/ethernet-physical-interfaces/</guid>
      <description>背景 最近逐渐接触到了一些高速的以太网的接口，被一大堆的名字搞得有点懵，所以特意学习了一下并整理成这篇博客。
更新：经 @z4yx 指出，还可以看华为的介绍文档
几几 BASE 杠什么是什么意思 在下文里，经常可以看到类似 100BASE-TX 这种写法，它表示的意思是：
BASE 前面的数字表示速率，比如 10，100，1000，10G 等等 BASE 之后的第一个字母，常见的 T 表示双绞线，S 表示 850nm 光纤，L 表示 1310nm 光纤，C 表示同轴电缆 之后可能还有别的字母，比如 X 表示 8b/10b 或者 4b/5b（FE）的编码，R 表示 64b/66b 的编码 之后可能还有别的数字，如果是 LAN PHY 表示的是所使用的 lane 数量；如果是 WAN PHY 表示的是传输的公里数 详见 Wikipedia - Ethernet Physical Layer # Naming Conventions 和 IEEE 802.3 1.2.3 节 Physical Layer and media notation：
The data rate, if only a number, is in Mb/s, and if suffixed by a “G”, is in Gb/s.</description>
    </item>
    
    <item>
      <title>ARM M1 MacBook Air 开箱</title>
      <link>https://jia.je/hardware/2020/11/19/arm-m1-macbookair/</link>
      <pubDate>Thu, 19 Nov 2020 18:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/11/19/arm-m1-macbookair/</guid>
      <description>购买 我是 11.12 的时候在 Apple Store 上下单的，选的是 MacBookAir，带 M1 芯片，8 核 CPU + 8 核 GPU，加了一些内存和硬盘。今天（11.19）的时候顺丰到货，比 Apple Store 上显示的预计到达时间 21-28 号要更早。另外，我也听朋友说现在一些线下的店也有货，也有朋友直接在京东上买到了 Mac mini，总之第一波 M1 的用户最近应该都可以拿到设备了。
现在这篇博客，就是在 ARM MBA 上编写的，使用的是 Intel 的 VSCode，毕竟 VSCode 的 ARM64 版不久后才正式发布。
开箱 从外观来看，一切都和 Intel MBA 一样，包装上也看不出区别，模具也是一样的。
进了系统才能看得出区别。预装的系统是 macOS Big Sur 11.0，之后手动更新到了目前最新的 11.0.1。
顺带 @FactorialN 同学提醒我在这里提一句：包装里有电源适配器，不太环保。
体验 ARM64 首先自然是传统艺能，证明一下确实是 Apple Silicon：
$ uname -a Darwin macbookair.lan 20.1.0 Darwin Kernel Version 20.1.0: Sat Oct 31 00:07:10 PDT 2020; root:xnu-7195.</description>
    </item>
    
    <item>
      <title>FIDO U2F、FIDO2 和 CTAP 的关系</title>
      <link>https://jia.je/hardware/2020/05/18/fido-u2f-fido2-ctap/</link>
      <pubDate>Mon, 18 May 2020 10:30:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/05/18/fido-u2f-fido2-ctap/</guid>
      <description>背景 2012 年，Yubico 和 Google 设计了 U2F 协议，第二年 U2F 成为 FIDO 组织的标准，之后加入了 NFC 的支持。之后，FIDO2 作为替代 U2F 的新标准产生，原来的 U2F 以兼容的方式成为了 CTAP1，而采用 CBOR 封装格式的 CTAP(CTAP2) 则是 FIDO2 的主要协议。
U2F 命令格式 U2F 定义了它的命令格式，基于 ISO7816-4 APDU（short APDU） ：
CLA INS P1 P2 Lc data Le 1 byte 1 byte 1 byte 1 byte 0-1 bytes variable length 0-1 bytes 比如 U2F_VERSION 就是：
CLA INS P1 P2 Lc data Le 00 03 00 00 0 empty 00 返回的数据就是 U2F_V2 的 ASCII 加上 9000 的状态。</description>
    </item>
    
    <item>
      <title>MIFARE Classic 上配置 NDEF</title>
      <link>https://jia.je/hardware/2020/05/10/mifare-classic-ndef/</link>
      <pubDate>Sun, 10 May 2020 09:19:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/05/10/mifare-classic-ndef/</guid>
      <description>背景 最近买了一堆 NFC 的智能卡拿来测试，其中一张 MIFARE Classic 的总是在 iOS 上读不出来，无论是以 Tag 模式还是 NDEF 模式。于是通过一系列的研究，终于知道上怎么一回事，然后成功地把一个 MIFARE Classic 卡配置成了 NDEF。
背景知识 NFC 有很多协议，其中 MIFARE Classic 基于 ISO 14443-3 Type A 标准，里面有一些 MIFARE 的命令。通过这些命令，就可以控制 MIFARE Classic 卡的内容。具体来说，以我使用的 MIFARE Classic EV1 4K S70 为例，这篇文章会涉及到如下的背景知识：
MIFARE Classic 内存布局 在 MIFARE Classic 中，有 Sector 和 Block 的概念，每个 Sector 有若干个 Block，其中最后一个 Block 是特殊的（称为 Sector Trailer），保存了这个 Sector 的一些信息：Key A、Access Bits、GPB 和 Key B。对于 Classic 4K，首先是 32 个有 4 blocks 的 sector，然后是 8 个 有 16 blocks 的 sector，整体的内存布局大概是：</description>
    </item>
    
    <item>
      <title>在命令行中进行 Vivado 仿真</title>
      <link>https://jia.je/hardware/2020/04/04/vivado-simulation-command/</link>
      <pubDate>Sat, 04 Apr 2020 18:50:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/04/04/vivado-simulation-command/</guid>
      <description>想要在命令行里进行 Vivado 仿真，所以查了下 Xilinx 的 UG900 文档，找到了命令行仿真的方法。首先是生成仿真所需的文件：
# assuming batch mode open_project xxx.xpr set_property top YOUR_SIM_TOP [current_fileset -simset] export_ip_user_files -no_script -force export_simulation -simulator xsim -force 可以把这些语句放到 tcl 文件里然后用 batch mode 执行。执行成功以后，会在 export_sim/xsim 目录下生成一些文件。里面会有生成的脚本以供仿真：
cd export_sim/xsim &amp;amp;&amp;amp; ./YOUR_SIM_TOP.sh 默认情况下它会执行 export_sim/xsim/cmd.tcl 里面的命令。如果想要记录 vcd 文件，修改内容为：
open_vcd log_vcd run 20us close_vcd quit 这样就可以把仿真的波形输出到 dump.vcd 文件，拖到本地然后用 GTKWave 看。更多支持的命令可以到 UG900 里找。</description>
    </item>
    
    <item>
      <title>在 Rocket Chip 上挂接 TLRAM</title>
      <link>https://jia.je/hardware/2020/03/17/rocket-chip-tlram-load/</link>
      <pubDate>Tue, 17 Mar 2020 23:20:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/03/17/rocket-chip-tlram-load/</guid>
      <description>最近遇到一个需求，需要在 Rocket Chip 里面开辟一块空间，通过 verilog 的 $readmemh 来进行初始化而不是用 BootROM，这样每次修改内容不需要重新跑一次 Chisel -&amp;gt; Verilog 的流程。然后到处研究了一下，找到了解决的方案：
首先是新建一个 TLRAM 然后挂接到 cbus 上：
import freechips.rocketchip.tilelink.TLRAM import freechips.rocketchip.tilelink.TLFragmenter import freechips.rocketchip.diplomacy.LazyModule import freechips.rocketchip.diplomacy.AddressSet trait HasTestRAM { this: BaseSubsystem =&amp;gt; val testRAM = LazyModule( new TLRAM(AddressSet(0x40000000, 0x1FFF), beatBytes = cbus.beatBytes) ) testRAM.node := cbus.coupleTo(&amp;#34;bootrom&amp;#34;) { TLFragmenter(cbus) := _ } } 这里的地址和大小都可以自由定义。然后添加到自己的 Top Module 中：
class TestTop(implicit p:Parameters) extends RocketSystem // ... with HasTestRAM //... { override lazy ... } 实际上这时候 TLRAM 就已经加入到了 TileLink 总线中。接着，为了让 firrtl 生成 $readmemh 的代码，需要两个步骤：</description>
    </item>
    
    <item>
      <title>在 Vivado 中对 chisel3 产生的 verilog 代码仿真</title>
      <link>https://jia.je/hardware/2020/02/10/simulate-chisel3-on-vivado/</link>
      <pubDate>Mon, 10 Feb 2020 23:09:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/10/simulate-chisel3-on-vivado/</guid>
      <description>默认情况下，chisel3 生成的 verilog 代码在 Vivado 中仿真会出现很多信号大面积变成 X。解决方法在一个不起眼的 Wiki 页面：Randomization flags：
`define RANDOMIZE_REG_INIT `define RANDOMIZE_MEM_INIT `define RANDOMIZE_GARBAGE_ASSIGN `define RANDOMIZE_INVALID_ASSIGN 在生成的 verilog 前面加上这四句，就可以正常仿真了。</description>
    </item>
    
    <item>
      <title>通过 BSCAN JTAG 对 Rocket Chip 进行调试</title>
      <link>https://jia.je/hardware/2020/02/10/rocket-chip-bscan-debug/</link>
      <pubDate>Mon, 10 Feb 2020 15:08:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/10/rocket-chip-bscan-debug/</guid>
      <description>前言 在上一个 post 里研究了原理，今天也是成功在 Artix 7 上实现了调试。效果如下：
OpenOCD 输出：
Info : JTAG tap: riscv.cpu tap/device found: 0x0362d093 (mfg: 0x049 (Xilinx), part: 0x362d, ver: 0x0) Info : datacount=1 progbufsize=16 Info : Disabling abstract command reads from CSRs. Info : Examined RISC-V core; found 1 harts Info : hart 0: XLEN=32, misa=0x40801105 Info : Listening on port 3333 for gdb connections GDB 输出：
Remote debugging using localhost:3333 0x0001018c in getc () at bootloader.</description>
    </item>
    
    <item>
      <title>研究 Rocket Chip 的 BSCAN 调试原理</title>
      <link>https://jia.je/hardware/2020/02/09/rocket-chip-bscan-analysis/</link>
      <pubDate>Sun, 09 Feb 2020 15:11:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/09/rocket-chip-bscan-analysis/</guid>
      <description>前言 最近 @jsteward 在研究如何通过 JTAG 对 FPGA 里的 Rocket Chip 进行调试。之前 @sequencer 已经做了一些实践，我们在重复他的工作，同时也研究了一下这是怎么工作的。
原理 我们从 @sequencer 得到了一份可用的 Scala 代码 和 OpenOCD 配置，并且了解到：
可以通过 openocd 找到并调试 Rocket Chip openocd 是通过 JTAG 向 FPGA 的 TAP 的 IR 写入 USER4，然后往 DR 写入特定格式的数据，然后控制 Rocket Chip 的 JTAG。 这里涉及到一个“封装”的过程，在一个仅可以控制 DR 的 JTAG 中控制另一个 JTAG。首先可以找到 OpenOCD 端的操作代码：
tunneled_ir[3].num_bits = 3; tunneled_ir[3].out_value = bscan_zero; tunneled_ir[3].in_value = NULL; tunneled_ir[2].num_bits = bscan_tunnel_ir_width; tunneled_ir[2].out_value = ir_dtmcontrol; tunneled_ir[1].in_value = NULL; tunneled_ir[1].</description>
    </item>
    
    <item>
      <title>在 macOS 烧写 Artix7 FPGA</title>
      <link>https://jia.je/hardware/2020/02/09/program-artix7-on-macos/</link>
      <pubDate>Sun, 09 Feb 2020 00:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/09/program-artix7-on-macos/</guid>
      <description>首先安装好 openocd：
brew install openocd --HEAD
测试所用版本为 0.10.0+dev-01052-g09580964 (2020-02-08-15:09)。
然后编写如下的 openocd.cfg：
adapter driver ftdi adapter speed 10000 ftdi_vid_pid 0x0403 0x6014 ftdi_layout_init 0x0008 0x004b source [find cpld/xilinx-xc7.cfg] init xc7_program xc7.tap pld load 0 /path/to/bitstream.bit shutdown 上面的 ftdi 开头的两行按照实际的 JTAG Adapter 修改。可以参考 openocd 自带的一些 cfg。
然后在 openocd.cfg 的目录运行 openocd 即可：
$ openocd Open On-Chip Debugger 0.10.0+dev-01052-g09580964 (2020-02-08-15:09) Licensed under GNU GPL v2 For bug reports, read http://openocd.org/doc/doxygen/bugs.html Info : auto-selecting first available session transport &amp;#34;jtag&amp;#34;.</description>
    </item>
    
    <item>
      <title>用 PulseView 配合 DSLogic 调试 SPI Flash</title>
      <link>https://jia.je/hardware/2019/08/02/pulseview-dslogic/</link>
      <pubDate>Fri, 02 Aug 2019 23:15:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/08/02/pulseview-dslogic/</guid>
      <description>最近需要用到逻辑分析仪来调试 SPI Flash，设备是 DreamSourceLab 的 DSLogic，最开始用的是官方的 DSView，确实能够抓到 SPI 的信号，也可以解析出一些 SPI Flash 的数据，但是很多是不完整的。
后来把源码下载下来，发现是基于 sigrok 和 PulseView 做的一个魔改版，然后 sigrok 官网上最新的版本已经支持了 DSLogic，于是就用 PulseView 替代 DSView。一开始遇到的问题是没有 firmware，一番搜索找到了解决方案，按照脚本下载好文件即可。
进到 PulseView 以后，把 SPI 的四路信号接上，然后抓了一段信号，解析：
可以看到它正确地解析出来了 Fast Read 命令。由于 DSView 它 fork 自一个比较老的版本，所以它并不能正确解析出来。
P.S. Linux 下它界面显示比 macOS 下好看一些，估计是没有适配好。</description>
    </item>
    
    <item>
      <title>在 FPGA 上实现路由器（3）</title>
      <link>https://jia.je/hardware/2019/06/02/router-on-fpga-3/</link>
      <pubDate>Sun, 02 Jun 2019 09:24:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/06/02/router-on-fpga-3/</guid>
      <description>前言 又半个月过去了，在写了上篇系列博文之后也是做了很多新的更改。上次做的主要是关于性能方面的提升，怎么提高频率，从而达到比较大的流量，而这段时间做的则是功能，做实现 RIP 协议和转发表的动态更新。
软件部分 软件部分目前是用 C 代码写的，用 Xilinx SDK 提供的各个 AXI 外设的驱动和 PS 自己的驱动，实现了所需要的，RIP 协议的处理，转发表的更新和统计信息的读取。
实际上做的时候比较粗暴，主要是通过三种 AXI 外设与硬件部分进行交互：AXI Stream FIFO，AXI GPIO 和 AXI BRAM Controller。其中 AXI Stream FIFO 是用来接收和发送需要 CPU 处理的以太网帧的，AXI GPIO 则是用来读取统计的信息，AXI BRAM Controller 是用来读写转发表的。最后在顶层设计中把这些外设连接起来。
硬件部分 硬件部分还是继续之前的部分往下写，添加了统计信息，直接暴露出去，让 CPU 走 AXI GPIO 读，因为不需要很高的精确度；转发表本身，一开始想的是自己写一些接口转换，后来发现，直接用 True Dual Port RAM 然后把一个 port 暴露给 AXI BRAM Controller 即可，免去了各种麻烦，PS 可以直接进行修改，不需要额外的工作。
最终效果 为了测试这套东西是否正常工作，就开了两个 Arch Linux 的虚拟机，分别 Bridge 到两个千兆的 USB 网卡上，都连到 FPGA 上。然后在两边都配上了 BIRD，配置 RIP 和一些路由，确实能更新硬件的转发表，并两边的 RIP 可以学习到对方的路由。</description>
    </item>
    
    <item>
      <title>在 FPGA 上实现路由器（2）</title>
      <link>https://jia.je/hardware/2019/05/15/router-on-fpga-2/</link>
      <pubDate>Wed, 15 May 2019 20:39:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/05/15/router-on-fpga-2/</guid>
      <description>前言 月初的时候，有了一个完整可用的路由器（上一篇系列博文），但当时测了一下速度，只有几十 Mb/s，只要往上提就会失效，得 reset 才能继续。当时也先没管性能的事情，先把和 OS 交互的部分做了。现在又回头来做性能调优。
之前，逻辑部分的主频只有 10 MHz，这自然不行，不提高肯定做不到千兆。于是试着把主频拉高，FIFO 加大，然后遇到了很多问题，慢慢修复了，学到了很多新知识，目前也接近千兆的水平了吧，贴图：
TCP 测速：
UDP 测速：
测试环境是 macOS 虚拟机外打虚拟机内，走网桥把虚拟机和一个 USB 网卡接起来，然后从另一个 USB 网卡打到路由器。
尝试 700Mb/s 接下来讲讲，在这个过程中遇到了什么问题，怎么解决的。第一个是速度过快就会挂，这肯定是丢包逻辑没写对，后来在仿真里开够了时间，于是就找到了一个 BUG，其实就是一行的修复。接着就是提高主频，但大家也知道，CPU 不能随便超频，由于各种延迟的原因，比如 Setup 时间，如果超了一个时钟周期的时间，本来应该下个周期就得到新数据的，结果到了下下周期才有，那有的状态可能就乱了，我目前遇到的也主要就是这个问题。
于是就对着 Timing 里汇报的各种问题修啊修，发现了很多以前没有注意到的问题，它们不影响功能，但是会让逻辑变慢。第一个问题是 High Fanout，以上就是说一个输出接到了很多输入，这看起来没啥问题，但数设课上也讲过，每个门的输入输出电流是有限制的，例如按书上的数据，一个门输出只能带十个门，更多只能级联一层。级联的话，延迟自然就高了。后来发现，这里的原因是，开了一个大的数组，但是没有变成 RAM，综合出了几千个逻辑单元，自然是出问题。解决方法很简单，用 xpm_memory_tpdram 即可。这样一搞，主频就能上 200MHz 了。
这个时候测了一下，发现 UDP 能打到 700Mb/s 了，TCP 由于丢包率比较高，只有 400Mb/s，距离预期还有一段距离。于是继续进行优化。
向 900Mb/s 进发 要继续提速，自然要提高主频。下一个主频目标就是 250MHz。随着提高主频，时序的要求也会更高，自然也出现了新的问题。
这次的问题主要在于，一个路径上逻辑门数过多，多的有 7 到 10 个，每一步零点几到一点几纳秒，叠起来 4 纳秒哪里够用。于是把一些不需要依赖条件的逻辑挪到条件外面，这样就减少了一些路径的依赖。
解决了这个以后，现在的 WNS（Worst Negative Slack）只剩下 0.6 ns 了。这时候的问题一部分还是来自于逻辑门过多，但这个时候就没这么简单了，只能继续细化流水线，打一拍，这样才能把延迟降下来。
虽然 Timing 没有完全解决，但还是写进了 FPGA 中。幸好工作一切正常，就得到了上面那个图片的结果，接近千兆的速度了。</description>
    </item>
    
    <item>
      <title>在 FPGA 上实现路由器</title>
      <link>https://jia.je/hardware/2019/04/24/router-on-fpga/</link>
      <pubDate>Wed, 24 Apr 2019 19:41:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/04/24/router-on-fpga/</guid>
      <description>最近在做 FPGA 上硬件的路由器，感觉接近一个基本可用的阶段了吧，大概谈一谈做这个的思路、过程和踩过的坑。
首先，做实验用的板子是 Alinx AX7021，FPGA 是 Xilinx xc7z020clg484-2，扩展板上有 4PL+1PS 个网口和千兆 KSZ9031RNX PHY，采用的接口是 RGMII。一开始做的自然是做 RGMII，但是遇到了困难，RGMII 在千兆模式下传输的是 DDR 信号，而时序和延迟就是个比较麻烦的事情。一开始先直接拿 Xilinx 的 AXI Ethernet IP 来用，然后上 ILA 看到了 IDDR 后的信号，第一次看到了完整的以太网帧，从 Preamble 和 SFD 到最后的 FCS。于是就特别振奋，想着手写 RGMII，先做收，再做发。确实，收很容易，很快就做出来了，但是写总是出问题，当时也不懂跨时钟域的一些问题，总之各种没调出来。于是就退而求其次，选择了 Xilinx 的 Tri Mode Ethernet IP 了。
Tri Mode Ethernet IP 有很多选项，为了简单，直接采用了 AXI-Stream 的接口，不要 AXI4-Lite 什么的，都不要，因为我需要直接写剩余的逻辑。其他东西能省也都省掉了。这个 IP 确实很给力，很快就可以完成收和发的操作了，这次终于知道了怎么处理跨时钟域的问题 — XPM FIFO ASYNC，一下推进了很大的进度。
既然可以收，也可以发了，就扩展到多个网口。这个 IP 中可以选择 Shared Logic 在内部，也可以在外部，研究了一下发现，应该是一个放内部，其余选外部，然后接起来就可以了。不过目前为了简单，还是只用了俩端口。在这个基础上，就开始解析收进来的以太网帧了。
第一步自然是填 ARP 表，自然问题来了，如果多个网口同时进来数据，怎么保证 ARP 表读写的正确性？自然就想到总线上需要做仲裁，于是写了一个简单的总线仲裁，顺带学习到了 unique case(z) 和 priority case(z) 的语法。然后 ARP 表怎么实现呢，大概就是一个哈希表，然后表里维护了（IP，MAC，PORT）三元组，然后实现了一些冲突和覆盖的处理逻辑，做这些的同时也对各个模块编写相应的测试。有了 ARP 表，就可以在解析以太网帧的时候，拆解出里面的信息，然后请求 ARP 表总线，然后写入。</description>
    </item>
    
    <item>
      <title>向咸鱼派写入 ArchlinuxARM</title>
      <link>https://jia.je/hardware/2018/11/06/archlinuxarm-on-sfpi/</link>
      <pubDate>Tue, 06 Nov 2018 19:18:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2018/11/06/archlinuxarm-on-sfpi/</guid>
      <description>之前由于我的 macOS 上不知道为啥不能把我的 TF 卡设备放到我的虚拟机里，所以之前就没能刷 ArchLinuxARM 上去。今天我想到了一个方法，完成了这件时期：
$ wget https://mirrors.tuna.tsinghua.edu.cn/archlinuxarm/os/ArchLinuxARM-armv7-latest.tar.gz $ dd if=/dev/zero of=archlinuxarm.img bs=1M count=1024 $ mkfs.ext4 archlinuxarm.img $ sudo mkdir -p /mnt/archlinuxarm $ sudo mount -o loop archlinuxarm.img /mnt/archlinuxarm $ sudo bsdtar -xpf ArchLinuxARM-armv7-latest.tar.gz -C /mnt/archlinuxarm $ sudo umount /mnt/archlinuxarm 这样就获得了一个 ext4 的 ArchlinuxARM 镜像。刚好解压出来不到 1G，所以开了 1G 的镜像刚好放得下。然后把 archlinuxarm.img 拷回 macOS，然后用 dd 写进去：
$ sudo dd if=archlinuxarm.img of=/dev/rdisk4s2 bs=1048576 这时候可以确认，我们确实是得到了一个正确的 ext4fs：
$ sudo /usr/local/opt/e2fsprogs/sbin/tune2fs -l /dev/disk4s2 不过，我们实际的分区大小可能不止 1G，所以可以修改一下大小：</description>
    </item>
    
    <item>
      <title>咸鱼派的启动配置</title>
      <link>https://jia.je/hardware/2018/11/05/salted-fish-pi/</link>
      <pubDate>Mon, 05 Nov 2018 22:17:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2018/11/05/salted-fish-pi/</guid>
      <description>最近刚拿到了一个咸鱼派的测试板子，准备自己把 U-Boot 和 Linux 内核这一套东西跑通，都用主线的东西，尽量减少魔改的部分。首先是编译 u-boot，我用的是现在的 master 分支的最新版 99431c1c：
$ # Archlinux $ sudo pacman -Sy arm-none-eabi-gcc $ make LicheePi_Zero_defconfig $ make ARCH=arm CROSS_COMPILE=arm-none-eabi- -j24 这时候会得到一个 u-boot-sunxi-with-spl.bin 的文件。我们只要把它写到 SD 卡的 8192 偏移处，就可以把 U-Boot 跑起来了：
$ diskutil unmountDisk /dev/disk4 $ sudo dd if=u-boot-sunxi-with-spl.bin of=/dev/disk4 bs=1024 seek=8 接着我们做一下分区。我采用的是 MBR 分区，这样保证不会和 U-Boot 冲突。使用 fdisk 进行分区，我从 1M 处开始分了一个 10M 的 FAT-32 分区作为启动分区，然后之后都是 EXT4 的系统盘分区。接着就是编译内核。
我用的是八月份时候的 4.18.2 内核，虽然不是很新但也足够新了。一番调整内核参数后，得到了一个可用的内核，然后把 zImage 和 sun8i-v3s-licheepi-zero.dtb 都复制到刚才创建的 FAT-32 启动分区，然后进入 U-Boot 进行启动：</description>
    </item>
    
    <item>
      <title>在荔枝糖（Lichee Tang）上初次体验 FPGA</title>
      <link>https://jia.je/hardware/2018/10/07/my-first-fpga-experience-on-lichee-tang/</link>
      <pubDate>Sun, 07 Oct 2018 22:34:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2018/10/07/my-first-fpga-experience-on-lichee-tang/</guid>
      <description>今天从张宇翔学长那拿到了 荔枝糖（Lichee Tang） 的 FPGA 板子，于是立即开始把前段时间学到的 Verilog 应用上来。不过想到现在我手上没有多少外设，然后又必须远程到 Windows 电脑上去操作，于是先实现了一下 UART 通信。
在网上找到了 ben-marshall/uart 一个简易的实现，很快做到了一直在串口上打印 A 字符。接着我开始尝试实现一个简单的串口回显。一开始，我直接把 UART 读到的数据直接输出，果然可以了，但是一旦传输速率跟不上了，就会丢失数据。于是我添加了 FIFO IP 核，然后把读入的数据存入 FIFO，又从 FIFO 中读取数据写入到 UART 中去。不过发现了一个小 BUG：每次打印的是倒数第二次输入的字符，即丢失了第一个字符。在张宇翔学长的帮助下找到了问题：当 FIFO 的读使能信号为高时，其数据在下一个时钟周期才来，于是解决方案就是等到数据来的时候再向 UART 中写数据：
always @ (posedge clk_in) begin uart_tx_en &amp;lt;= uart_fifo_re; end 这样就解决了这个问题。完整代码在 jiegec/learn_licheetang 中。</description>
    </item>
    
    <item>
      <title>在 macOS 上读取移动硬盘的 S.M.A.R.T. 信息</title>
      <link>https://jia.je/hardware/2018/09/07/reading-smart-info-of-external-drives-under-macos/</link>
      <pubDate>Fri, 07 Sep 2018 10:20:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2018/09/07/reading-smart-info-of-external-drives-under-macos/</guid>
      <description>之前想看看自己各个盘的情况，但是发现只能看电脑内置的 SSD 的 S.M.A.R.T 信息，而移动硬盘的都显示：
$ smartctl -a /dev/disk2 smartctl 6.6 2017-11-05 r4594 [Darwin 17.7.0 x86_64] (local build) Copyright (C) 2002-17, Bruce Allen, Christian Franke, www.smartmontools.org /dev/disk2: Unable to detect device type Please specify device type with the -d option. Use smartctl -h to get a usage summary 一开始我怀疑是个别盘不支持，但换了几个盘都不能工作，问题应该出现在了 USB 上。查了下资料，果然如此。根据 USB devices and smartmontools ，获取 S.M.A.R.T 信息需要直接发送 ATA 命令，但是由于经过了 USB，于是需要进行一个转换，导致无法直接发送 ATA 命令。这个问题自然是有解决方案，大概就是直接把 ATA 命令发送过去（pass-through）。上面这个地址里写到，如果需要在 macOS 上使用，需要安装一个内核驱动。可以找到，源码在 kasbert/OS-X-SAT-SMART-Driver 并且有一个带签名的安装包在 External USB / FireWire drive diagnostics support 中可以下载。丢到 VirusTotal 上没查出问题，用 v0.</description>
    </item>
    
  </channel>
</rss>
