<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Devops on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/devops/</link>
    <description>Recent content in Devops on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jun 2021 20:29:00 +0800</lastBuildDate><atom:link href="https://jia.je/devops/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>将 k8s rook ceph 集群迁移到 cephadm</title>
      <link>https://jia.je/devops/2021/06/25/ceph-k8s-to-external/</link>
      <pubDate>Fri, 25 Jun 2021 20:29:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/06/25/ceph-k8s-to-external/</guid>
      <description>背景 前段时间用 rook 搭建了一个 k8s 内部的 ceph 集群，但是使用过程中遇到了一些稳定性问题，所以想要用 cephadm 重建一个 ceph 集群。
重建过程 重建的时候，我首先用 cephadm 搭建了一个 ceph 集群，再把原来的 MON 数据导入，再恢复各个 OSD。理论上，可能有更优雅的办法，但我还是慢慢通过比较复杂的办法解决了。
cephadm 搭建 ceph 集群 首先，配置 TUNA 源，在各个节点上安装 docker-ce 和 cephadm。接着，在主节点上 bootstrap：
cephadm bootstrap --mon-ip HOST1_IP 此时，在主节点上会运行最基础的 ceph 集群，不过此时还没有任何数据。寻找 ceph 分区，会发现因为 FSID 不匹配而无法导入。所以，首先要恢复 MON 数据。
参考文档：cephadm install。
恢复 MON 数据 首先，关掉 rook ceph 集群，找到留存下来的 MON 数据目录，默认路径是 /var/lib/rook 下的 mon-[a-z] 目录，找到最新的一个即可。我把目录下的路径覆盖到 cephadm 生成的 MON 目录下，然后跑起来，发现有几个问题：
 cephadm 生成的 /etc/ceph/ceph.client.admin.keyring 与 MON 中保存的 auth 信息不匹配，导致无法访问 FSID 不一致，而 cephadm 会将各个设置目录放到 /var/lib/ceph/$FSID 下  第一个问题的解决办法就是临时用 MON 目录下的 keyring 进行认证，再创建一个新的 client.</description>
    </item>
    
    <item>
      <title>在 ESXi 中用 PERCCli 换 RAID 中的盘</title>
      <link>https://jia.je/devops/2021/04/15/vmware-esxi-perccli/</link>
      <pubDate>Thu, 15 Apr 2021 14:31:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/04/15/vmware-esxi-perccli/</guid>
      <description>背景 最近有一台机器的盘出现了报警，需要换掉，然后重建 RAID5 阵列。iDRAC 出现报错：
 Disk 2 in Backplane 1 of Integrated RAID Controller 1 is not functioning correctly. Virtual Disk 1 on Integrated RAID Controller 1 has become degraded. Error occurred on Disk2 in Backplane 1 of Integrated RAID Controller 1 : (Error 2)  安装 PERCCli 首先，因为系统是 VMware ESXi 6.7，所以在DELL 官网下载对应的文件。按照里面的 README 安装 vib：
esxcli software vib install -v /vmware-perccli-007.1420.vib 需要注意的是，如果复制上去 Linux 版本的 PERCCli，虽然也可以运行，但是找不到控制器。安装好以后，就可以运行 /opt/lsi/perccli/perccli 。接着，运行 perccli show all，可以看到类似下面的信息：</description>
    </item>
    
    <item>
      <title>用 fluentd 收集 k8s 中容器的日志</title>
      <link>https://jia.je/devops/2021/04/02/k8s-fluentd-log-collect/</link>
      <pubDate>Fri, 02 Apr 2021 23:19:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/04/02/k8s-fluentd-log-collect/</guid>
      <description>背景 在维护一个 k8s 集群的时候，一个很常见的需求就是把日志持久化存下来，一方面是方便日后回溯，一方面也是聚合 replicate 出来的同一个服务的日志。
在我目前的需求下，只需要把日志持久下来，还不需要做额外的分析。所以我并没有部署类似 ElasticSearch 的服务来对日志进行索引。
实现 实现主要参考官方的仓库：https://github.com/fluent/fluentd-kubernetes-daemonset。它把一些常用的插件打包到 docker 镜像中，然后提供了一些默认的设置，比如获取 k8s 日志和 pod 日志等等。为了达到我的需求，我希望：
 每个结点上有一个 fluentd 收集日志，forward 到单独的 log server 上的 fluentd log server 上的 fluentd 把收到的日志保存到文件  由于 log server 不由 k8s 管理，所以按照官网的方式手动安装：
$ curl -L https://toolbelt.treasuredata.com/sh/install-debian-buster-td-agent4.sh | sh 然后，编辑配置 /etc/td-agent/td-agent.conf：
&amp;lt;source&amp;gt; @type forward @id input_forward bind x.x.x.x &amp;lt;/source&amp;gt; &amp;lt;match **&amp;gt; @type file path /var/log/fluentd/k8s compress gzip &amp;lt;buffer&amp;gt; timekey 1d timekey_use_utc true timekey_wait 10m &amp;lt;/buffer&amp;gt; &amp;lt;/match&amp;gt; 分别设置输入：监听 fluentd forward 协议；输出：设置输出文件，和 buffer 配置。如有需要，可以加鉴权。</description>
    </item>
    
    <item>
      <title>用 gitlab ci 构建并部署应用到 k8s</title>
      <link>https://jia.je/devops/2021/03/16/gitlab-ci-k8s-integration/</link>
      <pubDate>Tue, 16 Mar 2021 08:41:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/03/16/gitlab-ci-k8s-integration/</guid>
      <description>背景 在 k8s 集群中部署了 gitlab-runner，并且希望在 gitlab ci 构建完成后，把新的 docker image push 到 private repo，然后更新应用。
参考文档：Gitlab CI 与 Kubernetes 的结合，Using Docker to build Docker images。
在 gitlab ci 中构建 docker 镜像 这一步需要 DinD 来实现在容器中构建容器。为了达到这个目的，首先要在 gitlab-runner 的配置中添加一个 volume 来共享 DinD 的证书路径：
gitlabUrl: REDACTED rbac: create: true runnerRegistrationToken: REDACTED runners: config: |[[runners]] [runners.kubernetes] image = &amp;#34;ubuntu:20.04&amp;#34; privileged = true [[runners.kubernetes.volumes.empty_dir]] name = &amp;#34;docker-certs&amp;#34; mount_path = &amp;#34;/certs/client&amp;#34; medium = &amp;#34;Memory&amp;#34; privileged: true 注意两点：1. privileged 2.</description>
    </item>
    
    <item>
      <title>常用交换机命令</title>
      <link>https://jia.je/devops/2021/03/12/switch-config/</link>
      <pubDate>Fri, 12 Mar 2021 11:35:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/03/12/switch-config/</guid>
      <description>背景 最近接触了 Cisco，DELL，Huawei，H3C，Ruijie 的网络设备，发现配置方式各有不同，故记录一下各个厂家的命令。
Huawei 测试型号：S5320
保存配置 &amp;lt;HUAWEI&amp;gt;save The current configuration will be written to flash:/vrpcfg.zip. Are you sure to continue?[Y/N]y Now saving the current configuration to the slot 0.... Save the configuration successfully. 进入配置模式 &amp;lt;HUAWEI&amp;gt; system-view 查看当前配置 [HUAWEI] display current-configuration 查看 LLDP 邻居 [HUAWEI]display lldp neighbor brief 查看 CDP 邻居 [HUAWEI]display cdp neighbor brief 启用 LLDP [HUAWEI]lldp enable 启用 CDP [HUAWEI-XGigabitEthernet0/0/1]lldp compliance cdp txrx 启用只读 SNMPv1 community [HUAWEI]snmp-agent sys-info version all Warning: This command may cause confliction in netconf status.</description>
    </item>
    
    <item>
      <title>用 k3s 部署 k8s</title>
      <link>https://jia.je/devops/2021/03/12/k3s-deploy/</link>
      <pubDate>Fri, 12 Mar 2021 00:41:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/03/12/k3s-deploy/</guid>
      <description>背景 最近需要部署一个 k8s 集群，觉得之前配置 kubeadm 太繁琐了，想要找一个简单的。比较了一下 k0s 和 k3s，最后选择了 k3s。
配置 k3s 的好处就是配置十分简单：https://rancher.com/docs/k3s/latest/en/quick-start/。不需要装 docker，也不需要装 kubeadm。
 在第一个 node 上跑：curl -sfL https://get.k3s.io | sh - 在第一个 node 上获取 token：cat /var/lib/rancher/k3s/server/node-token 在其他 node 上跑：curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -  然后就搞定了。从第一个 node 的 /etc/rancher/k3s/k3s.yaml获取 kubectl 配置。</description>
    </item>
    
    <item>
      <title>通过 rook 在 k8s 上部署 ceph 集群</title>
      <link>https://jia.je/devops/2021/03/12/ceph-on-k8s/</link>
      <pubDate>Fri, 12 Mar 2021 00:41:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/03/12/ceph-on-k8s/</guid>
      <description>背景 为了方便集群的使用，想在 k8s 集群里部署一个 ceph 集群。
Ceph 介绍
Ceph 有这些组成部分：
 mon：monitor mgr：manager osd：storage mds(optional)：用于 CephFS radosgw(optional：用于 Ceph Object Storage  配置 我们采用的是 rook 来部署 ceph 集群。
参考文档：https://rook.github.io/docs/rook/v1.5/ceph-examples.html
首先，克隆 rook 的仓库。建议选择一个 release 版本。
接着，运行下面的命令：
sudo apt install -y lvm2 # required kubectl apply -f rook/cluster/examples/kubernetes/ceph/crds.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/common.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/operator.yaml # debugging only kubectl apply -f rook/cluster/examples/kubernetes/ceph/toolbox.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/direct-mount.yaml # CephFS kubectl apply -f rook/cluster/examples/kubernetes/ceph/filesystem.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.</description>
    </item>
    
    <item>
      <title>在裸机上部署 ESXi 和 vCSA 7</title>
      <link>https://jia.je/devops/2020/10/18/deploy-esxi-vcsa-7/</link>
      <pubDate>Sun, 18 Oct 2020 00:08:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/10/18/deploy-esxi-vcsa-7/</guid>
      <description>之前在另一篇文章里提到过 vCSA 的安装，这次又在另一台机器上重新做了一遍，特此记录一下。
首先在官网上下载 ESXi+VCSA 7.0 ，应该得到两个文件：
7.9G VMware-VCSA-all-7.0.1-16860138.iso 358M VMware-VMvisor-Installer-7.0U1-16850804.x86_64.iso 首先安装 ESXi，用 UNetBootin 制作 ESXi 的安装光盘。注意不能用 dd，因为它是 CDFS 格式的，不能直接boot。启动以后，按照界面要求，一路安装即可。
接着，就可以用网页访问 ESXi 进行配置。比如安装一些 Linux 发行版，然后在 Linux 虚拟机里面 mount 上面的 VCSA 的 iso：
sudo mount /dev/sr0 /mnt 接着，复制并修改 /mnt/vcsa-cli-installer/templates/install/embedded_vCSA_on_ESi.json，按照代码注释进行修改。需要注意几点：
 密码都可以设为空，然后运行 cli 的时候输入 ESXi 的密码和 vCSA 的密码是不一样的 可以把 ceip 关掉，设置 ceip_enabled: false  接着，进行安装：
/mnt/vcsa-cli-installer/lin64/vcsa-deploy install --accept-eula /path/to/customized.json -v 慢慢等待它安装成功即可。
安装完成后，进入 vCSA，新建一个 Datacenter，然后选择新建的 Datacenter，选择 Add host，输入 ESXi 的地址和用户密码信息即可。</description>
    </item>
    
    <item>
      <title>在 Rpi4 上运行 buildroot</title>
      <link>https://jia.je/devops/2020/09/12/buildroot-rpi-pxe/</link>
      <pubDate>Sat, 12 Sep 2020 08:21:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/09/12/buildroot-rpi-pxe/</guid>
      <description>背景 需要给 rpi 配置一个 pxe 的最小环境，在上一篇博文了提到可以用 alpine，但发现有一些不好用的地方，所以试了试 buildroot。
PXE 设置和路由器设置 见 “在 Rpi4 上运行 Alpine Linux” 文章。
Buildroot 配置 下载 buildroot ：
&amp;gt; wget https://buildroot.org/downloads/buildroot-2020.08.tar.gz &amp;gt; unar buildroot-2020.08.tar.gz &amp;gt; cd buildroot-2020.08 &amp;gt; make raspberrypi4_64_defconfig 然后运行 make menuconfig ，在 Filesystem images 中打开 initramfs ，并设置 cpio 压缩为 gz 。然后直接编译：
&amp;gt; make -j4 $ ls -al target/images bcm2711-rpi-4-b.dtb* boot.vfat Image rootfs.cpio rootfs.cpio.gz rootfs.ext2 rootfs.ext4@ rpi-firmware/ sdcard.img 接着，在一个单独的目录里，把这些文件整理一下
&amp;gt; cd ~/rpi-buildroot &amp;gt; cp -r ~/buildroot-2020.</description>
    </item>
    
    <item>
      <title>在 rpi4 上用 PXE 运行 Alpine Linux</title>
      <link>https://jia.je/devops/2020/09/11/alpine-rpi-pxe/</link>
      <pubDate>Fri, 11 Sep 2020 23:42:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/09/11/alpine-rpi-pxe/</guid>
      <description>背景 需要给 rpi 配置一个 pxe 的最小环境，然后看到 alpine 有 rpi 的支持，所以尝试给 rpi4 配置 alpine 。
PXE 设置 第一步是设置 rpi4 的启动模式，打开 BOOT UART 并且打开 网络启动：
&amp;gt; cd /lib/firmware/raspberrypi/bootloader/critical &amp;gt; rpi-eeprom-config pieeprom-2020-04-16.bin &amp;gt; config.txt $ cat config.txt [all] BOOT_UART=1 WAKE_ON_GPIO=1 POWER_OFF_ON_HALT=0 DHCP_TIMEOUT=45000 DHCP_REQ_TIMEOUT=4000 TFTP_FILE_TIMEOUT=30000 TFTP_IP= TFTP_PREFIX=0 BOOT_ORDER=0x12 SD_BOOT_MAX_RETRIES=3 NET_BOOT_MAX_RETRIES=5 [none] FREEZE_VERSION=0 &amp;gt; sed &amp;#39;s/BOOT_UART=0/BOOT_UART=1/;s/BOOT_ORDER=0x1/BOOR_ORDER=0x21/&amp;#39; config.txt &amp;gt; config-pxe.txt &amp;gt; rpi-eeprom-config --out pieeprom-2020-04-16-pxe.bin --config config-pxe.txt pieeprom-2020-04-16.bin &amp;gt; rpi-eeprom-update -d -f pieeprom-2020-04-16.pxe.bin &amp;gt; reboot 重启以后，可以用 vcgencmd bootloader_config 查看当前的启动配置，看是否正确地更新了启动配置。比较重要的是 BOOT_ORDER，0x21 表示先尝试网络启动，再尝试 SD 卡启动。</description>
    </item>
    
    <item>
      <title>用 certbot 申请 route53 上的域名的 LetsEncrypt 证书并上传到 IAM</title>
      <link>https://jia.je/devops/2020/08/12/certbot-route53-letsencrypt-iam/</link>
      <pubDate>Wed, 12 Aug 2020 09:46:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/08/12/certbot-route53-letsencrypt-iam/</guid>
      <description>最近遇到了 AWS Certificate Manager 的一些限制，所以只能用 IAM 证书。于是上网找到了通过 certbot 申请 LE 证书，通过 route53 API 验证的方法。
首先配置 aws 的 credential。然后，按照 certbot：
pip3 install -U certbot certbot_dns_route53 然后，就可以申请证书了：
certbot certonly --dns-route53 --config-dir &amp;#34;./letsencrypt&amp;#34; --work-dir &amp;#34;./letsencrypt&amp;#34; --logs-dir &amp;#34;./letsencrypt&amp;#34; -d example.com --email a@b.com --agree-tos 如果申请成功，在当前目录下可以找到证书。然后上传到 IAM：
aws iam upload-server-certificate --server-certificate-name NameHere \  --certificate-body file://letsencrypt/live/example.com/cert.pem \  --private-key file://letsencrypt/live/example.com/privkey.pem \  --certificate-chain file://letsencrypt/live/example.com/chain.pem \  --path /cloudfront/ 如果要用于 cloudfront，才需要最后的路径参数；否则可以去掉。这样就完成了 IAM 证书的上传。</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 Prometheus</title>
      <link>https://jia.je/devops/2020/07/10/k8s-prometheus/</link>
      <pubDate>Fri, 10 Jul 2020 09:24:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/07/10/k8s-prometheus/</guid>
      <description>实验了一下在 k8s 中部署 Prometheus，因为它和 k8s 有比较好的集成，很多 App 能在 k8s 里通过 service discovery 被 Prometheus 找到并且抓取数据。实践了一下，其实很简单。
用 helm 进行配置：
helm upgrade --install prometheus stable/prometheus 这样就可以了，如果已经有 StorageClass （比如腾讯云的话，CBS 和 CFS），它就能自己起来了，然后在 Lens 里面也可以看到各种 metrics 的可视化。
如果是自建的单结点的 k8s 集群，那么还需要自己创造 PV，并且把 PVC 绑定上去。我采用的是 local 类型的 PV：
apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume-1 labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &amp;#34;/srv/k8s-data-1&amp;#34; --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume-2 labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &amp;#34;/srv/k8s-data-2&amp;#34; 这样，结点上的两个路径分别对应两个 PV，然后只要让 PVC 也用 manual 的 StorageClass 就可以了：</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 code-server</title>
      <link>https://jia.je/devops/2020/04/22/k8s-code-server/</link>
      <pubDate>Wed, 22 Apr 2020 19:02:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/04/22/k8s-code-server/</guid>
      <description>实验了一下在 k8s 中部署 code-server，并不复杂，和之前几篇博客的配置是类似的：
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: code labels: app: code spec: selector: matchLabels: app: code replicas: 1 template: metadata: labels: app: code spec: volumes: - name: code-volume persistentVolumeClaim: claimName: code-pvc initContainers: - name: home-init image: busybox command: [&amp;#34;sh&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;chown -R 1000:1000 /home/coder&amp;#34;] volumeMounts: - mountPath: &amp;#34;/home/coder&amp;#34; name: code-volume containers: - image: codercom/code-server:latest imagePullPolicy: Always name: code volumeMounts: - mountPath: &amp;#34;/home/coder&amp;#34; name: code-volume resources: limits: cpu: &amp;#34;0.</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 Drone 用于 CI</title>
      <link>https://jia.je/devops/2020/04/21/k8s-drone-ci/</link>
      <pubDate>Tue, 21 Apr 2020 18:10:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/04/21/k8s-drone-ci/</guid>
      <description>实验了一下在 k8s 中部署 CI，在 drone gitlab-ci 和 jenkins 三者中选择了 drone，因为它比较轻量，并且基于 docker，可以用 GitHub 上的仓库，比较方便。
首先，配置 helm：
helm repo add drone https://charts.drone.io kubectl create ns drone 参考 drone 的文档，编写 drone-values.yml:
ingress: enabled: true annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; cert-manager.io/cluster-issuer: &amp;#34;letsencrypt-prod&amp;#34; hosts: - host: drone.example.com paths: - &amp;#34;/&amp;#34; tls: - hosts: - drone.example.com secretName: drone-tls env: DRONE_SERVER_HOST: drone.example.com DRONE_SERVER_PROTO: https DRONE_USER_CREATE: username:YOUR_GITHUB_USERNAME,admin:true DRONE_USER_FILTER: YOUR_GITHUB_USERNAME DRONE_RPC_SECRET: REDACTED DRONE_GITHUB_CLIENT_ID: REDACTED DRONE_GITHUB_CLIENT_SECRET: REDACTED 需要首先去 GitHub 上配置 OAuth application，具体参考 drone 的文档。然后，生成一个 secret，设置 admin 用户并只允许 admin 用户使用 drone，防止其他人使用。然后应用：</description>
    </item>
    
    <item>
      <title>在 k8s 内用 Cert Manager 配合 Nginx Ingress Controller 配置 Let&#39;s Encrypt HTTPS 证书</title>
      <link>https://jia.je/devops/2020/04/17/k8s-nginx-cert-manager-le/</link>
      <pubDate>Fri, 17 Apr 2020 18:13:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/04/17/k8s-nginx-cert-manager-le/</guid>
      <description>上一篇博客讲了 nginx ingress 的配置，那自然第一步要配上 https。首先配置 cert-manager：
$ kubectl create namespace cert-manager $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.crds.yaml $ helm repo add jetstack https://charts.jetstack.io $ helm repo update $ helm install \  cert-manager jetstack/cert-manager \  --namespace cert-manager \  --version v0.14.1 然后，配置 Cluster Issuer，应用以下的 yaml：
apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: email: example@example.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: nginx 然后在 ingress 里面进行配置：</description>
    </item>
    
    <item>
      <title>在 TKE 上配置不使用 LB 的 Nginx Ingress Controller</title>
      <link>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</link>
      <pubDate>Fri, 17 Apr 2020 17:30:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</guid>
      <description>背景 想要在 k8s 里面 host 一个网站，但又不想额外花钱用 LB，想直接用节点的 IP。
方法 首先安装 nginx-ingress：
$ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update $ helm install ingress-controller nginx-stable/nginx-ingress --set controller.service.type=NodePort --set controller.hostNetwork=true 这里给 ingress controller chart 传了两个参数：第一个指定 service 类型是 NodePort，替代默认的 LoadBalancer；第二个指定 ingress controller 直接在节点上监听，这样就可以用节点的公网 IP 访问了。
然后配一个 ingress：
apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-example annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; spec: rules: - host: example.com http: paths: - path: / backend: serviceName: example-service servicePort: 80 然后就可以发现请求被正确路由到 example-service 的 80 端口了。</description>
    </item>
    
    <item>
      <title>体验 Tencent Kubernetes Engine</title>
      <link>https://jia.je/devops/2020/03/17/setup-k8s-tencentcloud/</link>
      <pubDate>Tue, 17 Mar 2020 21:56:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/03/17/setup-k8s-tencentcloud/</guid>
      <description>之前在机器上试验了一下 kubernetes，感觉挺不错的，所以就想把腾讯云上面的机器也交给 kubernetes 管理。找到容器服务，新建集群，选择模板里的标准托管集群就可以了。然后开启下面的公网访问，设置一个比较小的 IP 地址段，按照页面下面的要求合并一下 kube config（因为还有别的 k8s cluster）：
$ KUBECONFIG=~/.kube/config:/path/to/new/config kubectl config view --merge --flatten &amp;gt; new_config $ cp new_config ~/.kube/config 覆盖之前先确认原来的配置还在，然后就可以用 kubectl 切换到新的 context：
$ kubectl config get-contexts $ kubectl config use-context new-context-here $ kubectl get node NAME STATUS ROLES AGE VERSION 172.21.0.17 Ready &amp;lt;none&amp;gt; 75m v1.16.3-tke.3 可以看到我们的 k8s node 已经上线了。我一般习惯先配好 kubernetes-dashboard：
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.6/install/kubernetes/quick-install.yaml $ kubectl proxy &amp;amp; $ kubectl -n kubernetes-dashboard describe secret (kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &amp;#39;{print \$1}&amp;#39;) | tail -n1 | awk &amp;#39;{print \$2}&amp;#39; | pbcopy 然后在浏览器里访问 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/overview?</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群上部署 gitlab—runner</title>
      <link>https://jia.je/devops/2020/03/14/k8s-gitlab-runner/</link>
      <pubDate>Sat, 14 Mar 2020 23:05:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/03/14/k8s-gitlab-runner/</guid>
      <description>按照 GitLab 上的教程试着把 gitlab-runner 部署到 k8s 集群上，发现异常地简单，所以简单做个笔记：
编辑 values.yaml
gitlabUrl: GITLAB_URL runnerRegistrationToken: &amp;#34;REDACTED&amp;#34; rbac: create: true 此处的信息按照 &amp;ldquo;Set up a specific Runner manually&amp;rdquo; 下面的提示填写。然后用 Helm 进行安装：
$ helm repo add gitlab https://charts.gitlab.io $ kubectl create namespace gitlab-runner $ helm install --namespace gitlab-runner gitlab-runner -f values.yaml gitlab/gitlab-runner 然后去 Kubernetes Dashboard 就可以看到部署的情况，回到 GitLab 也可以看到出现了 “Runners activated for this project” ，表示配置成功。
参考配置：https://docs.gitlab.com/runner/install/kubernetes.html</description>
    </item>
    
    <item>
      <title>用 Kubernetes 部署无状态服务</title>
      <link>https://jia.je/devops/2020/03/10/k8s-deploy-with-hpa/</link>
      <pubDate>Tue, 10 Mar 2020 13:57:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/03/10/k8s-deploy-with-hpa/</guid>
      <description>背景 最近需要部署一个用来跑编译的服务，服务从 MQ 取任务，编译完以后提交任务。最初的做法是包装成 docker，然后用 docker-compose 来 scale up。但既然有 k8s 这么好的工具，就试着学习了一下，踩了很多坑，总结了一些需要用到的命令。
搭建 Docker Registry 首先搭建一个本地的 Docker Repository，首先设置密码：
$ mkdir auth $ htpasswd user pass &amp;gt; auth/passwd 然后运行 registry：
$ docker run -d -p 5000:5000 \  --restart=always \  --name registry \  -v &amp;#34;$(pwd)/registry&amp;#34;:/var/lib/registry \  -v &amp;#34;$(pwd)/auth&amp;#34;:/auth \  -e &amp;#34;REGISTRY_AUTH=htpasswd&amp;#34; \  -e &amp;#34;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&amp;#34; \  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \  registry:2 简单起见没有配 tls。然后吧本地的 image push 上去：</description>
    </item>
    
    <item>
      <title>用 jailkit 限制用户仅 scp</title>
      <link>https://jia.je/devops/2020/03/09/use-jailkit-for-scp-only-user/</link>
      <pubDate>Mon, 09 Mar 2020 13:48:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/03/09/use-jailkit-for-scp-only-user/</guid>
      <description>最近需要用 scp 部署到生产机器，但又不想出现安全问题，所以用了 jailkit 的方法。首先是创建单独的用户，然后生成 ssh key 来认证，不再赘述。此时是可以 scp了，但用户依然可以获得 shell，不够安全。
然后找到了下面参考链接，大概摘录一下所需要的命令和配置：
mkdir /path/to/jail chown root:root /path/to/jail chmod 701 /path/to/jail jk_init -j /path/to/jail scp sftp jk_lsh jk_jailuser -m -j /path/to/jail jailed_user vim /path/to/jail/etc/jailkit/jk_lsh.ini # Add following lines [jailed_user] paths = /usr/bin, /usr/lib exectuables = /usr/bin/scp 之后可以发现该用户的 shell 已经更改 jk_chrootsh ，并且只能用 scp 。
参考：https://blog.tinned-software.net/restrict-linux-user-to-scp-to-his-home-directory/</description>
    </item>
    
    <item>
      <title>为 Cisco WLC 配置 Telegraf</title>
      <link>https://jia.je/devops/2019/10/19/configure-telegraf-for-cisco-wlc/</link>
      <pubDate>Sat, 19 Oct 2019 10:14:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2019/10/19/configure-telegraf-for-cisco-wlc/</guid>
      <description>最近想到可以给 Cisco WLC 也配置一下监控，查了一下，果然有一些方法。大概研究了一下，找到了方法：
把 https://github.com/haad/net-snmp/tree/master/mibs 和 https://github.com/zampat/neteye4/tree/master/monitoring/monitoring-plugins/wireless/cisco/mibs 目录下的所有 .txt 文件放到 /usr/share/snmp/mibs 目录下。
然后把 https://github.com/zampat/neteye4/blob/master/monitoring/monitoring-plugins/wireless/cisco/telegraf.conf 下面 snmp 的配置复制到 telegraf 配置中，然后修改一下 IP 地址。
确保 Cisco WLC 的 SNMP 的 Public Community 已经配置好，然后就可以拿到数据了。
目前可以拿到 WLC 自身的一些运行˙状态信息、AP 的信息、SSID 的信息和 Client 的信息，基本满足了我们的需求。
参考：https://www.neteye-blog.com/2019/08/monitoring-a-cisco-wireless-controller/</description>
    </item>
    
    <item>
      <title>用 htpdate 替代 ntpdate 实现时间同步</title>
      <link>https://jia.je/devops/2019/05/25/htpdate-for-time-sync/</link>
      <pubDate>Sat, 25 May 2019 07:54:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2019/05/25/htpdate-for-time-sync/</guid>
      <description>最近用 ntpdate 的时候遇到了一些麻烦，时间同步总是遇到各种问题。后来搜了搜，发现了一个解决方案： htpdate ，它通过 HTTP 头里的 Date 字段获取时间，虽然没有 ntp 那么精确，但是大多时候都够用。
用法见 htpdate(8) 。</description>
    </item>
    
    <item>
      <title>Nginx 反代到 HTTPS 上游</title>
      <link>https://jia.je/devops/2019/05/22/nginx-ssl-upstream/</link>
      <pubDate>Wed, 22 May 2019 16:01:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2019/05/22/nginx-ssl-upstream/</guid>
      <description>这次遇到一个需求，要反代到不在内网的地址，为了保证安全，还是得上 HTTPS ，所以尝试了一下怎么给 upstream 配置自签名 HTTPS 证书的验证。
upstream subpath { server 4.3.2.1:4321; } server { listen 443 ssl; server_name test.example.com; location /abc { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_ssl_trusted_certificate /path/to/self_signed_cert.crt; proxy_ssl_name 1.2.3.4; // to override server name checking proxy_ssl_verify on; proxy_ssl_depth 2; proxy_ssl_reuse on; proxy_pass https://subpath; } } 可以用 openssl 获得自签名的 cert :
echo | openssl s_client -showcerts -connect 4.3.2.1:4321 2&amp;gt;/dev/null | \ openssl x509 -text &amp;gt; /path/to/self_signed_cert.crt ref: https://stackoverflow.</description>
    </item>
    
    <item>
      <title>在古老的 OS 上运行一个干净的新的环境</title>
      <link>https://jia.je/devops/2019/03/21/new-clean-env-on-old-os/</link>
      <pubDate>Thu, 21 Mar 2019 22:46:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2019/03/21/new-clean-env-on-old-os/</guid>
      <description>由于某些课程的原因，需要在一个 CentOS 7 上跑一些编译和运行代码。看到这么古老的软件，我心想不行，肯定要找新一些的软件来用。首先想到的是 tmux ，于是按照网上的脚本 很快装了一个 tmux 2.8 版本，果然好了很多。但是常用的很多软件依然是个问题。试了一下最近比较新的 code-server ，因为ABI问题跑不起来。
于是开始想玩骚操作。首先想到的是 Gentoo Prefix ，不过既然是别人的机器，还是算了。又找了 fakeroot 配合 alpine rootfs 的方案，但编译不过，估计是内核版本问题。又试了一下 fakechroot ，但它需要配合 fakeroot 使用，这就凉了。
然后又找了一些替代方案。一是 uchroot ，但由于 CMake 版本太老也编译不过。最后发现了 PRoot ，直接提供 prebuilt 然后很容易就可以跑起来：
$ ./proot -b /proc -b /dev -r $CHROOT /bin/busybox sh 于是就进到了 alpine 的 rootfs 中，下载地址。进去以后发现没有编辑器，于是出来把 apk 的源改了，加了 resolv.conf ，就成功地安装了很多很新的软件了。</description>
    </item>
    
    <item>
      <title>用 MuSSH 快速对多台机器进行软件包升级</title>
      <link>https://jia.je/devops/2018/07/15/use-mussh-to-upgrade-multiple-machines/</link>
      <pubDate>Sun, 15 Jul 2018 01:10:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/07/15/use-mussh-to-upgrade-multiple-machines/</guid>
      <description>Debian Stretch 9.5 刚刚更新，自己手上有不少 stretch 的机器，于是顺手把他们都升级了。不过，这个过程比较繁琐，于是我采用了 MuSSH 的方法，让这个效率可以提高，即自动同时 SSH 到多台机器上进行更新。
首先编写 hostlist 文件，一行一个地址，分别对应每台机器。
然后采用 MuSSH 对每台机器执行同样的命令
$ mussh -H hostlist -c &amp;#39;apt update &amp;amp;&amp;amp; apt upgrade -y&amp;#39; 此时要求，ssh 上去以后有相应的权限。这个有许多方法，不再赘述。然后就可以看到一台台机器升级，打上安全补丁，爽啊。</description>
    </item>
    
    <item>
      <title>在 Lemote Yeeloong 上安装 Debian jessie</title>
      <link>https://jia.je/devops/2018/07/11/installing-debian-in-lemote-yeeloong/</link>
      <pubDate>Wed, 11 Jul 2018 23:12:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/07/11/installing-debian-in-lemote-yeeloong/</guid>
      <description>参考网站：
gNewSense To MIPS Run a TFTP server on macOS Debian on Yeeloong Debian MIPS port wiki Debian MIPS port
首先，进入设备的 PMON：
Press Del to enter PMON 然后，下载 Debian Jessie 的 netboot 文件：
$ wget https://mirrors.tuna.tsinghua.edu.cn/debian/dists/jessie/main/installer-mipsel/current/images/loongson-2f/netboot/vmlinux-3.16.0-6-loongson-2f $ wget https://mirrors.tuna.tsinghua.edu.cn/debian/dists/jessie/main/installer-mipsel/current/images/loongson-2f/netboot/initrd.gz 以 macOS 为例，起一个 tftp 服务器以供远程下载：
# ln -s these files to /private/tftpboot: # initrd.gz # vmlinux-4.16.0-6-loongson-2f $ sudo launchctl load -F /System/Library/LaunchDaemons/tftp.plist # set addr manually to 192.168.2.1 回到 PMON ，配置远程启动：</description>
    </item>
    
    <item>
      <title>通过 systemd-run 直接在容器中执行命令</title>
      <link>https://jia.je/devops/2018/07/06/run-command-in-container-directly/</link>
      <pubDate>Fri, 06 Jul 2018 15:56:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/07/06/run-command-in-container-directly/</guid>
      <description>之前使用 systemd-nspawn 开了容器，然后通过 machinectl shell 进去，想要起一个服务然后丢到后台继续执行，但是发现离开这个 session 后这个进程总是会被杀掉，于是找了 systemd-run 的方案，即：
systemd-run --machine machine_name_here absolute_path_to_executable args_here 这样可以直接在容器中跑服务，而且用这个命令输出的临时 server 名称，还可以看到日志：
journalctl --machine machine_name_here -u unit_name_above </description>
    </item>
    
    <item>
      <title>在 Nginx 将某个子路径反代</title>
      <link>https://jia.je/devops/2018/06/01/nginx-proxy-subpath/</link>
      <pubDate>Fri, 01 Jun 2018 07:57:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/06/01/nginx-proxy-subpath/</guid>
      <description>现在遇到这么一个需求，访问根下面是提供一个服务，访问某个子路径（/abc），则需要提供另一个服务。这两个服务处于不同的机器上，我们现在通过反代把他们合在一起。在配置这个的时候，遇到了一些问题，最后得以解决。
upstream root { server 1.2.3.4:1234; } upstream subpath { server 4.3.2.1:4321; } server { listen 443 ssl; server_name test.example.com; # the last slash is useful, see below location /abc/ { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # the last slash is useful too, see below proxy_pass http://subpath/; } location / { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://root; } } 由于并不想 subpath 他看到路径中 /abc/ 这一层，导致路径和原来在根下不同，通过这样配置以后，特别是两个末尾的斜杠，可以让 nginx 把 GET /abc/index.</description>
    </item>
    
    <item>
      <title>在 VMware ESXi 上部署 vCSA 实践</title>
      <link>https://jia.je/devops/2018/05/20/deploy-vcsa-under-esxi/</link>
      <pubDate>Sun, 20 May 2018 16:44:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/05/20/deploy-vcsa-under-esxi/</guid>
      <description>首先获取 vCSA 的 ISO 镜像，挂载到 Linux 下（如 /mnt），然后找到 /mnt/vcsa-cli-installer/templates/install 下的 embedded_vCSA_on_ESXi.json ，复制到其它目录并且修改必要的字段，第一个 password 为 ESXi 的登录密码，一会在安装的过程中再输入。下面有个 deployment_option，根据你的集群大小来选择，我则是用的 small 。下面配置这台机器的 IP 地址，用内网地址即可。下面的 system_name 如果要写 fqdn ，记得要让这个域名可以解析到正确的地址，不然会安装失败，我因此重装了一次。下面的密码都可以留空，在命令行中输入即可。SSO 为 vSphere Client 登录时用的密码和域名，默认用户名为 Administrator@domain_name (默认的话，则是 Administrator@vsphere.local) 这个用户名在安装结束的时候也会提示。下面的 CEIP 我选择关闭，设置为 false 。
接下来进行安装。
$ /mnt/vcsa-cli-installer/lin64/vcsa-deploy install /path/to/embedded_vCSA_on_ESXi.json --accept-eula 一路输入密码，等待安装完毕即可。然后通过 443 端口进入 vSphere Client, 通过 5480 端口访问 vCSA 的管理页面。两个的密码可以不一样。
2018-05-21 Update: 想要设置 VMKernel 的 IPv6 网关的话，ESXi 中没找到配置的地方，但是在 vSphere Client 中可以进行相关配置。</description>
    </item>
    
    <item>
      <title>在 macOS 上试用 Gentoo/Prefix</title>
      <link>https://jia.je/devops/2017/12/27/try-gentoo-prefix-on-macos/</link>
      <pubDate>Wed, 27 Dec 2017 20:25:23 +0800</pubDate>
      
      <guid>https://jia.je/devops/2017/12/27/try-gentoo-prefix-on-macos/</guid>
      <description>前几天参加了许朋程主讲的Tunight，对Gentoo有了一定的了解，不过看到如此复杂的安装过程和长久的编译时间，又看看我的CPU，只能望而却步了。不过，有Gentoo/Prefix这个工具，使得我们可以在其它的操作系统（如macOS,Solaris等）上在一个 $EPREFIX 下跑 Portage ，也就是把 Portage 运行在别的操作系统，当作一个包管理器，并且可以和别的操作系统并存。
首先还是祭出官网：Project:Prefix。
首先设定好环境变量 $EPREFIX ，之后所有的东西都会安装到这个目录下，把 bootstrap-prefix.sh 下载到 $EPREFIX ，然后 ./bootstrap-prefix.sh ，会进行一系列的问题，一一回答即可。建议在运行前设置好 GENTOO_MIRRORS=http://mirrors.tuna.tsinghua.edu.cn/gentoo 由于TUNA没有对gentoo_prefix做镜像，只能把distfiles切换到TUNA的镜像上。
然后。。。
stage1&amp;hellip;
stage2..
stage3.
emerge -e @world BOOM
经过 n 次跑挂以后，终于搞完了 stage3 ，然后 SHELL=bash ./bootstrap-prefix.sh $EPREFIX startscript 生成 startprefix ，在外面的SHELL中向切进来的时候运行这个即可。
然后就可以使用Gentoo/Prefix了。注意！此时的 $PATH 仅限于 $EPREFIX 下几个目录和 /usr/bin /bin 所以很多东西都会出问题（Emacs, Vim, Fish etc）。小心不要把自己的目录什么的搞挂了。
然后就可以假装试用Gentoo了！
哈哈哈哈哈哈哈
死于编译 libgcrypt 和 llvm 。</description>
    </item>
    
  </channel>
</rss>
