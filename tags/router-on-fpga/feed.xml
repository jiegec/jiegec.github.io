<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>router-on-fpga on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/router-on-fpga/</link>
    <description>Recent content in router-on-fpga on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Jun 2019 09:24:00 +0800</lastBuildDate><atom:link href="https://jia.je/tags/router-on-fpga/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在 FPGA 上实现路由器（3）</title>
      <link>https://jia.je/hardware/2019/06/02/router-on-fpga-3/</link>
      <pubDate>Sun, 02 Jun 2019 09:24:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/06/02/router-on-fpga-3/</guid>
      <description>前言 又半个月过去了，在写了上篇系列博文之后也是做了很多新的更改。上次做的主要是关于性能方面的提升，怎么提高频率，从而达到比较大的流量，而这段时间做的则是功能，做实现 RIP 协议和转发表的动态更新。
软件部分 软件部分目前是用 C 代码写的，用 Xilinx SDK 提供的各个 AXI 外设的驱动和 PS 自己的驱动，实现了所需要的，RIP 协议的处理，转发表的更新和统计信息的读取。
实际上做的时候比较粗暴，主要是通过三种 AXI 外设与硬件部分进行交互：AXI Stream FIFO，AXI GPIO 和 AXI BRAM Controller。其中 AXI Stream FIFO 是用来接收和发送需要 CPU 处理的以太网帧的，AXI GPIO 则是用来读取统计的信息，AXI BRAM Controller 是用来读写转发表的。最后在顶层设计中把这些外设连接起来。
硬件部分 硬件部分还是继续之前的部分往下写，添加了统计信息，直接暴露出去，让 CPU 走 AXI GPIO 读，因为不需要很高的精确度；转发表本身，一开始想的是自己写一些接口转换，后来发现，直接用 True Dual Port RAM 然后把一个 port 暴露给 AXI BRAM Controller 即可，免去了各种麻烦，PS 可以直接进行修改，不需要额外的工作。
最终效果 为了测试这套东西是否正常工作，就开了两个 Arch Linux 的虚拟机，分别 Bridge 到两个千兆的 USB 网卡上，都连到 FPGA 上。然后在两边都配上了 BIRD，配置 RIP 和一些路由，确实能更新硬件的转发表，并两边的 RIP 可以学习到对方的路由。</description>
    </item>
    
    <item>
      <title>在 FPGA 上实现路由器（2）</title>
      <link>https://jia.je/hardware/2019/05/15/router-on-fpga-2/</link>
      <pubDate>Wed, 15 May 2019 20:39:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/05/15/router-on-fpga-2/</guid>
      <description>前言 月初的时候，有了一个完整可用的路由器（上一篇系列博文），但当时测了一下速度，只有几十 Mb/s，只要往上提就会失效，得 reset 才能继续。当时也先没管性能的事情，先把和 OS 交互的部分做了。现在又回头来做性能调优。
之前，逻辑部分的主频只有 10 MHz，这自然不行，不提高肯定做不到千兆。于是试着把主频拉高，FIFO 加大，然后遇到了很多问题，慢慢修复了，学到了很多新知识，目前也接近千兆的水平了吧，贴图：
TCP 测速：
UDP 测速：
测试环境是 macOS 虚拟机外打虚拟机内，走网桥把虚拟机和一个 USB 网卡接起来，然后从另一个 USB 网卡打到路由器。
尝试 700Mb/s 接下来讲讲，在这个过程中遇到了什么问题，怎么解决的。第一个是速度过快就会挂，这肯定是丢包逻辑没写对，后来在仿真里开够了时间，于是就找到了一个 BUG，其实就是一行的修复。接着就是提高主频，但大家也知道，CPU 不能随便超频，由于各种延迟的原因，比如 Setup 时间，如果超了一个时钟周期的时间，本来应该下个周期就得到新数据的，结果到了下下周期才有，那有的状态可能就乱了，我目前遇到的也主要就是这个问题。
于是就对着 Timing 里汇报的各种问题修啊修，发现了很多以前没有注意到的问题，它们不影响功能，但是会让逻辑变慢。第一个问题是 High Fanout，以上就是说一个输出接到了很多输入，这看起来没啥问题，但数设课上也讲过，每个门的输入输出电流是有限制的，例如按书上的数据，一个门输出只能带十个门，更多只能级联一层。级联的话，延迟自然就高了。后来发现，这里的原因是，开了一个大的数组，但是没有变成 RAM，综合出了几千个逻辑单元，自然是出问题。解决方法很简单，用 xpm_memory_tpdram 即可。这样一搞，主频就能上 200MHz 了。
这个时候测了一下，发现 UDP 能打到 700Mb/s 了，TCP 由于丢包率比较高，只有 400Mb/s，距离预期还有一段距离。于是继续进行优化。
向 900Mb/s 进发 要继续提速，自然要提高主频。下一个主频目标就是 250MHz。随着提高主频，时序的要求也会更高，自然也出现了新的问题。
这次的问题主要在于，一个路径上逻辑门数过多，多的有 7 到 10 个，每一步零点几到一点几纳秒，叠起来 4 纳秒哪里够用。于是把一些不需要依赖条件的逻辑挪到条件外面，这样就减少了一些路径的依赖。
解决了这个以后，现在的 WNS（Worst Negative Slack）只剩下 0.6 ns 了。这时候的问题一部分还是来自于逻辑门过多，但这个时候就没这么简单了，只能继续细化流水线，打一拍，这样才能把延迟降下来。
虽然 Timing 没有完全解决，但还是写进了 FPGA 中。幸好工作一切正常，就得到了上面那个图片的结果，接近千兆的速度了。</description>
    </item>
    
    <item>
      <title>在 FPGA 上实现路由器</title>
      <link>https://jia.je/hardware/2019/04/24/router-on-fpga/</link>
      <pubDate>Wed, 24 Apr 2019 19:41:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2019/04/24/router-on-fpga/</guid>
      <description>最近在做 FPGA 上硬件的路由器，感觉接近一个基本可用的阶段了吧，大概谈一谈做这个的思路、过程和踩过的坑。
首先，做实验用的板子是 Alinx AX7021，FPGA 是 Xilinx xc7z020clg484-2，扩展板上有 4PL+1PS 个网口和千兆 KSZ9031RNX PHY，采用的接口是 RGMII。一开始做的自然是做 RGMII，但是遇到了困难，RGMII 在千兆模式下传输的是 DDR 信号，而时序和延迟就是个比较麻烦的事情。一开始先直接拿 Xilinx 的 AXI Ethernet IP 来用，然后上 ILA 看到了 IDDR 后的信号，第一次看到了完整的以太网帧，从 Preamble 和 SFD 到最后的 FCS。于是就特别振奋，想着手写 RGMII，先做收，再做发。确实，收很容易，很快就做出来了，但是写总是出问题，当时也不懂跨时钟域的一些问题，总之各种没调出来。于是就退而求其次，选择了 Xilinx 的 Tri Mode Ethernet IP 了。
Tri Mode Ethernet IP 有很多选项，为了简单，直接采用了 AXI-Stream 的接口，不要 AXI4-Lite 什么的，都不要，因为我需要直接写剩余的逻辑。其他东西能省也都省掉了。这个 IP 确实很给力，很快就可以完成收和发的操作了，这次终于知道了怎么处理跨时钟域的问题 — XPM FIFO ASYNC，一下推进了很大的进度。
既然可以收，也可以发了，就扩展到多个网口。这个 IP 中可以选择 Shared Logic 在内部，也可以在外部，研究了一下发现，应该是一个放内部，其余选外部，然后接起来就可以了。不过目前为了简单，还是只用了俩端口。在这个基础上，就开始解析收进来的以太网帧了。
第一步自然是填 ARP 表，自然问题来了，如果多个网口同时进来数据，怎么保证 ARP 表读写的正确性？自然就想到总线上需要做仲裁，于是写了一个简单的总线仲裁，顺带学习到了 unique case(z) 和 priority case(z) 的语法。然后 ARP 表怎么实现呢，大概就是一个哈希表，然后表里维护了（IP，MAC，PORT）三元组，然后实现了一些冲突和覆盖的处理逻辑，做这些的同时也对各个模块编写相应的测试。有了 ARP 表，就可以在解析以太网帧的时候，拆解出里面的信息，然后请求 ARP 表总线，然后写入。</description>
    </item>
    
  </channel>
</rss>
