<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/k8s/</link>
    <description>Recent content in k8s on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jun 2021 20:29:00 +0000</lastBuildDate><atom:link href="https://jia.je/tags/k8s/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>将 k8s rook ceph 集群迁移到 cephadm</title>
      <link>https://jia.je/devops/2021/06/25/ceph-k8s-to-external/</link>
      <pubDate>Fri, 25 Jun 2021 20:29:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2021/06/25/ceph-k8s-to-external/</guid>
      <description>背景 前段时间用 rook 搭建了一个 k8s 内部的 ceph 集群，但是使用过程中遇到了一些稳定性问题，所以想要用 cephadm 重建一个 ceph 集群。
重建过程 重建的时候，我首先用 cephadm 搭建了一个 ceph 集群，再把原来的 MON 数据导入，再恢复各个 OSD。理论上，可能有更优雅的办法，但我还是慢慢通过比较复杂的办法解决了。
cephadm 搭建 ceph 集群 首先，配置 TUNA 源，在各个节点上安装 docker-ce 和 cephadm。接着，在主节点上 bootstrap：
cephadm bootstrap --mon-ip HOST1_IP 此时，在主节点上会运行最基础的 ceph 集群，不过此时还没有任何数据。寻找 ceph 分区，会发现因为 FSID 不匹配而无法导入。所以，首先要恢复 MON 数据。
参考文档：cephadm install。
恢复 MON 数据 首先，关掉 rook ceph 集群，找到留存下来的 MON 数据目录，默认路径是 /var/lib/rook 下的 mon-[a-z] 目录，找到最新的一个即可。我把目录下的路径覆盖到 cephadm 生成的 MON 目录下，然后跑起来，发现有几个问题：
 cephadm 生成的 /etc/ceph/ceph.client.admin.keyring 与 MON 中保存的 auth 信息不匹配，导致无法访问 FSID 不一致，而 cephadm 会将各个设置目录放到 /var/lib/ceph/$FSID 下  第一个问题的解决办法就是临时用 MON 目录下的 keyring 进行认证，再创建一个新的 client.</description>
    </item>
    
    <item>
      <title>用 fluentd 收集 k8s 中容器的日志</title>
      <link>https://jia.je/devops/2021/04/02/k8s-fluentd-log-collect/</link>
      <pubDate>Fri, 02 Apr 2021 23:19:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2021/04/02/k8s-fluentd-log-collect/</guid>
      <description>背景 在维护一个 k8s 集群的时候，一个很常见的需求就是把日志持久化存下来，一方面是方便日后回溯，一方面也是聚合 replicate 出来的同一个服务的日志。
在我目前的需求下，只需要把日志持久下来，还不需要做额外的分析。所以我并没有部署类似 ElasticSearch 的服务来对日志进行索引。
实现 实现主要参考官方的仓库：https://github.com/fluent/fluentd-kubernetes-daemonset。它把一些常用的插件打包到 docker 镜像中，然后提供了一些默认的设置，比如获取 k8s 日志和 pod 日志等等。为了达到我的需求，我希望：
 每个结点上有一个 fluentd 收集日志，forward 到单独的 log server 上的 fluentd log server 上的 fluentd 把收到的日志保存到文件  由于 log server 不由 k8s 管理，所以按照官网的方式手动安装：
$ curl -L https://toolbelt.treasuredata.com/sh/install-debian-buster-td-agent4.sh | sh 然后，编辑配置 /etc/td-agent/td-agent.conf：
&amp;lt;source&amp;gt; @type forward @id input_forward bind x.x.x.x &amp;lt;/source&amp;gt; &amp;lt;match **&amp;gt; @type file path /var/log/fluentd/k8s compress gzip &amp;lt;buffer&amp;gt; timekey 1d timekey_use_utc true timekey_wait 10m &amp;lt;/buffer&amp;gt; &amp;lt;/match&amp;gt; 分别设置输入：监听 fluentd forward 协议；输出：设置输出文件，和 buffer 配置。如有需要，可以加鉴权。</description>
    </item>
    
    <item>
      <title>用 gitlab ci 构建并部署应用到 k8s</title>
      <link>https://jia.je/devops/2021/03/16/gitlab-ci-k8s-integration/</link>
      <pubDate>Tue, 16 Mar 2021 08:41:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2021/03/16/gitlab-ci-k8s-integration/</guid>
      <description>背景 在 k8s 集群中部署了 gitlab-runner，并且希望在 gitlab ci 构建完成后，把新的 docker image push 到 private repo，然后更新应用。
参考文档：Gitlab CI 与 Kubernetes 的结合，Using Docker to build Docker images。
在 gitlab ci 中构建 docker 镜像 这一步需要 DinD 来实现在容器中构建容器。为了达到这个目的，首先要在 gitlab-runner 的配置中添加一个 volume 来共享 DinD 的证书路径：
gitlabUrl: REDACTED rbac: create: true runnerRegistrationToken: REDACTED runners: config: |[[runners]] [runners.kubernetes] image = &amp;#34;ubuntu:20.04&amp;#34; privileged = true [[runners.kubernetes.volumes.empty_dir]] name = &amp;#34;docker-certs&amp;#34; mount_path = &amp;#34;/certs/client&amp;#34; medium = &amp;#34;Memory&amp;#34; privileged: true 注意两点：1. privileged 2.</description>
    </item>
    
    <item>
      <title>用 k3s 部署 k8s</title>
      <link>https://jia.je/devops/2021/03/12/k3s-deploy/</link>
      <pubDate>Fri, 12 Mar 2021 00:41:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2021/03/12/k3s-deploy/</guid>
      <description>背景 最近需要部署一个 k8s 集群，觉得之前配置 kubeadm 太繁琐了，想要找一个简单的。比较了一下 k0s 和 k3s，最后选择了 k3s。
配置 k3s 的好处就是配置十分简单：https://rancher.com/docs/k3s/latest/en/quick-start/。不需要装 docker，也不需要装 kubeadm。
 在第一个 node 上跑：curl -sfL https://get.k3s.io | sh - 在第一个 node 上获取 token：cat /var/lib/rancher/k3s/server/node-token 在其他 node 上跑：curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -  然后就搞定了。从第一个 node 的 /etc/rancher/k3s/k3s.yaml获取 kubectl 配置。</description>
    </item>
    
    <item>
      <title>通过 rook 在 k8s 上部署 ceph 集群</title>
      <link>https://jia.je/devops/2021/03/12/ceph-on-k8s/</link>
      <pubDate>Fri, 12 Mar 2021 00:41:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2021/03/12/ceph-on-k8s/</guid>
      <description>背景 为了方便集群的使用，想在 k8s 集群里部署一个 ceph 集群。
Ceph 介绍
Ceph 有这些组成部分：
 mon：monitor mgr：manager osd：storage mds(optional)：用于 CephFS radosgw(optional：用于 Ceph Object Storage  配置 我们采用的是 rook 来部署 ceph 集群。
参考文档：https://rook.github.io/docs/rook/v1.5/ceph-examples.html
首先，克隆 rook 的仓库。建议选择一个 release 版本。
接着，运行下面的命令：
sudo apt install -y lvm2 # required kubectl apply -f rook/cluster/examples/kubernetes/ceph/crds.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/common.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/operator.yaml # debugging only kubectl apply -f rook/cluster/examples/kubernetes/ceph/toolbox.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/direct-mount.yaml # CephFS kubectl apply -f rook/cluster/examples/kubernetes/ceph/filesystem.yaml kubectl apply -f rook/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 Prometheus</title>
      <link>https://jia.je/devops/2020/07/10/k8s-prometheus/</link>
      <pubDate>Fri, 10 Jul 2020 09:24:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/07/10/k8s-prometheus/</guid>
      <description>实验了一下在 k8s 中部署 Prometheus，因为它和 k8s 有比较好的集成，很多 App 能在 k8s 里通过 service discovery 被 Prometheus 找到并且抓取数据。实践了一下，其实很简单。
用 helm 进行配置：
helm upgrade --install prometheus stable/prometheus 这样就可以了，如果已经有 StorageClass （比如腾讯云的话，CBS 和 CFS），它就能自己起来了，然后在 Lens 里面也可以看到各种 metrics 的可视化。
如果是自建的单结点的 k8s 集群，那么还需要自己创造 PV，并且把 PVC 绑定上去。我采用的是 local 类型的 PV：
apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume-1 labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &amp;#34;/srv/k8s-data-1&amp;#34; --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-volume-2 labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &amp;#34;/srv/k8s-data-2&amp;#34; 这样，结点上的两个路径分别对应两个 PV，然后只要让 PVC 也用 manual 的 StorageClass 就可以了：</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 code-server</title>
      <link>https://jia.je/devops/2020/04/22/k8s-code-server/</link>
      <pubDate>Wed, 22 Apr 2020 19:02:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/04/22/k8s-code-server/</guid>
      <description>实验了一下在 k8s 中部署 code-server，并不复杂，和之前几篇博客的配置是类似的：
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: code labels: app: code spec: selector: matchLabels: app: code replicas: 1 template: metadata: labels: app: code spec: volumes: - name: code-volume persistentVolumeClaim: claimName: code-pvc initContainers: - name: home-init image: busybox command: [&amp;#34;sh&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;chown -R 1000:1000 /home/coder&amp;#34;] volumeMounts: - mountPath: &amp;#34;/home/coder&amp;#34; name: code-volume containers: - image: codercom/code-server:latest imagePullPolicy: Always name: code volumeMounts: - mountPath: &amp;#34;/home/coder&amp;#34; name: code-volume resources: limits: cpu: &amp;#34;0.</description>
    </item>
    
    <item>
      <title>在 k8s 中部署 Drone 用于 CI</title>
      <link>https://jia.je/devops/2020/04/21/k8s-drone-ci/</link>
      <pubDate>Tue, 21 Apr 2020 18:10:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/04/21/k8s-drone-ci/</guid>
      <description>实验了一下在 k8s 中部署 CI，在 drone gitlab-ci 和 jenkins 三者中选择了 drone，因为它比较轻量，并且基于 docker，可以用 GitHub 上的仓库，比较方便。
首先，配置 helm：
helm repo add drone https://charts.drone.io kubectl create ns drone 参考 drone 的文档，编写 drone-values.yml:
ingress: enabled: true annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; cert-manager.io/cluster-issuer: &amp;#34;letsencrypt-prod&amp;#34; hosts: - host: drone.example.com paths: - &amp;#34;/&amp;#34; tls: - hosts: - drone.example.com secretName: drone-tls env: DRONE_SERVER_HOST: drone.example.com DRONE_SERVER_PROTO: https DRONE_USER_CREATE: username:YOUR_GITHUB_USERNAME,admin:true DRONE_USER_FILTER: YOUR_GITHUB_USERNAME DRONE_RPC_SECRET: REDACTED DRONE_GITHUB_CLIENT_ID: REDACTED DRONE_GITHUB_CLIENT_SECRET: REDACTED 需要首先去 GitHub 上配置 OAuth application，具体参考 drone 的文档。然后，生成一个 secret，设置 admin 用户并只允许 admin 用户使用 drone，防止其他人使用。然后应用：</description>
    </item>
    
    <item>
      <title>在 k8s 内用 Cert Manager 配合 Nginx Ingress Controller 配置 Let&#39;s Encrypt HTTPS 证书</title>
      <link>https://jia.je/devops/2020/04/17/k8s-nginx-cert-manager-le/</link>
      <pubDate>Fri, 17 Apr 2020 18:13:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/04/17/k8s-nginx-cert-manager-le/</guid>
      <description>上一篇博客讲了 nginx ingress 的配置，那自然第一步要配上 https。首先配置 cert-manager：
$ kubectl create namespace cert-manager $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.crds.yaml $ helm repo add jetstack https://charts.jetstack.io $ helm repo update $ helm install \  cert-manager jetstack/cert-manager \  --namespace cert-manager \  --version v0.14.1 然后，配置 Cluster Issuer，应用以下的 yaml：
apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: email: example@example.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: nginx 然后在 ingress 里面进行配置：</description>
    </item>
    
    <item>
      <title>在 TKE 上配置不使用 LB 的 Nginx Ingress Controller</title>
      <link>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</link>
      <pubDate>Fri, 17 Apr 2020 17:30:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</guid>
      <description>背景 想要在 k8s 里面 host 一个网站，但又不想额外花钱用 LB，想直接用节点的 IP。
方法 首先安装 nginx-ingress：
$ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update $ helm install ingress-controller nginx-stable/nginx-ingress --set controller.service.type=NodePort --set controller.hostNetwork=true 这里给 ingress controller chart 传了两个参数：第一个指定 service 类型是 NodePort，替代默认的 LoadBalancer；第二个指定 ingress controller 直接在节点上监听，这样就可以用节点的公网 IP 访问了。
然后配一个 ingress：
apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-example annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; spec: rules: - host: example.com http: paths: - path: / backend: serviceName: example-service servicePort: 80 然后就可以发现请求被正确路由到 example-service 的 80 端口了。</description>
    </item>
    
    <item>
      <title>体验 Tencent Kubernetes Engine</title>
      <link>https://jia.je/devops/2020/03/17/setup-k8s-tencentcloud/</link>
      <pubDate>Tue, 17 Mar 2020 21:56:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/03/17/setup-k8s-tencentcloud/</guid>
      <description>之前在机器上试验了一下 kubernetes，感觉挺不错的，所以就想把腾讯云上面的机器也交给 kubernetes 管理。找到容器服务，新建集群，选择模板里的标准托管集群就可以了。然后开启下面的公网访问，设置一个比较小的 IP 地址段，按照页面下面的要求合并一下 kube config（因为还有别的 k8s cluster）：
$ KUBECONFIG=~/.kube/config:/path/to/new/config kubectl config view --merge --flatten &amp;gt; new_config $ cp new_config ~/.kube/config 覆盖之前先确认原来的配置还在，然后就可以用 kubectl 切换到新的 context：
$ kubectl config get-contexts $ kubectl config use-context new-context-here $ kubectl get node NAME STATUS ROLES AGE VERSION 172.21.0.17 Ready &amp;lt;none&amp;gt; 75m v1.16.3-tke.3 可以看到我们的 k8s node 已经上线了。我一般习惯先配好 kubernetes-dashboard：
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.6/install/kubernetes/quick-install.yaml $ kubectl proxy &amp;amp; $ kubectl -n kubernetes-dashboard describe secret (kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &amp;#39;{print \$1}&amp;#39;) | tail -n1 | awk &amp;#39;{print \$2}&amp;#39; | pbcopy 然后在浏览器里访问 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/overview?</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群上部署 gitlab—runner</title>
      <link>https://jia.je/devops/2020/03/14/k8s-gitlab-runner/</link>
      <pubDate>Sat, 14 Mar 2020 23:05:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/03/14/k8s-gitlab-runner/</guid>
      <description>按照 GitLab 上的教程试着把 gitlab-runner 部署到 k8s 集群上，发现异常地简单，所以简单做个笔记：
编辑 values.yaml
gitlabUrl: GITLAB_URL runnerRegistrationToken: &amp;#34;REDACTED&amp;#34; rbac: create: true 此处的信息按照 &amp;ldquo;Set up a specific Runner manually&amp;rdquo; 下面的提示填写。然后用 Helm 进行安装：
$ helm repo add gitlab https://charts.gitlab.io $ kubectl create namespace gitlab-runner $ helm install --namespace gitlab-runner gitlab-runner -f values.yaml gitlab/gitlab-runner 然后去 Kubernetes Dashboard 就可以看到部署的情况，回到 GitLab 也可以看到出现了 “Runners activated for this project” ，表示配置成功。
参考配置：https://docs.gitlab.com/runner/install/kubernetes.html</description>
    </item>
    
    <item>
      <title>用 Kubernetes 部署无状态服务</title>
      <link>https://jia.je/devops/2020/03/10/k8s-deploy-with-hpa/</link>
      <pubDate>Tue, 10 Mar 2020 13:57:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/03/10/k8s-deploy-with-hpa/</guid>
      <description>背景 最近需要部署一个用来跑编译的服务，服务从 MQ 取任务，编译完以后提交任务。最初的做法是包装成 docker，然后用 docker-compose 来 scale up。但既然有 k8s 这么好的工具，就试着学习了一下，踩了很多坑，总结了一些需要用到的命令。
搭建 Docker Registry 首先搭建一个本地的 Docker Repository，首先设置密码：
$ mkdir auth $ htpasswd user pass &amp;gt; auth/passwd 然后运行 registry：
$ docker run -d -p 5000:5000 \  --restart=always \  --name registry \  -v &amp;#34;$(pwd)/registry&amp;#34;:/var/lib/registry \  -v &amp;#34;$(pwd)/auth&amp;#34;:/auth \  -e &amp;#34;REGISTRY_AUTH=htpasswd&amp;#34; \  -e &amp;#34;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&amp;#34; \  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \  registry:2 简单起见没有配 tls。然后吧本地的 image push 上去：</description>
    </item>
    
  </channel>
</rss>
