<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rcore on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jiege.ch/tags/rcore/</link>
    <description>Recent content in rcore on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Apr 2019 12:13:00 +0800</lastBuildDate>
    
	<atom:link href="https://jiege.ch/tags/rcore/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>rCore 软路由实现</title>
      <link>https://jiege.ch/software/rcore-soft-router/</link>
      <pubDate>Sun, 07 Apr 2019 12:13:00 +0800</pubDate>
      
      <guid>https://jiege.ch/software/rcore-soft-router/</guid>
      <description>最近在研究软路由在 rCore 上的实现，但限于硬件限制，目前先在虚拟机里测试。软路由大概要做这些东西：
1. 抓包，解析包里的内容 2. 查路由表，找到下一跳在哪 3. 查ARP，知道下一跳的 MAC 地址 4. 减少TTL，更新 IP Checksum 5. 把包发出去  第一步直接拿 smoltcp 的 Raw Socket 即可，但是目前只能抓指定 IP Protocol 的包，我用的是 ICMP ，但其他的就还抓不了，需要继续改 Smoltcp 源代码。
第二步用的是之前刚修好的 treebitmap 库，它提供了路由表的查询功能，目前路由表还是写死的，之后会用已经部分实现好的 Netlink 接口读取出来。
第三步则是 ioctl 发请求，然后从 smoltcp 内部的 ARP cache 里读取。
第四步很简单，不用多说。
第五步则需要指定出端口，用了一个 index ，放在一个特定的 sockaddr 中。
最后的效果就是，能双向转发 ping 通。
网络拓扑：
可以，这很玄学。
后续在想在真机上实验，但是还缺一个网卡驱动，不然就可以用神奇的办法来做这个实验了。</description>
    </item>
    
    <item>
      <title>静态编译 sqlite3</title>
      <link>https://jiege.ch/software/static-building-sqlite/</link>
      <pubDate>Sun, 24 Mar 2019 19:13:00 +0800</pubDate>
      
      <guid>https://jiege.ch/software/static-building-sqlite/</guid>
      <description>最近 rCore 支持了动态链接库，于是想着在测试 sqlite 的时候直接用动态的，不过出现了玄学的问题，它会访问一个不存在的地址，看代码也没看出个所以然来。所以研究了一下 sqlite 的静态编译。首先在 configure 的时候尝试了一下：
$ ./configure CC=x86_64-linux-musl-gcc --disable-shared --enabled-static  发现 libsqlite 确实是静态了，但是 sqlite3 并不是。一番研究以后，发现是 libtool 的原因，只要这样编译：
$ make LTLINK_EXTRAS=-all-static  就可以编译出静态的 sqlite3 ：
sqlite3: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped  </description>
    </item>
    
    <item>
      <title>交叉编译 Nginx 1.14.2 到 RISC-V</title>
      <link>https://jiege.ch/software/cross-compiling-nginx-to-riscv/</link>
      <pubDate>Fri, 22 Mar 2019 23:18:00 +0800</pubDate>
      
      <guid>https://jiege.ch/software/cross-compiling-nginx-to-riscv/</guid>
      <description>最近又把一定的精力放到了 RISC-V 64 上的 rCore 用户态程序的支持上，同时也借到了 HiFive Unleashed 板子，所以有真实硬件可以拿来跑了。在这之前先在 QEMU 上把能跑的都跑起来。
由于 rCore 对 glibc 的支持一直有问题，RISC-V 也不例外，所以还是选择用 musl 来做这件事情。一般搜索，终于找到了 Linux 下能用的 musl-riscv-toolchain 。编译好工具链以后，很多需要 libc 的用户态都能跑了，于是想着试一下 nginx 的编译。试着编译了一下，遇到了各种问题，最后搜到了交叉编译Hi3536上面使用的nginx，里面的方法解决了这个问题。最后总结出了这样的 patch :
diff --git a/nginx-1.14.2/auto/cc/name b/nginx-1.14.2/auto/cc/name index ded93f5..d6ab27a 100644 --- a/nginx-1.14.2/auto/cc/name +++ b/nginx-1.14.2/auto/cc/name @@ -7,7 +7,7 @@ if [ &amp;quot;$NGX_PLATFORM&amp;quot; != win32 ]; then ngx_feature=&amp;quot;C compiler&amp;quot; ngx_feature_name= - ngx_feature_run=yes + ngx_feature_run=no ngx_feature_incs= ngx_feature_path= ngx_feature_libs= diff --git a/nginx-1.14.2/auto/lib/openssl/make b/nginx-1.14.2/auto/lib/openssl/make index 126a238..7a0e768 100644 --- a/nginx-1.14.2/auto/lib/openssl/make +++ b/nginx-1.</description>
    </item>
    
    <item>
      <title>在 rCore 上运行 nginx</title>
      <link>https://jiege.ch/programming/2019/03/08/running-nginx-on-rcore/</link>
      <pubDate>Fri, 08 Mar 2019 18:07:00 +0800</pubDate>
      
      <guid>https://jiege.ch/programming/2019/03/08/running-nginx-on-rcore/</guid>
      <description>阿 西 吧 nginx 终于能在 rCore 上跑了 orrrrrrrz
通过这半个多月来的大量开发，我和王润基 @wangrunji0408 学长算是终于完成了第一个 milestone：跑起来一个 nginx 。遇到了很多困难，大概有这些：
 syscall 实现不全。各种方面都缺，然后 nginx 在编译的时候又检测到比较新的 OS 版本，所以很多 syscall 都用了新的来替代老的，例如 readv/writev pread/pwrite accept4 等等，所以这方面做了一些工作。另外，还有很多新的 syscall 进来，太多了我就不细说了，基本上一个commit做一点一个commit做一点这个样子。 nginx 用到了 SSE 的寄存器 xmm ，但是之前是没有开的。所以把 sse 打开，然后切换上下文的时候把 sse 通过 fxsave 保存和 fxrstor 恢复（有意思的是，as居然不认这俩，只好手动写字节码），然后为了 16bit 的对齐又写了几行汇编代码。这块问题不大，今天一会就搞定了。但是如果要性能更高一些的话，可能需要在第一次使用 xmm 的时候再开始保存，大概就是加一个bit的事情。 文件系统有点崩。实现还是有很多 BUG ，表现就是需要经常重新 mksfs 一下，再重启加载完好的 fs ，有时候强制关机一下就又崩了。 内存管理做了一些改变。为了实现更加完整的 mmap mumap 和 mprotect ，又发现了一些新的 BUG 在里面，然后慢慢修复了。就是实现的有点粗暴。 死锁问题。这个其实现在还会出现，只是还没调出来，也不会百分百出现。我们计划在锁上面做一些死锁检测，例如记住是谁上锁的，等等。现在就遇到一个很玄学的死锁问题。  然后代码也是一边在写一边在重构吧，很多地方现在都写得很粗暴，FIXME和TODO留了很多，很多地方也写得不够优雅。以后再慢慢重构+优化吧。
截图留念：
再往前的话，还有很多小的问题，例如网卡的中断启用了但没有改 mask ，所以啥也没收到，靠 QEMU Tracing 找到问题。还有一个很有意思的现象，就是如果 elf 的 program header 没有 phdr 这个项的时候，我们发现，可以通过第一个load（如果加载了完整的 elf 头的话），我们可以从这里推断出 phdr 的地址（load的虚拟地址加偏移），然后丢到 auxv 里去让 musl 配置 tls。总之这些都解决了。也不用去考虑兼容 litc 了，已经全部向 linux 靠拢了，稳。</description>
    </item>
    
    <item>
      <title>实现网络的 syscall</title>
      <link>https://jiege.ch/programming/2019/03/04/implement-network-syscalls/</link>
      <pubDate>Mon, 04 Mar 2019 16:40:00 +0800</pubDate>
      
      <guid>https://jiege.ch/programming/2019/03/04/implement-network-syscalls/</guid>
      <description>有了网卡驱动，接下来要做的就做网络的 syscall 了。为了测试，首先在 busybox 里找可以用来测试的 applet ，由于没有实现 poll ，所以 nc telnet 啥的都用不了。最后选择到了 ping 和 pscan 上。
ping大家都很了解，pscan就是一个扫端口的，对一个ip连续的若干个端口发起 tcp 请求。这就要求我提供 raw socket和tcp socket状态的支持。由于网络栈本身是异步的，但 read connect 这些函数在不调 setsockopt 的前提下又是同步的，然而现在又没有 signal 可以用，要是 block 了就再也出不来了。于是就采用了 Condvar 的办法，拿一个全局的条件变量，当 poll 不到内容的时候，先把线程拿掉，等到网络栈更新了，再恢复。这样至少不会把 cpu 也 block 住。
然后就是把 socket 部分改了又改吧，数据结构的设计改了几次，为了解决 ownership 问题上锁啊也有点多，但是也更细了，虽然实际上可能没有必要，因为上面还有大的锁。不过性能还不是现在考虑的重点，关键还要先把 send recv accept bind listen 啥的写得差不多了，然后还有把 poll/select 实现了，这个很关键。
中间遇到的最大的坑就是，接收 pci interrupt 的时候总是啥也没有，然后靠万能的 qemu trace 发现，原来是 mask 掉了，所以啥也收不了，然后最后的解决方案就是用 MSI Interrupt #55 搞定了这个问题。至于为啥是 55 呢，因为 23 + 32 = 55 啊（误</description>
    </item>
    
    <item>
      <title>使用 Rust 实现 e1000 驱动</title>
      <link>https://jiege.ch/programming/2019/02/26/network-driver-again/</link>
      <pubDate>Tue, 26 Feb 2019 20:30:00 +0800</pubDate>
      
      <guid>https://jiege.ch/programming/2019/02/26/network-driver-again/</guid>
      <description>是的。我又来了。上次做了使用 Rust 实现 VirtIO 驱动之后，继续往 rCore 加更多的驱动支持。由于现在工作重点是 x86_64 下的 syscall 实现，所以选了一个比较有代表性的驱动 e1000 来实现。其实如果只是为了在 qemu 下运行的话，其实只需要支持 virtio-pci 就可以了，原来的 virtio-net 直接拿来用就可以了。
为什么挑 e1000 呢，一方面是支持的设备多，有真实硬件可以测试，虽然不一定要裸机上跑，但是可以通过 PCI passthrough 来测试驱动的正确性。另一方面是网上的资料比较多，有现成的简单的代码可以借鉴。这次主要借鉴了三个来源：一是 Biscuit OS， 二是 Judge Duck OS ，三是 Linux 。
首先是实现了简单的 PCI 总线的枚举，然后找到对应的设备，激活，并且找到映射的内存地址，然后把原来 C 语言的实现搬运到 Rust 中。这个过程中遇到很多坑，例如一开始我以为内核里 pa 和 va 是一个固定的偏移，不过多次尝试后才发现这个假设只对 riscv 平台里的实现成立。
这个时候就可以收到外面给进来的以太网帧了。接着就是把它接入到 smoltcp 的 API 中。但是发包又不工作了，尝试了很多次，各种方法也不行。其中特别要提到的就是 qemu 的 tracing API ，它在帮助我调试之前的 virtio 驱动和这次的 e1000(e) 驱动中起到了很大的帮助。不过，遗憾的是，发包相关的代码里的 trace 不足以让我找到问题的所在，我只好采用了最后一招：
下载 QEMU ，自己改，然后自己编译。
这个方法果然很有效啊，经过简单的几个修改，很快就定位到问题所在了，原来就是一个简单的错误，把 4 写成了 8 。这个过程中我也发现 QEMU 在 incremental build 的时候似乎会 segfault ，我没管这么多，反正编译也不慢，次数也不多，每次 clean 再 build 问题也不大。</description>
    </item>
    
    <item>
      <title>使用 Rust 实现 VirtIO 驱动</title>
      <link>https://jiege.ch/programming/2019/01/29/virtio-drivers-implementation/</link>
      <pubDate>Tue, 29 Jan 2019 17:23:00 +0800</pubDate>
      
      <guid>https://jiege.ch/programming/2019/01/29/virtio-drivers-implementation/</guid>
      <description>背景 最近在给 rCore 添加驱动层的支持。一开始是想做网卡驱动，后来发现， qemu-system-riscv32 只支持如下的驱动：
# qemu-system-riscv32 -device help Storage devices: name &amp;quot;scsi-cd&amp;quot;, bus SCSI, desc &amp;quot;virtual SCSI CD-ROM&amp;quot; name &amp;quot;scsi-disk&amp;quot;, bus SCSI, desc &amp;quot;virtual SCSI disk or CD-ROM (legacy)&amp;quot; name &amp;quot;scsi-hd&amp;quot;, bus SCSI, desc &amp;quot;virtual SCSI disk&amp;quot; name &amp;quot;virtio-blk-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-scsi-device&amp;quot;, bus virtio-bus Network devices: name &amp;quot;virtio-net-device&amp;quot;, bus virtio-bus Input devices: name &amp;quot;virtconsole&amp;quot;, bus virtio-serial-bus name &amp;quot;virtio-keyboard-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-mouse-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-serial-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-tablet-device&amp;quot;, bus virtio-bus name &amp;quot;virtserialport&amp;quot;, bus virtio-serial-bus Display devices: name &amp;quot;virtio-gpu-device&amp;quot;, bus virtio-bus Misc devices: name &amp;quot;loader&amp;quot;, desc &amp;quot;Generic Loader&amp;quot; name &amp;quot;virtio-balloon-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-crypto-device&amp;quot;, bus virtio-bus name &amp;quot;virtio-rng-device&amp;quot;, bus virtio-bus  所以要实现网卡的话，只能实现这里的 virtio-net-device ，而 VirtIO 驱动之间有很多共通的地方，于是顺带把 gpu mouse 和 blk 实现了。</description>
    </item>
    
  </channel>
</rss>