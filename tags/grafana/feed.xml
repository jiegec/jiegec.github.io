<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>grafana on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/grafana/</link>
    <description>Recent content in grafana on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 13 Jan 2019 18:36:00 +0000</lastBuildDate><atom:link href="https://jia.je/tags/grafana/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Grafana 中可视化 Ping 时把 Timeout 显示为指定值</title>
      <link>https://jia.je/software/2019/01/13/grafana-influxdb-visualize-ping/</link>
      <pubDate>Sun, 13 Jan 2019 18:36:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2019/01/13/grafana-influxdb-visualize-ping/</guid>
      <description>刚遇到一个需求，就是用 Telegraf 收集 ping 信息，然后在 Grafana 里可视化当前的延迟，如果超时了，就显示一个指定值，如 999 ，这样就可以放到一个 Gauge 里面可视化了。但是，问题在于，Telegraf 的 ping input 在超时的时候只会在 result_code 里写一个 2 ，其他项都是空的，因而如果直接用 GROUP BY time(interval) fill(999) 会导致最新的一个数据经常得到 999 。这意味着需要根据 &amp;ldquo;result_code&amp;rdquo; 来进行区分 Timeout 的情况。最后捣腾了很久，得到了这个方案：
 select &amp;quot;average_response_ms&amp;quot; * (2 - &amp;quot;result_code&amp;quot;) / 2 + &amp;quot;result_code&amp;quot; / 2 * 999 from (select &amp;quot;average_response_ms&amp;quot;, &amp;quot;result_code&amp;quot; from ping where $timeFilter fill(0)) 最后的方法很粗糙：当 &amp;ldquo;result_code&amp;rdquo; 是 0 也就是成功的时候，得到延迟，而当 &amp;ldquo;result_code&amp;rdquo; 是 2 也就是超时的时候，直接得到 999 。这样就解决了这个问题。</description>
    </item>
    
    <item>
      <title>Grafana Variable 的 regex 过滤方式</title>
      <link>https://jia.je/software/2019/01/10/grafana-variable-regex-exclusion/</link>
      <pubDate>Thu, 10 Jan 2019 12:47:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2019/01/10/grafana-variable-regex-exclusion/</guid>
      <description>用 InfluxDB 收集到 Mountpoint 数据的时候，经常会掺杂一些不关心的，如 TimeMachine ，KSInstallAction 和 AppTranslocation 等等。所以，为了在 Variables 里过滤掉他们，需要用 Regex 进行处理。网上有人提供了方案，就是通过 Negative Lookahead 实现：
/^(?!.*TimeMachine)(?!.*KSInstallAction)(?!.*\/private)/ 这样就可以把不想看到的这些 mountpoint 隐藏，节省页面空间了。当然了，这里其实也可以用白名单的方法进行处理，直接写 regex 就可以了。</description>
    </item>
    
    <item>
      <title>《加速奔向2019》小程序编写和运营回顾</title>
      <link>https://jia.je/software/2018/12/27/wxapp-recap/</link>
      <pubDate>Thu, 27 Dec 2018 19:56:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2018/12/27/wxapp-recap/</guid>
      <description>前言 关注清华的同学可能知道，昨天，“清华大学”公众号发了一篇名为《2018，我们共芳华丨@THUers 致相伴一年的你，请查收这份心意》的推送，内容大概就是，有那么100个新年台历礼品要送出去，大家如果想要的话，就扫描小程序。小程序模仿了火车抢票的病毒式营销的模式，要求大家分享到群聊或者朋友圈，让别人给自己加速，加速到 2019 的前 100 名即可填写信息领取奖品。
然后大家就在推送里看到了我。就酱。
开始 这件事情据说策划了有一段时间了，只是因为各种原因一直没有做，最后这个锅就路由到了我的头上。一开始说就是个加速小程序，逻辑很简单，但后来逐渐发现需求越来越多，主要是界面上的，动画上的，还有一些非技术因素的功能，嗯。这其实算是一个不大好的软件工程案例。
过程 线上的问题与解决方案 然后就是上线了。大概是昨天（2018-12-27）中午的时候推送发出去，很快流量就开始来了。很快，在朋友圈看到有同学在转发了，也有人反映说，网络有点卡，加载资源有点多。我去机器上用 iftop 看了下，流量大概是 250Mb/s ，没打到千兆。我一开始看了下，CPU 和内存占用都良好，以为是网络出口限制的问题，就想着没办法了，就这样吧，扛过去再说。不过，忽然有了转机。
TUNA 技术群里，忽然有人在讨论 SOMAXCONN 的问题，我想到，会不会是有些参数没开够大，导致了性能瓶颈，又受到啊荣的点拨，立马调整了这些变量：
net.core.somaxconn fs.file-max net.core.netdev_max_backlog net.ipv4.tcp_max_syn_backlog nginx: worker_rlimit_nofile nginx: event.worker_connections 很快带宽从 200Mb/s 左右打到了 400Mb/s 多，在 iftop 中看到的峰值接近 600Mb/s，见下图：
事后回来看，发现配置一套科学的监控系统真的很有用，如 TCP 连接的状态图：
这里最高的黄线代表的是 TIME_WAIT ，意味着很多的 TCP 连接都卡在了等待资源上，而一当我修改参数以后，立刻就降了下来，ESTBALISHED 的连接有了显著的提升。这个问题从另一个图也可以明地看出：
这个图是 TCP Handshake Issues ，可以看到无论是 activeopen 还是 passiveopen ，都很高，意味着这里无论是发还是收都遇到了问题。而修改参数以后，这些问题立马得到了很好的改善。
其实这些本应该在上线前做好的，但我低估了清华大学的影响力，没有做好相应的准备，还是在优秀的运维人员的指点下得到了较好的效果。
用户数据分析 当然了，除了 Grafana+InfluxDB+Telegraf 这一套监控系统，我们也部署了 ElasticSearch+Logstash+Kibana ，只不过我们还是用 Grafana 做了 ElasticSearch 的前端了。通过对 Nginx 日志的分析，我们得到了这些关键的数据（从12-26 12:00到12-27 12:00一天时间）：</description>
    </item>
    
    <item>
      <title>Grafana 可视化实践：清华大学 2018 年度人物评选</title>
      <link>https://jia.je/software/2018/12/07/grafana-visualize-vote18/</link>
      <pubDate>Fri, 07 Dec 2018 23:03:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2018/12/07/grafana-visualize-vote18/</guid>
      <description>最近这段时间，清华内部正在投票选出今年的年度人物，想到最近刚好在学习使用 Grafana+InfluxDB+Telegraf 全家桶，于是想着能不能写个爬虫把数据都拿下来，然后用 Grafana 画出来，就可以得到一个投票随时间变化的趋势。爬虫很简单，就是登录，获取页面信息，然后按照 InfluxDB 的输入格式进行输出即可。代码放在了 jiegec/student-tsinghua-vote18 下。
接着就是用 Grafana 进行可视化，大概得到了这样一个曲线：
为保护隐私，把名字隐去了。实际上的投票时间是从 12-3 号开始到 12-7号结束，但由于宿舍停电的原因所以采样的点在半夜的时候都没有，所以看起来有点奇怪，但还是能够反应总体的趋势的。比如可以看到前两名很早就一马当先，而后一直遥遥领先，下面的选手则排名变动很大，特别是截止前最后一段时间，大家都在拼命拉票，可见大家都是 DDL 选手啊。如果对上面这个图求个导，看看变化率的话：
这显现出了很有意思的一个趋势，就是每天十二点左右都有一个高峰期，然后在零点前大概熄灯附近的时间也是一个高峰期，另外就是截止前最后的抢票阶段，大家都在疯狂拉票，从中午拉到最后时刻。由于停电的原因，在零点附近的数据都比较的鬼畜，不过影响不大，趋势一目了然。
Grafana 真香！期望可以学到更多高端的查询语法和可视化的骚操作，现在有很多东西不知道该怎么可视化，比较苦恼，不知道大家有没有什么经验可以分享。</description>
    </item>
    
    <item>
      <title>配置 Grafana&#43;InfluxDB&#43;Telegraf 并添加 MIIO 数据来源</title>
      <link>https://jia.je/software/2018/11/27/grafana-influxdb-telegraf-miio/</link>
      <pubDate>Tue, 27 Nov 2018 20:33:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2018/11/27/grafana-influxdb-telegraf-miio/</guid>
      <description>之前一直想配一个监控系统，现在有机会了，就简单配了一下。发现真的特别简单，用 Homebrew 安装这三个软件并且都跑起来，然后稍微动一下配置，就可以得到可观的效果了。
然后想利用 miio 配置一下，把宿舍的空气净化器各项参数拿到，以 Telegraf 的插件形式定时上报，然后通过 Grafana 进行可视化。插件放在了 jiegec/tools 下，就是一个简单的 Python 脚本。配置方法如下：
编辑 /usr/local/etc/telegraf.d/miio.conf：
[[inputs.exec]] commands = [&amp;#34;/usr/local/bin/python3 /Volumes/Data/tools/telegraf/miio.py MIID_HERE&amp;#34;] timeout = &amp;#34;5s&amp;#34; data_format = &amp;#34;influx&amp;#34; 默认了 miio 路径为 /usr/local/bin/miio 。</description>
    </item>
    
  </channel>
</rss>
