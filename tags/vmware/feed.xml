<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vmware on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/vmware/</link>
    <description>Recent content in vmware on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 24 Sep 2022 14:18:00 +0800</lastBuildDate><atom:link href="https://jia.je/tags/vmware/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ESXi 配置 LACP 链路聚合</title>
      <link>https://jia.je/devops/2022/09/24/vmware-esxi-lacp/</link>
      <pubDate>Sat, 24 Sep 2022 14:18:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2022/09/24/vmware-esxi-lacp/</guid>
      <description>背景 给 ESXi 接了两路 10Gbps 的以太网，需要用 LACP 来聚合。ESXi 自己不能配置 LACP，需要配合 vCenter Server 的 Distributed Switch 来配置。
步骤 参考文档：LACP Support on a vSphere Distributed Switch
第一步是创建一个 Distributed Switch。找到 Cluster，点击 ACTIONS，在 Distributed Switch 里面选择 New Distributed Switch。里面的选项都可以用默认的，按需修改。
第二步，找到刚刚创建的 Distributed Switch，点击 Configure，在 Settings 下点击 LACP，点击 NEW，选项可以用默认的，按需修改。
第三步，找到 Distributed Switch，点击 ACTIONS，点击 Add and Manage Hosts，找到要配置的主机，在 Manage physical adapters 这一步，找到要加入到链路聚合的 vmnic，每个要聚合的 vmnic 都在右边的 Assign uplink 处选择刚刚创建的 LAG 下的 Uplink，按顺序，一一对应。其余选项可以使用默认的。这一步配置好以后，在交换机上应该就可以看到 LACP 正常运转。
第四步，如果要把虚拟机连到链路聚合的网络上，找到虚拟机，点击 ACTIONS，点击 Edit Settings，新建一个网卡，Network adapter 处选择刚刚创建的 Distributed Port Group。这一步是让虚拟机多一个网卡，可以连接到 Distributed Switch 上。这一步配置好以后，虚拟机就可以收到来自其他物理机的网络流量，但是发送不出去。</description>
    </item>
    
    <item>
      <title>NUC11 ESXi 中 iGPU 直通虚拟机</title>
      <link>https://jia.je/system/2022/05/05/nuc11-igpu-passthrough/</link>
      <pubDate>Thu, 05 May 2022 10:24:00 +0800</pubDate>
      
      <guid>https://jia.je/system/2022/05/05/nuc11-igpu-passthrough/</guid>
      <description>背景 之前在 NUC11PAKi5 上装了 ESXI 加几个虚拟机系统，但是自带的 iGPU Intel Iris Xe Graphics(Tiger Lake GT-2) 没用上，感觉有些浪费。因此想要给 Windows 直通。在直通到 Windows 后发现会无限重启，最后直通到 Linux 中。
步骤 第一步是到 esxi 的设备设置的地方，把 iGPU 的 Passthrough 打开，这时候会提示需要重启，但是如果重启，会发现还是处于 Needs reboot 状态。网上进行搜索，发现是 ESXi 自己占用了 iGPU 的输出，解决方法如下：
$ esxcli system settings kernel set -s vga -v FALSE 这样设置以后就不会在显卡输出上显示 dcui 了，这是一个比较大的缺点，但是平时也不用自带的显示输出，就无所谓了。
第二步，重启以后，这时候看设备状态就是 Active。回到 Windows 虚拟机，添加 PCI device，然后启动。这时候，我遇到了这样的错误：
Module ‘DevicePowerOn’ power on failed Failed to register the device pciPassthru0 搜索了一番，解决方法是关掉 IOMMU。在虚拟机设计中关掉 IOMMU，就可以正常启动了。
第三步，进入 Windows，这时候就可以看到有一个新的未知设备了，VID=8086，PID=9a49；等待一段时间，Windows 自动安装好了驱动，就可以正常识别了。GPU-Z 中可以看到效果如下：</description>
    </item>
    
    <item>
      <title>在 M1 上运行 Windows ARM 虚拟机</title>
      <link>https://jia.je/os/2022/01/30/windows-on-arm-on-m1/</link>
      <pubDate>Sun, 30 Jan 2022 20:50:00 +0800</pubDate>
      
      <guid>https://jia.je/os/2022/01/30/windows-on-arm-on-m1/</guid>
      <description>目前 Windows ARM 出了预览版，可以从 Windows Insider Preview Downloads 下载，得到一个 9.5GB 的 vhdx 文件。
接着，用 qemu-img 转换为 vmdk 格式：
$ qemu-img convert Windows11_InsiderPreview_Client_ARM64_en-us_22533.vhdx -O vmdk -o adapter_type=lsilogic Windows11_InsiderPreview_Client_ARM64_en-us_22533.vmdk 转换后，在 VMWare Fusion for Apple Silicon Tech Preview 中，选择从已有的 vmdk 中创建虚拟机，启动前修改一些设置，特别是内存，默认 256MB 肯定不够，默认单核 CPU 也太少了一些。内存不足可能导致安装失败，记住要第一次启动前设置。
启动以后会无法访问网络，按照下面网页里的方法设置网络：
https://www.gerjon.com/vmware/vmware-fusion-on-apple-silicion-m1/
需要注意的是，bcdedit 选项填的 IP 地址一般是 bridge 上的地址，比如 bridge101 的地址。
然后就可以正常工作了！
在 VMWare 论坛里，还谈到了下面几个问题的解决方法：
为了让声音工作，可以修改 vmx 文件，设置 guestOS：
guestOS = &amp;#34;arm-windows11-64&amp;#34; 这样声音就可以正常播放了。
分辨率的问题，可以用 RDP 来解决：首先在虚拟机里打开 Remote Desktop，然后用 macOS 的 Microsoft Remote Desktop Beta 访问即可。</description>
    </item>
    
    <item>
      <title>ESXi 常用信息</title>
      <link>https://jia.je/devops/2021/10/05/vmware-esxi-notes/</link>
      <pubDate>Tue, 05 Oct 2021 21:19:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/10/05/vmware-esxi-notes/</guid>
      <description>常用链接 检查 CPU microcode 版本： vsish -e cat /hardware/cpu/cpuList/0 | grep -i -E &amp;#39;family|model|stepping|microcode|revision&amp;#39; ESXi 从 6.7 到 6.7U1 升级时出现版本问题 ESXi 6.7 OEM 版本下载 ESXi 7.0 OEM 版本下载 ESXi 7.0 标准版下载 NUC 11 ESXi 7.0 网卡支持 $ esxcli software vib install -d $PWD/Net-Community-Driver_1.2.0.0-1vmw.700.1.0.15843807_18028830.zip 离线升级方法 下载 Offline Bundle 文件 上传到 ESXi datastore 中 在 /vmfs/volumes/ 里找到更新文件 查询 profile 列表 esxcli software sources profile list -d &amp;lt;zip&amp;gt; 更新到 profile esxcli software profile update -p &amp;lt;profile&amp;gt; -d &amp;lt;zip&amp;gt; ref: Upgrade or Update a Host with Image Profiles</description>
    </item>
    
    <item>
      <title>在 ESXi 中用 PERCCli 换 RAID 中的盘</title>
      <link>https://jia.je/devops/2021/04/15/vmware-esxi-perccli/</link>
      <pubDate>Thu, 15 Apr 2021 14:31:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2021/04/15/vmware-esxi-perccli/</guid>
      <description>背景 最近有一台机器的盘出现了报警，需要换掉，然后重建 RAID5 阵列。iDRAC 出现报错：
Disk 2 in Backplane 1 of Integrated RAID Controller 1 is not functioning correctly. Virtual Disk 1 on Integrated RAID Controller 1 has become degraded. Error occurred on Disk2 in Backplane 1 of Integrated RAID Controller 1 : (Error 2) 安装 PERCCli 首先，因为系统是 VMware ESXi 6.7，所以在DELL 官网下载对应的文件。按照里面的 README 安装 vib：
esxcli software vib install -v /vmware-perccli-007.1420.vib 如果要升级系统，需要先卸载 vib：esxcli software vib remove -n vmware-perccli，因为升级的时候会发现缺少新版系统的 perccli，建议先卸载，升级后再安装新的。
需要注意的是，如果复制上去 Linux 版本的 PERCCli，虽然也可以运行，但是找不到控制器。安装好以后，就可以运行 /opt/lsi/perccli/perccli 。接着，运行 perccli show all，可以看到类似下面的信息：</description>
    </item>
    
    <item>
      <title>在裸机上部署 ESXi 和 vCSA 7</title>
      <link>https://jia.je/devops/2020/10/18/deploy-esxi-vcsa-7/</link>
      <pubDate>Sun, 18 Oct 2020 00:08:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2020/10/18/deploy-esxi-vcsa-7/</guid>
      <description>之前在另一篇文章里提到过 vCSA 的安装，这次又在另一台机器上重新做了一遍，特此记录一下。
首先在官网上下载 ESXi+VCSA 7.0 ，应该得到两个文件：
7.9G VMware-VCSA-all-7.0.1-16860138.iso 358M VMware-VMvisor-Installer-7.0U1-16850804.x86_64.iso 首先安装 ESXi，用 UNetBootin 制作 ESXi 的安装光盘。注意不能用 dd，因为它是 CDFS 格式的，不能直接 boot。启动以后，按照界面要求，一路安装即可。
接着，就可以用网页访问 ESXi 进行配置。比如安装一些 Linux 发行版，然后在 Linux 虚拟机里面 mount 上面的 VCSA 的 iso：
sudo mount /dev/sr0 /mnt 接着，复制并修改 /mnt/vcsa-cli-installer/templates/install/embedded_vCSA_on_ESi.json，按照代码注释进行修改。需要注意几点：
密码都可以设为空，然后运行 cli 的时候输入 ESXi 的密码和 vCSA 的密码是不一样的 可以把 ceip 关掉，设置 ceip_enabled: false 接着，进行安装：
/mnt/vcsa-cli-installer/lin64/vcsa-deploy install --accept-eula /path/to/customized.json -v 慢慢等待它安装成功即可。
安装完成后，进入 vCSA，新建一个 Datacenter，然后选择新建的 Datacenter，选择 Add host，输入 ESXi 的地址和用户密码信息即可。</description>
    </item>
    
    <item>
      <title>在 macOS 上创建 ESP 镜像文件</title>
      <link>https://jia.je/software/2019/09/14/create-esp-partition-macos/</link>
      <pubDate>Sat, 14 Sep 2019 16:07:00 +0800</pubDate>
      
      <guid>https://jia.je/software/2019/09/14/create-esp-partition-macos/</guid>
      <description>最近 rCore 添加了 UEFI 支持，在 QEMU 里跑自然是没有问题，然后尝试放到 VMWare 虚拟机里跑，这时候问题就来了：需要一个带有 ESP 盘的 vmdk 虚拟盘。搜索了一下网络，找到了解决方案：
hdiutil create -fs fat32 -ov -size 60m -volname ESP -format UDTO -srcfolder esp uefi.cdr 其中 60m esp 和 uefi.cdr 都可以按照实际情况修改。它会把 esp 目录下的文件放到 ESP 分区中，然后得到一个镜像文件：
uefi.cdr: DOS/MBR boot sector; partition 1 : ID=0xb, start-CHS (0x3ff,254,63), end-CHS (0x3ff,254,63), startsector 1, 122879 sectors, extended partition table (last) 接着转换为 vmdk：
qemu-img convert -O vmdk uefi.cdr uefi.vmdk 这样就可以了。</description>
    </item>
    
    <item>
      <title>在 VMware ESXi 上部署 vCSA 实践</title>
      <link>https://jia.je/devops/2018/05/20/deploy-vcsa-under-esxi/</link>
      <pubDate>Sun, 20 May 2018 16:44:00 +0800</pubDate>
      
      <guid>https://jia.je/devops/2018/05/20/deploy-vcsa-under-esxi/</guid>
      <description>首先获取 vCSA 的 ISO 镜像，挂载到 Linux 下（如 /mnt），然后找到 /mnt/vcsa-cli-installer/templates/install 下的 embedded_vCSA_on_ESXi.json，复制到其它目录并且修改必要的字段，第一个 password 为 ESXi 的登录密码，一会在安装的过程中再输入。下面有个 deployment_option，根据你的集群大小来选择，我则是用的 small。下面配置这台机器的 IP 地址，用内网地址即可。下面的 system_name 如果要写 fqdn，记得要让这个域名可以解析到正确的地址，不然会安装失败，我因此重装了一次。下面的密码都可以留空，在命令行中输入即可。SSO 为 vSphere Client 登录时用的密码和域名，默认用户名为 Administrator@domain_name (默认的话，则是 Administrator@vsphere.local) 这个用户名在安装结束的时候也会提示。下面的 CEIP 我选择关闭，设置为 false。
接下来进行安装。
$ /mnt/vcsa-cli-installer/lin64/vcsa-deploy install /path/to/embedded_vCSA_on_ESXi.json --accept-eula 一路输入密码，等待安装完毕即可。然后通过 443 端口进入 vSphere Client, 通过 5480 端口访问 vCSA 的管理页面。两个的密码可以不一样。
2018-05-21 Update: 想要设置 VMKernel 的 IPv6 网关的话，ESXi 中没找到配置的地方，但是在 vSphere Client 中可以进行相关配置。</description>
    </item>
    
    <item>
      <title>使用 Nginx 转发 VMware ESXi</title>
      <link>https://jia.je/networking/2018/05/08/nginx-proxy-vmware-esxi/</link>
      <pubDate>Tue, 08 May 2018 19:26:00 +0800</pubDate>
      
      <guid>https://jia.je/networking/2018/05/08/nginx-proxy-vmware-esxi/</guid>
      <description>我们的 VMware ESXi 在一台 NAT Router 之后，但是我们希望通过域名可以直接访问 VMware ESXi。我们首先的尝试是，把 8443 转发到它的 443 端口，比如：
socat TCP-LISTEN:8443,reuseaddr,fork TCP:esxi_addr:443 它能工作地很好（假的，如果你把 8443 换成 9443 它就不工作了），但是，我们想要的是，直接通过 esxi.example.org 就可以访问它。于是，我们需要 Nginx 在其中做一个转发的功能。在这个过程中遇到了很多的坑，最后终于是做好了（VMware Remote Console 等功能还不行，需要继续研究）。
首先讲讲为啥把 8443 换成 9443 不能工作吧 &amp;ndash; 很简单，ESXi 的网页界面会请求 8443 端口。只是恰好我用 8443 转发到 443，所以可以正常工作。这个很迷，但是测试的结果确实如此。VMware Remote Console 还用到了别的端口，我还在研究之中。
来谈谈怎么配置这个 Nginx 转发吧。首先是 80 跳转 443:
server { listen 80; listen 8080; server_name esxi.example.org; return 301 https://$host$request_uri; } 这个很简单，接下来是转发 443 端口：
server { listen 443 ssl; server_name esxi.</description>
    </item>
    
  </channel>
</rss>
