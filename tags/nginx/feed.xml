<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nginx on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/nginx/</link>
    <description>Recent content in nginx on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jul 2021 00:16:00 +0800</lastBuildDate><atom:link href="https://jia.je/tags/nginx/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Nginx 处理 POST 请求出现 Internal Server Error 排查一则</title>
      <link>https://jia.je/software/2021/07/16/nginx-post-internal-server-error/</link>
      <pubDate>Fri, 16 Jul 2021 00:16:00 +0800</pubDate>
      
      <guid>https://jia.je/software/2021/07/16/nginx-post-internal-server-error/</guid>
      <description>前言 最近一个服务忽然出现问题，用户反馈，HTTP POST 一个小的 body 不会出错，POST 一个大的 body 就会 500 Internal Server Error。
排查 观察后端日志，发现没有出错的那一个请求。观察 Nginx 日志，发现最后一次日志是几个小时前。最后几条 Nginx 日志写的是 a client request body is buffered to a temporary file。
结论 继续研究后，发现是硬盘满了。Nginx 在处理 POST body 的时候，如果 body 超过阈值，会写入到临时文件中：
Syntax: client_body_buffer_size size; Default: client_body_buffer_size 8k|16k; Context: http, server, location Sets buffer size for reading client request body. In case the request body is larger than the buffer, the whole body or only its part is written to a temporary file.</description>
    </item>
    
    <item>
      <title>在 TKE 上配置不使用 LB 的 Nginx Ingress Controller</title>
      <link>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</link>
      <pubDate>Fri, 17 Apr 2020 17:30:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2020/04/17/tke-nginx-ingress-without-lb/</guid>
      <description>背景 想要在 k8s 里面 host 一个网站，但又不想额外花钱用 LB，想直接用节点的 IP。
方法 首先安装 nginx-ingress：
$ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update $ helm install ingress-controller nginx-stable/nginx-ingress --set controller.service.type=NodePort --set controller.hostNetwork=true 这里给 ingress controller chart 传了两个参数：第一个指定 service 类型是 NodePort，替代默认的 LoadBalancer；第二个指定 ingress controller 直接在节点上监听，这样就可以用节点的公网 IP 访问了。
然后配一个 ingress：
apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-example annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; spec: rules: - host: example.com http: paths: - path: / backend: serviceName: example-service servicePort: 80 然后就可以发现请求被正确路由到 example-service 的 80 端口了。</description>
    </item>
    
    <item>
      <title>用 Nginx 作为 RTMP 服务器并提供直播服务</title>
      <link>https://jia.je/software/2019/09/28/use-nginx-as-rtmp-server/</link>
      <pubDate>Sat, 28 Sep 2019 10:15:00 +0000</pubDate>
      
      <guid>https://jia.je/software/2019/09/28/use-nginx-as-rtmp-server/</guid>
      <description>Nginx 除了可以做 HTTP 服务器以外，还可以做 RTMP 服务器，同时转成 HLS 再提供给用户，这样可以实现一个直播的服务器，用 OBS 推上来即可。
首先要安装 nginx-rtmp-server 模块，很多的发行版都已经包含了，它的主页是 https://github.com/arut/nginx-rtmp-module ，下面很多内容也是来自于它的教程中。
接着，配置 Nginx ，在 nginx.conf 的顶层中添加如下的配置：
rtmp { server { listen 1935; chunk_size 4096; application live { live on; record off; hls on; hls_path /path/to/save/hls; hls_fragment 1s; hls_playlist_length 10s; } } } 这里表示 Nginx 要在 1935 监听一个 RTMP 服务器，然后把 live 下的视频切成片然后存在目录下，提供一个 m3u8 文件以供播放器使用。这里的参数都可以按照实际需求进行调整。这时候应该可以看到 nginx 正确监听1935 端口，这是 rtmp 的默认端口。
接着，需要在一个 HTTP server 路径下把 HLS serve 出去：
 location /hls { # Serve HLS fragments types { application/vnd.</description>
    </item>
    
    <item>
      <title>Nginx 反代到 HTTPS 上游</title>
      <link>https://jia.je/devops/2019/05/22/nginx-ssl-upstream/</link>
      <pubDate>Wed, 22 May 2019 16:01:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2019/05/22/nginx-ssl-upstream/</guid>
      <description>这次遇到一个需求，要反代到不在内网的地址，为了保证安全，还是得上 HTTPS ，所以尝试了一下怎么给 upstream 配置自签名 HTTPS 证书的验证。
upstream subpath { server 4.3.2.1:4321; } server { listen 443 ssl; server_name test.example.com; location /abc { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_ssl_trusted_certificate /path/to/self_signed_cert.crt; proxy_ssl_name 1.2.3.4; // to override server name checking proxy_ssl_verify on; proxy_ssl_depth 2; proxy_ssl_reuse on; proxy_pass https://subpath; } } 可以用 openssl 获得自签名的 cert :
echo | openssl s_client -showcerts -connect 4.3.2.1:4321 2&amp;gt;/dev/null | \ openssl x509 -text &amp;gt; /path/to/self_signed_cert.crt ref: https://stackoverflow.</description>
    </item>
    
    <item>
      <title>在 rCore 上运行 nginx</title>
      <link>https://jia.je/programming/2019/03/08/running-nginx-on-rcore/</link>
      <pubDate>Fri, 08 Mar 2019 18:07:00 +0000</pubDate>
      
      <guid>https://jia.je/programming/2019/03/08/running-nginx-on-rcore/</guid>
      <description>阿 西 吧 nginx 终于能在 rCore 上跑了 orrrrrrrz
通过这半个多月来的大量开发，我和王润基 @wangrunji0408 学长算是终于完成了第一个 milestone：跑起来一个 nginx 。遇到了很多困难，大概有这些：
 syscall 实现不全。各种方面都缺，然后 nginx 在编译的时候又检测到比较新的 OS 版本，所以很多 syscall 都用了新的来替代老的，例如 readv/writev pread/pwrite accept4 等等，所以这方面做了一些工作。另外，还有很多新的 syscall 进来，太多了我就不细说了，基本上一个commit做一点一个commit做一点这个样子。 nginx 用到了 SSE 的寄存器 xmm ，但是之前是没有开的。所以把 sse 打开，然后切换上下文的时候把 sse 通过 fxsave 保存和 fxrstor 恢复（有意思的是，as居然不认这俩，只好手动写字节码），然后为了 16bit 的对齐又写了几行汇编代码。这块问题不大，今天一会就搞定了。但是如果要性能更高一些的话，可能需要在第一次使用 xmm 的时候再开始保存，大概就是加一个bit的事情。 文件系统有点崩。实现还是有很多 BUG ，表现就是需要经常重新 mksfs 一下，再重启加载完好的 fs ，有时候强制关机一下就又崩了。 内存管理做了一些改变。为了实现更加完整的 mmap mumap 和 mprotect ，又发现了一些新的 BUG 在里面，然后慢慢修复了。就是实现的有点粗暴。 死锁问题。这个其实现在还会出现，只是还没调出来，也不会百分百出现。我们计划在锁上面做一些死锁检测，例如记住是谁上锁的，等等。现在就遇到一个很玄学的死锁问题。  然后代码也是一边在写一边在重构吧，很多地方现在都写得很粗暴，FIXME和TODO留了很多，很多地方也写得不够优雅。以后再慢慢重构+优化吧。
截图留念：
再往前的话，还有很多小的问题，例如网卡的中断启用了但没有改 mask ，所以啥也没收到，靠 QEMU Tracing 找到问题。还有一个很有意思的现象，就是如果 elf 的 program header 没有 phdr 这个项的时候，我们发现，可以通过第一个load（如果加载了完整的 elf 头的话），我们可以从这里推断出 phdr 的地址（load的虚拟地址加偏移），然后丢到 auxv 里去让 musl 配置 tls。总之这些都解决了。也不用去考虑兼容 litc 了，已经全部向 linux 靠拢了，稳。</description>
    </item>
    
    <item>
      <title>调整 Nginx 和 PHP 的上传文件大小限制</title>
      <link>https://jia.je/programming/2018/06/10/nginx-php-upload-size-limit/</link>
      <pubDate>Sun, 10 Jun 2018 16:04:00 +0000</pubDate>
      
      <guid>https://jia.je/programming/2018/06/10/nginx-php-upload-size-limit/</guid>
      <description>之前迁移的 MediaWiki ，有人提出说无法上传一个 1.4M 的文件。我去看了一下网站，上面写的是限制在 2M ，但是一上传就说 Entity Too Large，无法上传。后来经过研究，是 Nginx 对 POST 的大小进行了限制，同时 PHP 也有限制。
Nginx 的话，可以在 nginx.conf 的 http 中添加，也可以在 server 或者 location 中加入这么一行：
client_max_body_size 100m; 我的建议是，尽量缩小范围到需要的地方，即 location &amp;gt; server &amp;gt; http 。
在 PHP 中，则修改 /etc/php/7.0/fpm/php.ini ：
post_max_size = 100M 回到 MediaWiki 的上传页面，可以看到显示的大小限制自动变成了 100M ，这个是从 PHP 的配置中直接获得的。</description>
    </item>
    
    <item>
      <title>在 Nginx 将某个子路径反代</title>
      <link>https://jia.je/devops/2018/06/01/nginx-proxy-subpath/</link>
      <pubDate>Fri, 01 Jun 2018 07:57:00 +0000</pubDate>
      
      <guid>https://jia.je/devops/2018/06/01/nginx-proxy-subpath/</guid>
      <description>现在遇到这么一个需求，访问根下面是提供一个服务，访问某个子路径（/abc），则需要提供另一个服务。这两个服务处于不同的机器上，我们现在通过反代把他们合在一起。在配置这个的时候，遇到了一些问题，最后得以解决。
upstream root { server 1.2.3.4:1234; } upstream subpath { server 4.3.2.1:4321; } server { listen 443 ssl; server_name test.example.com; # the last slash is useful, see below location /abc/ { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # the last slash is useful too, see below proxy_pass http://subpath/; } location / { proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://root; } } 由于并不想 subpath 他看到路径中 /abc/ 这一层，导致路径和原来在根下不同，通过这样配置以后，特别是两个末尾的斜杠，可以让 nginx 把 GET /abc/index.</description>
    </item>
    
    <item>
      <title>使用 Nginx 转发 VMware ESXi</title>
      <link>https://jia.je/networking/2018/05/08/nginx-proxy-vmware-esxi/</link>
      <pubDate>Tue, 08 May 2018 19:26:00 +0000</pubDate>
      
      <guid>https://jia.je/networking/2018/05/08/nginx-proxy-vmware-esxi/</guid>
      <description>我们的 VMware ESXi 在一台 NAT Router 之后，但是我们希望通过域名可以直接访问 VMware ESXi 。我们首先的尝试是，把 8443 转发到它的 443 端口，比如：
socat TCP-LISTEN:8443,reuseaddr,fork TCP:esxi_addr:443 它能工作地很好（假的，如果你把 8443 换成 9443 它就不工作了），但是，我们想要的是，直接通过 esxi.example.org 就可以访问它。于是，我们需要 Nginx 在其中做一个转发的功能。在这个过程中遇到了很多的坑，最后终于是做好了 （VMware Remote Console等功能还不行，需要继续研究）。
首先讲讲为啥把 8443 换成 9443 不能工作吧 &amp;ndash; 很简单，ESXi 的网页界面会请求 8443 端口。只是恰好我用 8443 转发到 443， 所以可以正常工作。这个很迷，但是测试的结果确实如此。VMware Remote Console 还用到了别的端口，我还在研究之中。
来谈谈怎么配置这个 Nginx 转发吧。首先是 80 跳转 443:
server { listen 80; listen 8080; server_name esxi.example.org; return 301 https://$host$request_uri; } 这个很简单，接下来是转发 443 端口：
 server { listen 443 ssl; server_name esxi.</description>
    </item>
    
  </channel>
</rss>
