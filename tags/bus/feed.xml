<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bus on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/bus/</link>
    <description>Recent content in bus on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jan 2023 00:10:00 +0800</lastBuildDate><atom:link href="https://jia.je/tags/bus/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CHI 学习笔记</title>
      <link>https://jia.je/hardware/2023/01/12/chi-notes/</link>
      <pubDate>Thu, 12 Jan 2023 00:10:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2023/01/12/chi-notes/</guid>
      <description>本文的内容已经整合到知识库中。
CHI 介绍 CHI 协议是 AMBA 5 标准中的缓存一致性协议，前身是 ACE 协议。最新的 CHI 标准可以从 AMBA 5 CHI Architecture Specification 处下载。
相比 AXI，CHI 更加复杂，进行了分层：协议层，物理层和链路层。因此，CHI 适用于片上网络，支持根据 Node ID 进行路由，而不像 AXI 那样只按照物理地址进行路由。CHI 的地位就相当于 Intel 的环形总线。CHI 也可以桥接到 CCIX 上，用 CCIX 连接 SMP 的的多个 Socket，或者连接支持 CCIX 的显卡等等。
缓存行状态 首先回顾 ACE 的缓存行状态，共有五种，与 MOESI 相对应：
UniqueDirty: Modified SharedDirty: Owned UniqueClean: Exclusive SharedClean: Shared Invalid: Invalid 在此基础上，CHI 考虑缓存行只有部分字节有效的情况，即 Full，Partial 或者 Empty。因此 CHI 的缓存行状态共有七种：
UniqueDirty: Modified UniqueDirtyPartial: 新增，可能有部分字节合法，在写回的时候，需要和下一级缓存或者内存中的合法缓存行内容进行合并 SharedDirty: Owned UniqueClean: Exclusive UniqueCleanEmpty: 新增，所有字节都不合法，但是本缓存占有该缓存行，如果要修改的话，不需要通知其他缓存 SharedClean: Shared Invalid: Invalid 可以看到，比较特别的就是 UniqueDirtyPartial 和 UniqueCleanEmpty。CHI 标准在 4.</description>
    </item>
    
    <item>
      <title>PCIe Bifurcation</title>
      <link>https://jia.je/hardware/2023/01/05/pcie-bifurcation/</link>
      <pubDate>Thu, 05 Jan 2023 15:41:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2023/01/05/pcie-bifurcation/</guid>
      <description>本文的内容已经整合到知识库中。
背景 最近看到两篇关于 PCIe Bifurcation 的文章：
intel 部分桌面级 CPU 的 pcie 通道拆分另类低成本实现 Intel Alder Lake 12 代酷睿 CPU PCIe 拆分实现方法 文章讲的是如何在 CPU 上进行跳线，从而实现 PCIe Bifurcation 的配置。正好借此机会来研究一下 PCIe Bifurcation。
PCIe Bifurcation PCIe Bifurcation 的目的是让 PCIe 有更好的灵活性。从 CPU 出来的几路 PCIe，它的宽度一般是确定的，比如有一个 x16，但是实际使用的时候，想要接多个设备，例如把 x16 当成两个 x8 来用，这就是 PCIe Bifurcation。这需要 PCIe 两端的支持，CPU 端需要可配置 PCIe Bifurcation，不然只能从一个 x16 降级到一个 x8，剩下的 8x 就没法利用了；设备端需要拆分卡，把 x16 的信号分成两路，然后提供两个 PCIe 插槽以及使用 Clock Buffer 来提供下游设备的时钟，有时则是主板设计时就做了拆分，不需要额外的拆分卡。
那么怎么配置 CPU 端的 PCIe Bifurcation 呢？其实就是上面两篇文章提到的办法：CPU 根据 CFG 信号来决定 PCIe Bifurcation 配置，例如要选择 1x16，2x8 还是 1x8+2x4 等等。简单总结一下实现思路都是：</description>
    </item>
    
    <item>
      <title>CXL 学习笔记</title>
      <link>https://jia.je/hardware/2022/11/20/cxl-notes/</link>
      <pubDate>Sun, 20 Nov 2022 23:05:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/11/20/cxl-notes/</guid>
      <description>本文的内容已经整合到知识库中。
背景 前段时间学习了 PCIe，趁此机会，进一步学习一下密切相关的 CXL。
CXL 的标准是公开下载的：https://www.computeexpresslink.org/download-the-specification，我目前参考的是 2022 年 8 月 1 日的 CXL 3.0 版本。
CXL 设备类型 CXL 对 PCIe 的重要的扩展，一是在于让设备可以和 CPU 实现缓存一致性（CXL.cache），二是可以做远程的内存（CXL.mem）。
具体下来，CXL 标准主要定义了三类设备：
CXL Type 1: 设备带有与 CPU 一致的缓存，实现 CXL.io 和 CXL.cache CXL Type 2: 设备带有自己的内存和与 CPU 一致的缓存，实现 CXL.io，CXL.cache 和 CXL.mem CXL Type 3: 设备带有自己的内存，实现 CXL.io 和 CXL.mem CXL 传输层 CXL.io CXL.io 基本上就是 PCIe 协议：
CXL.io provides a non-coherent load/store interface for I/O devices. Figure 3-1 shows where the CXL.</description>
    </item>
    
    <item>
      <title>PCIe 学习笔记</title>
      <link>https://jia.je/hardware/2022/11/12/pcie-notes/</link>
      <pubDate>Sat, 12 Nov 2022 14:56:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/11/12/pcie-notes/</guid>
      <description>本文的内容已经整合到知识库中。
背景 最近在知乎上看到 LogicJitterGibbs 的 资料整理：可以学习 1W 小时的 PCIe，我跟着资料学习了一下，然后在这里记录一些我学习 PCIe 的笔记。
下面的图片主要来自 PCIe 3.0 标准以及 MindShare 的 PCIe 3.0 书本。
分层 PCIe 定义了三个层：Transaction Layer，Data Link Layer，Physical Layer，和 TCP/IP 四层模型很像。PCIe 也是基于 Packet 传输的。
Transaction Layer Transaction Layer 的核心是 Transaction Layer Packet(TLP)。TLP 格式：
即可选的若干个 Prefix，一个 Header，可选的 Data Payload，可选的 Digest。
Prefix 和 Header 开头的一个字节是 Fmt[2:0] 和 Type[4:0] 字段。Fmt 决定了 header 的长度，有无数据，或者这是一个 Prefix。
它支持几类 Packet：
Memory: MMIO Read Request(MRd)/Completion(CplD) Write Request(MWr): 注意只有 Request，没有 Completion AtomicOp Request(FetchAdd/Swap/CAS)/Completion(CplD) Locked Memory Read(MRdLk)/Completion(CplDLk): Legacy IO: Legacy Read Request(IORd)/Completion(CplD) Write Request(IOWr)/Completion(Cpl) Configuration: 访问配置空间 Read Request(CfgRd0/CfgRd1)/Completion(CplD) Write Request(CfgWr0/CfgWr1)/Completion(Cpl) Message: 传输 event Request(Msg/MsgD) 括号里的是 TLP Type，对应了它 Fmt 和 Type 字段的取值。如果 Completion 失败了，原来应该是 CplD/CplDLk 的 Completion 会变成不带数据的 Cpl/CplLk。</description>
    </item>
    
    <item>
      <title>「教学」Wishbone 总线协议</title>
      <link>https://jia.je/hardware/2022/06/19/wishbone/</link>
      <pubDate>Sun, 19 Jun 2022 17:05:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/06/19/wishbone/</guid>
      <description>本文的内容已经整合到知识库中。
背景 最近在研究如何把 Wishbone 总线协议引入计算机组成原理课程，因此趁此机会学习了一下 Wishbone 的协议。
总线 总线是什么？总线通常用于连接 CPU 和外设，为了更好的兼容性和可复用性，会想到能否设计一个统一的协议，其中 CPU 实现的是发起请求的一方（又称为 master），外设实现的是接收请求的一方（又称为 slave），那么如果要添加外设、或者替换 CPU 实现，都会变得比较简单，减少了许多适配的工作量。
那么，我们来思考一下，一个总线协议需要包括哪些内容？对于 CPU 来说，程序会读写内存，读写内存就需要以下几个信号传输到内存：
地址（addr）：例如 32 位处理器就是 32 位地址，或者按照内存的大小计算地址线的宽度 数据（w_data 和 r_data）：分别是写数据和读数据，宽度通常为 32 位 或 64 位，也就是一个时钟周期可以传输的数据量 读还是写（we）：高表示写，低表示读 字节有效（be）：例如为了实现单字节写，虽然 w_data 可能是 32 位宽，但是实际写入的是其中的一个字节 除了请求的内容以外，为了表示 CPU 想要发送请求，还需要添加 valid 信号：高表示发送请求，低表示不发送请求。很多时候，外设的速度比较慢，可能无法保证每个周期都可以处理请求，因此外设可以提供一个 ready 信号：当 valid=1 &amp;amp;&amp;amp; ready=1 的时候，发送并处理请求；当 valid=1 &amp;amp;&amp;amp; ready=0 的时候，表示外设还没有准备好，此时 CPU 需要一直保持 valid=1 不变，等到外设准备好后，valid=1 &amp;amp;&amp;amp; ready=1 请求生效。
简单总结一下上面的需求，可以得到 master 和 slave 端分别的信号列表。这次，我们在命名的时候用 _o 表示输出、_i 表示输入，可以得到 master 端（CPU 端）的信号：</description>
    </item>
    
    <item>
      <title>「教学」ACE 缓存一致性协议</title>
      <link>https://jia.je/hardware/2022/05/16/ace/</link>
      <pubDate>Mon, 16 May 2022 00:34:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/16/ace/</guid>
      <description>本文的内容已经整合到知识库中。
背景 最近几天分析了 TileLink 的缓存一致性协议部分内容，见TileLink 总线协议分析，趁此机会研究一下之前尝试过研究，但是因为缺少一些基础知识而弃坑的 ACE 协议分析。
下面主要参考了 IHI0022E 的版本，也就是 AXI4 对应的 ACE 版本。
回顾 首先回顾一下一个缓存一致性协议需要支持哪些操作。对于较上一级 Cache 来说，它需要这么几件事情：
读或写 miss 的时候，需要请求这个缓存行的数据，并且更新自己的状态，比如读取到 Shared，写入到 Modified 等。 写入一个 valid &amp;amp;&amp;amp; !dirty 的缓存行的时候，需要升级自己的状态，比如从 Shared 到 Modified。 需要 evict 一个 valid &amp;amp;&amp;amp; dirty 的缓存行的时候，需要把 dirty 数据写回，并且降级自己的状态，比如 Modified -&amp;gt; Shared/Invalid。如果需要 evict 一个 valid &amp;amp;&amp;amp; !dirty 的缓存行，可以选择通知，也可以选择不通知下一级。 收到 snoop 请求的时候，需要返回当前的缓存数据，并且更新状态。 需要一个方法来通知下一级 Cache/Interconnect，告诉它第一和第二步完成了。 如果之前看过我的 TileLink 分析，那么上面的这些操作对应到 TileLink 就是：
读或写 miss 的时候，需要请求这个缓存行的数据（发送 AcquireBlock，等待 GrantData），并且更新自己的状态，比如读取到 Shared，写入到 Modified 等。 写入一个 valid &amp;amp;&amp;amp; !</description>
    </item>
    
    <item>
      <title>「教学」内存认证算法</title>
      <link>https://jia.je/hardware/2022/05/10/memory-authentication/</link>
      <pubDate>Tue, 10 May 2022 20:28:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/10/memory-authentication/</guid>
      <description>背景 之前 @松 给我讲过一些内存认证（Memory Authentication）算法的内容，受益匪浅，刚好今天某硬件群里又讨论到了这个话题，于是趁此机会再学习和整理一下相关的知识。
内存认证计算的背景是可信计算，比如要做一些涉及重要数据的处理，从软件上，希望即使系统被攻击非法进入了，也可以保证重要信息不会泄漏；从硬件上，希望即使系统可以被攻击者进行一些物理的操作（比如导出或者修改内存等等），也可以保证攻击者无法读取或者篡改数据。
下面的内容主要参考了 Hardware Mechanisms for Memory Authentication: A Survey of Existing Techniques and Engines 这篇 2009 年的文章。
威胁模型 作为一个防御机制，首先要确定攻击方的能力。一个常见的威胁模型是认为，攻击者具有物理的控制，可以任意操控内存中的数据，但是无法读取或者修改 CPU 内部的数据。也就是说，只有 CPU 芯片内的数据是可信的，离开了芯片都是攻击者掌控的范围。一个简单的想法是让内存中保存的数据是加密的，那么怎样攻击者可以如何攻击加密的数据？下面是几个典型的攻击方法：
Spoofing attack：把内存数据改成任意攻击者控制的数据；这种攻击可以通过签名来解决 Splicing or relocation attack：把某一段内存数据挪到另一部分，这样数据的签名依然是正确的；所以计算签名时需要把地址考虑进来，这样地址变了，验证签名就会失败 Replay attack：如果同一个地址的内存发生了改变，攻击者可以把旧的内存数据再写进去，这样签名和地址都是正确的；为了防止重放攻击，还需要引入计数器或者随机 nonce Authentication Primitives 为了防御上面几种攻击方法，上面提到的文章里提到了如下的思路：
一是 Hash Function，把内存分为很多个块，每一块计算一个密码学 Hash 保存在片内，那么读取数据的时候，把整块数据读取进来，计算一次 Hash，和片内保存的结果进行比对；写入数据的时候，重新计算一次修改后数据的 Hash，更新到片内的存储。这个方法的缺点是没有加密，攻击者可以看到内容，只不过一修改就会被 CPU 发现（除非 Hash 冲突），并且存储代价很大：比如 512-bit 的块，每一块计算一个 128-bit 的 Hash，那就浪费了 25% 的空间，而片内空间是十分宝贵的。
二是 MAC Function，也就是密码学的消息验证码，它需要一个 Key，保存在片内；由于攻击者不知道密码，根据 MAC 的性质，攻击者无法篡改数据，也无法伪造 MAC，所以可以直接把计算出来的 MAC 也保存到内存里。为了防御重放攻击，需要引入随机的 nonce，并且把 nonce 保存在片内，比如每 512-bit 的数据，保存 64-bit 的 nonce，这样片内需要保存 12.</description>
    </item>
    
    <item>
      <title>TileLink 总线协议分析</title>
      <link>https://jia.je/hardware/2022/05/09/tilelink/</link>
      <pubDate>Mon, 09 May 2022 16:15:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/09/tilelink/</guid>
      <description>本文的内容已经整合到知识库中。
背景 最近在研究一些支持缓存一致性的缓存的实现，比如 rocket-chip 的实现和 sifive 的实现，因此需要研究一些 TileLink 协议。本文讨论的时候默认读者具有一定的 AXI 知识，因此很多内容会直接参考 AXI。
信号 根据 TileLink Spec 1.8.0，TileLink 分为以下三种：
TL-UL: 只支持读写，不支持 burst，类比 AXI-Lite TL-UH：支持读写，原子指令，预取，支持 burst，类比 AXI+ATOP（AXI5 引入的原子操作） TL-C：在 TL-UH 基础上支持缓存一致性协议，类比 AXI+ACE/CHI TileLink Uncached TileLink Uncached(TL-UL 和 TL-UH) 包括了两个 channel：
A channel: M-&amp;gt;S 发送请求，类比 AXI 的 AR/AW/W D channel: S-&amp;gt;M 发送响应，类比 AXI 的 R/W 因此 TileLink 每个周期只能发送读或者写的请求，而 AXI 可以同时在 AR 和 AW channel 上发送请求。
一些请求的例子：
读：M-&amp;gt;S 在 A channel 上发送 Get，S-&amp;gt;M 在 D channel 上发送 AccessAckData 写：M-&amp;gt;S 在 A channel 上发送 PutFullData/PutPartialData，S-&amp;gt;M 在 D channel 是发送 AccessAck 原子操作：M-&amp;gt;S 在 A channel 上发送 ArithmeticData/LogicalData，S-&amp;gt;M 在 D channel 上发送 AccessAckData 预取操作：M-&amp;gt;S 在 A channel 上发送 Intent，S-&amp;gt;M 在 D channel 上发送 AccessAck AXI4ToTL 针对 AXI4ToTL 模块的例子，来分析一下如何把一个 AXI4 Master 转换为 TileLink。</description>
    </item>
    
  </channel>
</rss>
