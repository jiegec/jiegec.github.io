<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>riscv on 杰哥的{运维,编程,调板子}小笔记</title>
    <link>https://jia.je/tags/riscv/</link>
    <description>Recent content in riscv on 杰哥的{运维,编程,调板子}小笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 May 2022 09:22:00 +0800</lastBuildDate><atom:link href="https://jia.je/tags/riscv/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在 libvirt 中运行 RISC-V 虚拟机</title>
      <link>https://jia.je/software/2022/05/31/qemu-rv64-in-libvirt/</link>
      <pubDate>Tue, 31 May 2022 09:22:00 +0800</pubDate>
      
      <guid>https://jia.je/software/2022/05/31/qemu-rv64-in-libvirt/</guid>
      <description>背景 我在 libvirt 中跑了几个 KVM 加速的虚拟机，然后突发奇想，既然 libvirt 背后是 qemu，然后 qemu 是支持跨指令集的，那是否可以让 libvirt 来运行 RISC-V 架构的虚拟机？经过一番搜索，发现可以跑 ARM：How To: Running Fedora-ARM under QEMU，既然如此，我们也可以试试用 libvirt 来运行 RV64 虚拟机。
准备 rootfs 第一步是根据 Debian 的文档 Creating a riscv64 chroot 来创建 rootfs，然后再用 virt-make-fs 来打包。
首先是用 mmdebstrap 来生成一个 chroot：
$ sudo mkdir -p /tmp/riscv64-chroot $ sudo apt install mmdebstrap qemu-user-static binfmt-support debian-ports-archive-keyring $ sudo mmdebstrap --architectures=riscv64 --include=&amp;#34;debian-ports-archive-keyring&amp;#34; sid /tmp/riscv64-chroot &amp;#34;deb http://deb.debian.org/debian-ports sid main&amp;#34; &amp;#34;deb http://deb.debian.org/debian-ports unreleased main&amp;#34; 进入 chroot 以后，进行一些配置：</description>
    </item>
    
    <item>
      <title>向 Rocket Chip 添加自定义调试信号</title>
      <link>https://jia.je/hardware/2022/05/13/rocket-chip-custom-debug/</link>
      <pubDate>Fri, 13 May 2022 08:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/05/13/rocket-chip-custom-debug/</guid>
      <description>背景 最近在尝试把核心作为一个 Tile 加到 Rocket System 中，所以想要把核心之前自定义的调试信号接到顶层上去。Rocket System 自带的支持是 trace，也就是输出每个周期 retire 的指令信息，但和自定义的不大一样，所以研究了一下怎么添加自定义的调试信号，并且连接到顶层。
分析 Trace 信号连接方式 首先，观察 Rocket Chip 自己使用的 Trace 信号是如何连接到顶层的。在顶层上，可以找到使用的是 testchipip.CanHaveTraceIO:
trait CanHaveTraceIO { this: HasTiles =&amp;gt;  val module: CanHaveTraceIOModuleImp   // Bind all the trace nodes to a BB; we&amp;#39;ll use this to generate the IO in the imp  val traceNexus = BundleBridgeNexusNode[Vec[TracedInstruction]]()  val tileTraceNodes = tiles.flatMap {  case ext_tile: WithExtendedTraceport =&amp;gt; None  case tile =&amp;gt; Some(tile)  }.</description>
    </item>
    
    <item>
      <title>试用沁恒 CH32V307 评估板</title>
      <link>https://jia.je/hardware/2022/04/19/wch-ch32v307-eval/</link>
      <pubDate>Tue, 19 Apr 2022 22:53:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/04/19/wch-ch32v307-eval/</guid>
      <description>背景 之前有一天看到朋友在捣鼓 CH32V307，因此自己也萌生了试用 CH32V307 评估板的兴趣，于是在沁恒官网申请样品，很快就接到电话了解情况，几天后就顺丰送到了，不过因为疫情原因直到现在才拿到手上，只能说疫情期间说不定货比人还快。
开箱 收到的盒子里有一个 CH32V307 评估板，和一个 WCH-Link，相关资料可以在 官网 或者 openwch/ch32v307 下载。在说明书中有如下的图示：
板子自带的跳线帽不是很多，建议自备一些，或者用杜邦线替代。比较重要的是 WCH-Link 子板上 CH549 和 CH2V307 连接的几个信号，和下面 BOOT0/1 的选择。
WCH-Link 可以看到评估板自带了一个 WCH-Link，所以不需要附赠的那一个，直接把 11 号 Type-C 连接到电脑上即可。这里还遇到一个小插曲，用 Type-C to Type-C 的线连电脑上不工作，连 PWR LED 都点不亮，换一根 Type-A to Type-C 的就可以，没有继续研究是什么原因。电脑上可以看到 WCH-Link 的设备：VID=1a86, PID=8010。比较有意思的是，在 RISC-V 模式（CON 灯不亮）的时候 PID 是 8010，ARM 模式（CON 灯亮）的时候 PID 是 8011，从 RISC-V 模式切换到 ARM 模式的方法是连接 TX 和 GND 后上电，反过来要用 MounRiver，详见 WCH-Link 使用说明 V1.0 V1.3 和原理图 V1.1。</description>
    </item>
    
    <item>
      <title>通过 JTAG 对 VCU128 上的 Rocket Chip 进行调试</title>
      <link>https://jia.je/hardware/2022/03/09/rocket-chip-jtag-debug/</link>
      <pubDate>Wed, 09 Mar 2022 19:04:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2022/03/09/rocket-chip-jtag-debug/</guid>
      <description>前言 两年前，我尝试过用 BSCAN JTAG 来配置 Rocket Chip 的调试，但是这个方法不是很好用，具体来说，如果有独立的一组 JTAG 信号，配置起来会更方便，而且不用和 Vivado 去抢，OpenOCD 可以和 Vivado hw_server 同时运行和工作。但是，苦于 VCU128 上没有 PMOD 接口，之前一直没考虑过在 VCU128 上配置独立的 JTAG。然后最近研究了一下，终于解决了这个问题。
寻找 JTAG 接口 前几天在研究别的问题的时候，看到 VCU128 文档中的这段话：
The FT4232HL U8 multi-function USB-UART on the VCU128 board provides three level-shifted UART connections through the single micro-AB USB connector J2. • Channel A is configured in JTAG mode to support the JTAG chain • Channel B implements 4-wire UART0 (level-shifted) FPGA U1 bank 67 connections • Channel C implements 4-wire UART1 (level-shifted) FPGA U1 bank 67 connections • Channel D implements 2-wire (level-shifted) SYSCTLR U42 bank 501 connections  其中 Channel A 是到 FPGA 本身的 JTAG 接口，是给 Vivado 用的，如果是通过 BSCAN 的方式，也是在这个 Channel 上，但是需要经过 FPGA 自己的 TAP 再隧道到 BSCAN 上，比较麻烦。Channel B 和 C 是串口，Channel D 是连接 VCU128 上的 System Controller 的。之前的时候，都是直接用 Channel B 做串口，然后突发奇想：注意到这里是 4-wire UART，说明连接到 FPGA 是四条线，那是不是也可以拿来当 JTAG 用？</description>
    </item>
    
    <item>
      <title>RISC-V Vector 1.0 工具链构建</title>
      <link>https://jia.je/software/2022/01/25/rvv-1.0-toolchain/</link>
      <pubDate>Tue, 25 Jan 2022 21:41:00 +0800</pubDate>
      
      <guid>https://jia.je/software/2022/01/25/rvv-1.0-toolchain/</guid>
      <description>不久前 RVV 1.0 标准终于是出来了，但是工具链的支持目前基本还处于刚 upstream 还没有 release 的状态。而目前 RVV 1.0 的支持主要在 LLVM 上比较活跃，因此也是采用 LLVM Clang + GCC Newlib Toolchain 的方式进行配合，前者做 RVV 1.0 的编译，后者提供 libc 等基础库。
UPDATE: LLVM 14 已经发布，这个版本已经支持 RVV 1.0，直接从 https://apt.llvm.org 等地安装 LLVM 14 即可。
LLVM Clang 直接采用 upstream 即可。编译选项：
$ cmake -G Ninja ../llvm -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_INSTALL_PREFIX=/prefix/llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=&amp;#34;clang&amp;#34; -DLLVM_TARGETS_TO_BUILD=&amp;#34;RISCV&amp;#34; $ ninja $ ninja install $ /prefix/llvm/bin/clang --version clang version 14.0.0 (https://github.com/llvm/llvm-project.git 8d298355ca3778a47fd6b3110aeee03ea5e8e02b) Target: x86_64-unknown-linux-gnu Thread model: posix InstalledDir: /data/llvm/bin 还需要配合一个 GCC 工具链才可以完整地工作。可以直接采用 riscv-gnu-toolchain nightly 版本，比如 riscv64-elf-ubuntu-20.</description>
    </item>
    
    <item>
      <title>RISC-V Debug 分析</title>
      <link>https://jia.je/hardware/2021/12/12/riscv-debug/</link>
      <pubDate>Sun, 12 Dec 2021 14:01:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/12/12/riscv-debug/</guid>
      <description>参考文档  RISC-V Debug Spec 0.13 IEEE Standard for JTAG 1149.1-2013 OpenOCD 相关代码  背景 之前用过一些 RISC-V 核心，但是遇到调试相关的内容的时候就两眼一抹黑，不知道原理，出了问题也不知道如何排查，趁此机会研究一下工作原理。
架构 为了调试 RISC-V 核心，需要很多部件一起工作。按 RISC-V Debug Spec 所述，有这么几部分：
 Debugger: GDB，连接到 OpenOCD 启动的 GDB Server Debug Translator: OpenOCD，向 GDB 提供 Server 实现，同时会通过 FTDI 等芯片控制 JTAG Debug Transport Hardware: 比如 FTDI 的芯片，可以提供 USB 接口，让 OpenOCD 控制 JTAG 信号 TMS/TDI/TCK 的变化，并读取 TDO Debug Transport Module: 在芯片内部的 JTAG 控制器（TAP），符合 JTAG 标准 Debug Module Interface：RISC-V 自定义的一系列寄存器，通过这些寄存器来控制 Debug Module 的行为 Debug Module：调试器，控制 RISC-V 核心，同时也支持直接访问总线，也有内部的 Program Buffer  可以看到，DMI 是实际的调试接口，而 JTAG 可以认为是一个传输协议。</description>
    </item>
    
    <item>
      <title>移植系统到 Rocket Chip on VCU128</title>
      <link>https://jia.je/hardware/2021/10/18/port-system-to-rocket-chip-on-vcu128/</link>
      <pubDate>Mon, 18 Oct 2021 08:35:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2021/10/18/port-system-to-rocket-chip-on-vcu128/</guid>
      <description>背景 最近需要在 VCU128 上搭建一个 SOC，然后想到可以把 OpenSBI、U-Boot 和 Linux 移植到这个平台上方便测试，于是又开始折腾这些东西。代码仓库都已经开源：
 rocket-chip-vcu128 opensbi u-boot linux  Rocket Chip on VCU128 第一部分是基于之前 rocket2thinpad 在 Thinpad 上移植 Rocket Chip 的经验，做了一些更新，主要是因为 VCU128 的外设不大一样，同时我也要运行更复杂的程序，主要做了这些事情：
 添加了 VCU128 的内存和外设：HBM、SPI、I2C、UART、ETH 打开了更多核心选项：S-mode 和 U-mode  主要踩过的坑：
 BSCAN 不工作，估计是因为一些参数不对，@jsteward 之前在 zcu 平台上做了一些测试，估计要用类似的办法进行修改；我最后直接去掉了这部分逻辑 这个板子的 PHY RESET 信号要通过 I2C 接口访问 TI 的 Port Expander，所以没法直接连，要通过 gpio 输出来手动 reset SPI Startup Flash 的时序配置，见我之前的博客 Xilinx PCS/PMA IP 也会自己挂一个设备到 MDIO bus上，应该有自己的 PHY 地址，而不要和物理的 PHY 冲突  U-Boot 在 U-Boot 上花了比较多的时间，用它的目的主要是：</description>
    </item>
    
    <item>
      <title>在 Rocket Chip 上挂接 TLRAM</title>
      <link>https://jia.je/hardware/2020/03/17/rocket-chip-tlram-load/</link>
      <pubDate>Tue, 17 Mar 2020 23:20:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/03/17/rocket-chip-tlram-load/</guid>
      <description>最近遇到一个需求，需要在 Rocket Chip 里面开辟一块空间，通过 verilog 的 $readmemh 来进行初始化而不是用 BootROM ，这样每次修改内容不需要重新跑一次 Chisel -&amp;gt; Verilog 的流程。然后到处研究了一下，找到了解决的方案：
首先是新建一个 TLRAM 然后挂接到 cbus 上：
import freechips.rocketchip.tilelink.TLRAM import freechips.rocketchip.tilelink.TLFragmenter import freechips.rocketchip.diplomacy.LazyModule import freechips.rocketchip.diplomacy.AddressSet  trait HasTestRAM { this: BaseSubsystem =&amp;gt;  val testRAM = LazyModule(  new TLRAM(AddressSet(0x40000000, 0x1FFF), beatBytes = cbus.beatBytes)  )   testRAM.node := cbus.coupleTo(&amp;#34;bootrom&amp;#34;) { TLFragmenter(cbus) := _ } } 这里的地址和大小都可以自由定义。然后添加到自己的 Top Module 中：
class TestTop(implicit p:Parameters) 	extends RocketSystem 	// .</description>
    </item>
    
    <item>
      <title>通过 BSCAN JTAG 对 Rocket Chip 进行调试</title>
      <link>https://jia.je/hardware/2020/02/10/rocket-chip-bscan-debug/</link>
      <pubDate>Mon, 10 Feb 2020 15:08:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/10/rocket-chip-bscan-debug/</guid>
      <description>前言 在上一个 post 里研究了原理，今天也是成功在 Artix 7 上实现了调试。效果如下：
OpenOCD 输出：
Info : JTAG tap: riscv.cpu tap/device found: 0x0362d093 (mfg: 0x049 (Xilinx), part: 0x362d, ver: 0x0) Info : datacount=1 progbufsize=16 Info : Disabling abstract command reads from CSRs. Info : Examined RISC-V core; found 1 harts Info : hart 0: XLEN=32, misa=0x40801105 Info : Listening on port 3333 for gdb connections GDB 输出：
Remote debugging using localhost:3333 0x0001018c in getc () at bootloader.</description>
    </item>
    
    <item>
      <title>研究 Rocket Chip 的 BSCAN 调试原理</title>
      <link>https://jia.je/hardware/2020/02/09/rocket-chip-bscan-analysis/</link>
      <pubDate>Sun, 09 Feb 2020 15:11:00 +0800</pubDate>
      
      <guid>https://jia.je/hardware/2020/02/09/rocket-chip-bscan-analysis/</guid>
      <description>前言 最近 @jsteward 在研究如何通过 JTAG 对 FPGA 里的 Rocket Chip 进行调试。之前 @sequencer 已经做了一些实践，我们在重复他的工作，同时也研究了一下这是怎么工作的。
原理 我们从 @sequencer 得到了一份可用的 Scala 代码 和 OpenOCD 配置，并且了解到：
 可以通过 openocd 找到并调试 Rocket Chip openocd 是通过 JTAG 向 FPGA 的 TAP 的 IR 写入 USER4，然后往 DR 写入特定格式的数据，然后控制 Rocket Chip 的 JTAG。  这里涉及到一个“封装”的过程，在一个仅可以控制 DR 的 JTAG 中控制另一个 JTAG。首先可以找到 OpenOCD 端的操作代码：
tunneled_ir[3].num_bits = 3; tunneled_ir[3].out_value = bscan_zero; tunneled_ir[3].in_value = NULL; tunneled_ir[2].num_bits = bscan_tunnel_ir_width; tunneled_ir[2].out_value = ir_dtmcontrol; tunneled_ir[1].in_value = NULL; tunneled_ir[1].</description>
    </item>
    
    <item>
      <title>交叉编译 Nginx 1.14.2 到 RISC-V</title>
      <link>https://jia.je/software/2019/03/22/cross-compiling-nginx-to-riscv/</link>
      <pubDate>Fri, 22 Mar 2019 23:18:00 +0800</pubDate>
      
      <guid>https://jia.je/software/2019/03/22/cross-compiling-nginx-to-riscv/</guid>
      <description>最近又把一定的精力放到了 RISC-V 64 上的 rCore 用户态程序的支持上，同时也借到了 HiFive Unleashed 板子，所以有真实硬件可以拿来跑了。在这之前先在 QEMU 上把能跑的都跑起来。
由于 rCore 对 glibc 的支持一直有问题，RISC-V 也不例外，所以还是选择用 musl 来做这件事情。一般搜索，终于找到了 Linux 下能用的 musl-riscv-toolchain 。编译好工具链以后，很多需要 libc 的用户态都能跑了，于是想着试一下 nginx 的编译。试着编译了一下，遇到了各种问题，最后搜到了交叉编译Hi3536上面使用的nginx，里面的方法解决了这个问题。最后总结出了这样的 patch :
diff --git a/nginx-1.14.2/auto/cc/name b/nginx-1.14.2/auto/cc/name index ded93f5..d6ab27a 100644 --- a/nginx-1.14.2/auto/cc/name +++ b/nginx-1.14.2/auto/cc/name @@ -7,7 +7,7 @@ if [ &amp;#34;$NGX_PLATFORM&amp;#34; != win32 ]; then   ngx_feature=&amp;#34;C compiler&amp;#34;  ngx_feature_name= - ngx_feature_run=yes + ngx_feature_run=no  ngx_feature_incs=  ngx_feature_path=  ngx_feature_libs= diff --git a/nginx-1.14.2/auto/lib/openssl/make b/nginx-1.</description>
    </item>
    
    <item>
      <title>体验 Fedora on RISCV</title>
      <link>https://jia.je/os/2018/05/24/trying-fedora-on-riscv/</link>
      <pubDate>Thu, 24 May 2018 23:40:00 +0800</pubDate>
      
      <guid>https://jia.je/os/2018/05/24/trying-fedora-on-riscv/</guid>
      <description>看到 RISCV 很久了，但一直没能体验。最近工具链不断更新， QEMU 在 2.12.0 也正式加入了 riscv 的模拟。但是自己编译一个内核又太麻烦，就找到了 Fedora 做的 RISCV port，下载下来试用了一下。之前试过一次，但是遇到了一些问题，刚才总算是成功地搞出来了。
官方文档地址： https://fedorapeople.org/groups/risc-v/disk-images/readme.txt 首先下载 https://fedorapeople.org/groups/risc-v/disk-images/ 下的 bbl vmlinux 和 stage4-disk.img.xz 三个文件，然后解压 stage4-disk.img.xz ，大约有 5G 的样子。之前作者在脚本里作死开得特别大，导致我以前光是解压这一步就成功不了。现在终于解决了。
然后启动 qemu 命令打开虚拟机：
qemu-system-riscv64 \  -nographic \  -machine virt \  -m 2G \  -kernel bbl \  -object rng-random,filename=/dev/urandom,id=rng0 \  -device virtio-rng-device,rng=rng0 \  -append &amp;#34;console=ttyS0 ro root=/dev/vda&amp;#34; \  -device virtio-blk-device,drive=hd0 \  -drive file=stage4-disk.img,format=raw,id=hd0 \  -device virtio-net-device,netdev=usernet \  -netdev user,id=usernet,hostfwd=tcp::10000-:22 这段命令摘自 readme.</description>
    </item>
    
  </channel>
</rss>
